<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">

<head >
  <meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0, user-scalable=0"><meta data-n-head="ssr" http-equiv="Content-Security-Policy" content="default-src * data:; child-src * &#x27;self&#x27; blob: http:;img-src * &#x27;self&#x27; data: http:; script-src &#x27;self&#x27; &#x27;unsafe-inline&#x27; &#x27;unsafe-eval&#x27; *; media-src * data: blob:;style-src &#x27;self&#x27; &#x27;unsafe-inline&#x27; *"><meta data-n-head="ssr" name="keywords" content="Autonomous robots,scene understanding,deep learning,object detection,pose estimation"><meta data-n-head="ssr" name="description" content="Autonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed."><meta data-n-head="ssr" name="dc.title" content="Deep learning-based scene understanding for autonomous robots: a survey"><meta data-n-head="ssr" name="journal_id" content="ir.2023.22"><meta data-n-head="ssr" name="dc.date" content="2023-08-15"><meta data-n-head="ssr" name="dc.identifier" content="doi:10.20517/ir.2023.22"><meta data-n-head="ssr" name="dc.publisher" content="OAE Publishing Inc."><meta data-n-head="ssr" name="dc.type" content="Review"><meta data-n-head="ssr" name="dc.source" content=" Intell Robot 2023;3(3):374-401."><meta data-n-head="ssr" name="dc.citation.spage" content="374"><meta data-n-head="ssr" name="dc.citation.epage" content="401"><meta data-n-head="ssr" name="dc.creator" content="Jianjun Ni"><meta data-n-head="ssr" name="dc.creator" content="Yan Chen"><meta data-n-head="ssr" name="dc.creator" content="Guangyi Tang"><meta data-n-head="ssr" name="dc.creator" content="Jiamei Shi"><meta data-n-head="ssr" name="dc.creator" content="Weidong Cao"><meta data-n-head="ssr" name="dc.creator" content="Pengfei Shi"><meta data-n-head="ssr" name="dc.subject" content="Autonomous robots"><meta data-n-head="ssr" name="dc.subject" content="scene understanding"><meta data-n-head="ssr" name="dc.subject" content="deep learning"><meta data-n-head="ssr" name="dc.subject" content="object detection"><meta data-n-head="ssr" name="dc.subject" content="pose estimation"><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Wang&amp;nbsp;X, Gong&amp;nbsp;T, Xie&amp;nbsp;Y. An improved adaptive ORB-SLAM method for monocular vision robot under dynamic environments. Int J Mach Learn Cyber 2022;13:3821-36."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Li&amp;nbsp;J, Xu&amp;nbsp;Z, Zhu&amp;nbsp;D, et al. Bio-inspired intelligence with applications to robotics: a survey. Intell Robot 2021;1:58-83."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Tang&amp;nbsp;M, Chen&amp;nbsp;Y, Cao&amp;nbsp;W. An improved cooperative control method for hybrid unmanned aerial-ground system in multitasks. Int J Aerosp Eng 2020; doi: 10.1155/2020/9429108."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhao&amp;nbsp;ZQ, Zheng&amp;nbsp;P, Xu&amp;nbsp;ST, Wu&amp;nbsp;X. Object Detection with Deep Learning: A Review. IEEE Trans Neural Netw Learn Syst 2019;30:3212-32."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Garcia-Garcia&amp;nbsp;A, Orts-Escolano&amp;nbsp;S, Oprea&amp;nbsp;S, Villena-Martinez&amp;nbsp;V, Martinez-Gonzalez&amp;nbsp;P, Garcia-rodriguez&amp;nbsp;J. A survey on deep learning techniques for image and video semantic segmentation. Appl Soft Comput 2018;70:41-65."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;L, Huang&amp;nbsp;Y. A survey of 3D point cloud and deep learning-based approaches for scene understanding in autonomous driving. IEEE Intell Transport Syst Mag 2022;14:135-54."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Naseer&amp;nbsp;M, Khan&amp;nbsp;S, Porikli&amp;nbsp;F. Indoor scene understanding in 2.5/3D for autonomous agents: a survey. IEEE Access 2019;7:1859-87."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhu&amp;nbsp;M, Ferstera&amp;nbsp;A, Dinulescu&amp;nbsp;S, et al. A peristaltic soft, wearable robot for compression therapy and massage. IEEE Robot Autom 2023;8:4665-72."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Sun&amp;nbsp;P, Shan&amp;nbsp;R, Wang&amp;nbsp;S. An intelligent rehabilitation robot with passive and active direct switching training: improving intelligence and security of human-robot interaction systems. IEEE Robot Automat Mag 2023;30:72-83."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;TM, Tao&amp;nbsp;Y, Liu&amp;nbsp;H. Current researches and future development trend of intelligent robot: a review. Int J Autom Comput 2018;15:525-46."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lowe&amp;nbsp;DG. Distinctive image features from scale-invariant keypoints. Int J comput vis 2004;60:91-110."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhou&amp;nbsp;H, Yuan&amp;nbsp;Y, Shi&amp;nbsp;C. Object tracking using SIFT features and mean shift. Comput Vis Image Underst 2009;113:345-52."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Oliva&amp;nbsp;A, Torralba&amp;nbsp;A. Modeling the shape of the scene: a holistic representation of the spatial envelope. Int J comput vis 2001;42:145-75."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hofmann&amp;nbsp;T. Unsupervised learning by probabilistic latent semantic analysis. Mach Learn 2001;42:177-96."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Sarhan&amp;nbsp;S, Nasr&amp;nbsp;AA, Shams&amp;nbsp;MY. Multipose face recognition-based combined adaptive deep learning vector quantization. Comput Intell Neurosci 2020;2020:8821868."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Liu&amp;nbsp;B, Wu&amp;nbsp;H, Su&amp;nbsp;W, Zhang&amp;nbsp;W, Sun&amp;nbsp;J. Rotation-invariant object detection using Sector-ring HOG and boosted random ferns. Vis Comput 2018;34:707-19."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang X, Han TX, Yan S. An HOG-LBP human detector with partial occlusion handling. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 32-39."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Vailaya&amp;nbsp;A, Figueiredo&amp;nbsp;MA, Jain&amp;nbsp;AK, Zhang&amp;nbsp;HJ. Image classification for content-based indexing. XX 2001;10:117-30."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Li LJ, Su H, Xing EP, Fei-Fei L. Object bank: a high-level image representation for scene classification &amp;amp; semantic feature sparsification. In: Proceedings of the 23rd International Conference on Neural Information Processing Systems; 2010. p. 1378–86. Available from: https://proceedings.neurips.cc/paper/2010/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang&amp;nbsp;L, Li&amp;nbsp;W, Yu&amp;nbsp;L, Sun&amp;nbsp;L, Dong&amp;nbsp;X, Ning&amp;nbsp;X. GmFace: an explicit function for face image representation. Displays 2021;68:102022."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ning&amp;nbsp;X, Gong&amp;nbsp;K, Li&amp;nbsp;W, Zhang&amp;nbsp;L, Bai&amp;nbsp;X, et al. Feature refinement and filter network for person re-identification. IEEE Trans Circuits Syst Video Technol 2021;31:3391-402."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Chen&amp;nbsp;Y, Chen&amp;nbsp;Y, Zhu&amp;nbsp;J, Ali&amp;nbsp;D, Cao&amp;nbsp;W. A survey on theories and applications for self-driving cars based on deep learning methods. Appl Sci 2020;10:2749."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition. Providence; 2012 Jun 16-21; RI, USA. IEEE; 2012. p. 3354-61."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Caesar H, Bankiti V, Lang AH, et al. nuScenes: a multimodal dataset for autonomous driving. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11618-28."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hinterstoisser S, Lepetit V, Ilic S, et al. Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes. In: Lee KM, Matsushita Y, Rehg JM, Hu Z, editors. Computer Vision - ACCV; 2013. p. 548-62."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Piga&amp;nbsp;NA, Onyshchuk&amp;nbsp;Y, Pasquale&amp;nbsp;G, Pattacini&amp;nbsp;U, Natale&amp;nbsp;L. ROFT: Real-time tptical flow-aided 6D object pose and velocity tracking. IEEE Robot Autom Lett 2022;7:159-66."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Everingham&amp;nbsp;M, Gool&amp;nbsp;LV, Williams&amp;nbsp;CKI, Winn&amp;nbsp;JM, Zisserman&amp;nbsp;A. The pascal visual object classes (VOC) challenge. Int J Comput Vis 2010;88:303-38."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Cordts M, Omran M, Ramos S, et al. The cityscapes dataset for semantic urban scene understanding. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016 Jun 27-30; Las Vegas, NV, USA. IEEE; 2016. p. 3213-23."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang W, Shen J, Guo F, Cheng MM, Borji A. Revisiting video saliency: a large-scale benchmark and a new model. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 4894-903."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Kristan M, Leonardis A, Matas J, et al. The visual object tracking VOT2017 challenge results. In: 2017 IEEE International Conference on Computer Vision Workshops (ICCVW); 2017 Oct 22-29; Venice, Italy. IEEE; 2017. p. 1949-72."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Shen&amp;nbsp;K, Chen&amp;nbsp;Y, Yang&amp;nbsp;SX. An improved SSD-like deep network-based object detection method for indoor scenes. IEEE Trans Instrum Meas 2023;72:1-15."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Qian&amp;nbsp;R, Lai&amp;nbsp;X, Li&amp;nbsp;X. BADet: boundary-aware 3D object detection from point clouds. Pattern Recognit 2022;125:108524."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Shi&amp;nbsp;S, Wang&amp;nbsp;Z, Shi&amp;nbsp;J, Wang&amp;nbsp;X, Li&amp;nbsp;H. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE Trans Pattern Anal Mach Intell 2021;43:2647-64."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Li&amp;nbsp;Y, Ma&amp;nbsp;L, Zhong&amp;nbsp;Z, Cao&amp;nbsp;D, Li&amp;nbsp;J. TGNet: geometric graph CNN on 3-D point cloud segmentation. IEEE Trans Geosci Remote Sens 2020;58:3588-600."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yan&amp;nbsp;Y, Mao&amp;nbsp;Y, Li&amp;nbsp;B. SECOND: sparsely embedded convolutional detection. Sensors 2018;18:3337."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Qi CR, Liu W, Wu C, Su H, Guibas LJ. Frustum pointnets for 3D object detection from RGB-D data. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018 Jun 18-23; Salt Lake City, UT, USA; IEEE; 2018. p. 918-27."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang Z, Jia K. Frustum convNet: sliding frustums to aggregate local point-wise features for amodal 3D object detection. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS); 2019 Nov 03-08; Macau, China. IEEE; 2019. p. 1742-49."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen Y, Liu S, Shen X, Jia J. Fast point R-CNN. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea. IEEE; 2019. p. 9774-83."><meta data-n-head="ssr" name="citation_reference" content="citation_title=He C, Zeng H, Huang J, Hua XS, Zhang L. Structure aware single-stage 3D object detection from point cloud. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11870-9."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Liu Z, Zhao X, Huang T, Hu R, Zhou Y, Bai X. TANet: robust 3D object detection from point clouds with triple attention. In: 34th AAAI Conference on Artificial Intelligence, AAAI; 2020 Feb 7-12; New York, NY, United states. California: AAAI; 2020. p. 11677-84."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yin T, Zhou X, Krahenbuhl P. Center-based 3D Object Detection and Tracking. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021. Virtual, Online, United states; 2021. pp. 11779 – 11788."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang H, Shi S, Yang Z, et al. RBGNet: ray-based Grouping for 3D Object Detection. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2022 Jun 18-24; New Orleans, LA, USA. IEEE; 2022. p. 1100-09."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen&amp;nbsp;Y, Ni&amp;nbsp;J, Tang&amp;nbsp;G, Cao&amp;nbsp;W, Yang&amp;nbsp;SX. An improved dense-to-sparse cross-modal fusion network for 3D object detection in RGB-D images. Multimed Tools Appl 2023; doi: 10.1007/s11042-023-15845-5."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hoang&amp;nbsp;DC, Stork&amp;nbsp;JA, Stoyanov&amp;nbsp;T. Voting and attention-based pose relation learning for object pose estimation from 3D point clouds. IEEE Robot Autom Lett 2022;7:8980-7."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yue K, Sun M, Yuan Y, Zhou F, Ding E, Xu F. Compact generalized non-local network. arXiv. [Preprint.] November 1, 2018. Available from: https://arxiv.org/abs/1810.13125 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen H, Wang P, Wang F, Tian W, Xiong L, Li H. Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation. arXiv. [Preprint.] August 11, 2022 Available from: https://arxiv.org/abs/2203.13254 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Moon G, Chang JY, Lee KM. V2V-poseNet: voxel-to-voxel prediction network for accurate 3D hand and human pose estimation from a single depth map. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 5079-88."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Li Z, Wang G, Ji X. CDPN: coordinates-based disentangled pose network for real-time RGB-based 6-DoF object pose estimation. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea (South). IEEE; 2020. p. 7677-86."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang H, Sridhar S, Huang J, Valentin J, Song S, Guibas LJ. Normalized object coordinate space for category-level 6D object pose and size estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 2637-46."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yu X, Zhuang Z, Koniusz P, Li H. 6DoF object pose estimation via differentiable proxy voting loss. arXiv. [Preprint.] Febuary 11, 2020. Available from: https://arxiv.org/abs/2002.03923 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen W, Jia X, Chang HJ, Duan J, Leonardis A. G2L-net: global to local network for real-time 6D pose estimation with embedding vector features. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 4232-41."><meta data-n-head="ssr" name="citation_reference" content="citation_title=He Y, Sun W, Huang H, Liu J, Fan H, Sun J. PVN3D: a deep point-wise 3D keypoints voting network for 6DoF pose estimation. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19 Seattle, WA, USA. IEEE; 2020. pp. 11629-38."><meta data-n-head="ssr" name="citation_reference" content="citation_title=He Y, Huang H, Fan H, Chen Q, Sun J. FFB6D: a full flow bidirectional fusion network for 6D pose estimation. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2021 Jun 20-25; Nashville, TN, USA. IEEE; 2021. p. 3002-12."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. arXiv. [Preprint.] November 14, 2014. Available from: https://arxiv.org/abs/1411.4038 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Badrinarayanan&amp;nbsp;V, Kendall&amp;nbsp;A, Cipolla&amp;nbsp;R. SegNet: a deep convolutional encoder-decoder architecture for image segmentation. IEEE Trans Pattern Anal Mach Intell 2017;39:2481-95."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. Semantic image segmentation with deep convolutional nets and fully connected CRFs. arXiv. [Preprint.] December 22, 2014. Available from: https://arxiv.org/abs/1412.7062 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen&amp;nbsp;LC, Papandreou&amp;nbsp;G, Kokkinos&amp;nbsp;I, Murphy&amp;nbsp;K, Yuille&amp;nbsp;AL. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Trans Pattern Anal Mach Intell 2017;40:834-48."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lin G, Milan A, Shen C, Reid I. Refinenet: multi-path refinement networks for high-resolution semantic segmentation. In: 2017IEEE conference on computer vision and pattern recognition (CVPR); 2017. p. 1925-34."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zeng L, Zhang S, Wang P, Li Z, Hu Y, Xie T. Defect detection algorithm for magnetic particle inspection of aviation ferromagnetic parts based on improved DeepLabv3+. Meas Sci Technol 2023;34: 065401. Measurement Science and Technology 2023;34: 065401."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yin&amp;nbsp;R, Cheng&amp;nbsp;Y, Wu&amp;nbsp;H, Song&amp;nbsp;Y, Yu&amp;nbsp;B, Niu&amp;nbsp;R. Fusionlane: multi-sensor fusion for lane marking semantic segmentation using deep neural networks. IEEE Trans Intell Transport Syst 2022;23:1543-53."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hu&amp;nbsp;P, Perazzi&amp;nbsp;F, Heilbron&amp;nbsp;FC, et al. Real-time semantic segmentation with fast attention. IEEE Robot Autom Lett 2021;6:263-70."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Sun&amp;nbsp;Y, Zuo&amp;nbsp;W, Yun&amp;nbsp;P, Wang&amp;nbsp;H, Liu&amp;nbsp;M. FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion. IEEE Trans Automat Sci Eng 2021;18:1000-11."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Yang M, Yu K, Zhang C, Li Z, Yang K. DenseASPP for semantic segmentation in street scenes. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 3684-92."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang H, Dana K, Shi J, et al. Context encoding for semantic segmentation. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 7151-60."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Fu J, Liu J, Tian H, et al. Dual attention network for scene segmentation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 3141-9."><meta data-n-head="ssr" name="citation_reference" content="citation_title=He J, Deng Z, Zhou L, Wang Y, Qiao Y. Adaptive pyramid context network for semantic segmentation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 7511-20."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang C, Lin G, Liu F, Yao R, Shen C. CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 5212-21."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Liu J, He J, Zhang J, Ren JS, Li H. EfficientFCN: holistically-guided decoding for semantic segmentation. In: Vedaldi A, Bischof H, Brox T, Frahm JM, editors. Computer Vision – ECCV 2020. Cham: Springer; 2020. p. 1-17."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ha Q, Watanabe K, Karasawa T, Ushiku Y, Harada T. MFNet: towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes. In: 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS); 2017 Sep 24-28; Vancouver, BC, Canada. IEEE; 2017. p. 5108-15."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Cheng&amp;nbsp;B, Schwing&amp;nbsp;AG, Kirillov&amp;nbsp;A. Per-pixel classification is not all you need for semantic segmentation. Signal Process Image Commun 2021;88:17864-75."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhou&amp;nbsp;W, Yue&amp;nbsp;Y, Fang&amp;nbsp;M, Qian&amp;nbsp;X, Yang&amp;nbsp;R, Yu&amp;nbsp;L. BCINet: bilateral cross-modal interaction network for indoor scene understanding in RGB-D images. Inf Fusion 2023;94:32-42."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lou&amp;nbsp;J, Lin&amp;nbsp;H, Marshall&amp;nbsp;D, Saupe&amp;nbsp;D, Liu&amp;nbsp;H. TranSalNet: towards perceptually relevant visual saliency prediction. Neurocomputing 2022;494:455-67."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Judd T, Ehinger K, Durand F, Torralba A. Learning to predict where humans look. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 2106-13."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ishikura&amp;nbsp;K, Kurita&amp;nbsp;N, Chandler&amp;nbsp;DM, Ohashi&amp;nbsp;G. Saliency detection based on multiscale extrema of local perceptual color differences. IEEE Trans Image Process 2018;27:703-17."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zou&amp;nbsp;W, Zhuo&amp;nbsp;S, Tang&amp;nbsp;Y, Tian&amp;nbsp;S, Li&amp;nbsp;X, Xu&amp;nbsp;C. STA3D: spatiotemporally attentive 3D network for video saliency prediction. Pattern Recognit Lett 2021;147:78-84."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang W, Shen J, Dong X, Borji A. Salient object detection driven by fixation prediction. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 1711-20."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Huang&amp;nbsp;R, Xing&amp;nbsp;Y, Wang&amp;nbsp;Z. RGB-D salient object detection by a CNN with multiple layers fusion. IEEE Signal Process Lett 2019;26:552-6."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;N, Gong&amp;nbsp;X. Adaptive fusion for RGB-D salient object detection. IEEE Access 2019;7:55277-84."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang&amp;nbsp;J, Yu&amp;nbsp;M, Jiang&amp;nbsp;G, Qi&amp;nbsp;Y. CMP-based saliency model for stereoscopic omnidirectional images. Digit Signal Process 2020;101:102708."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Fang&amp;nbsp;Y, Zhang&amp;nbsp;C, Min&amp;nbsp;X, et al. DevsNet: deep video saliency network using short-term and long-term cues. Pattern Recognit 2020;103:107294."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Li&amp;nbsp;F, Zheng&amp;nbsp;J, fang&amp;nbsp;Zhang Y, Liu&amp;nbsp;N, Jia&amp;nbsp;W. AMDFNet: adaptive multi-level deformable fusion network for RGB-D saliency detection. Neurocomputing 2021;465:141-56."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lee&amp;nbsp;H, Kim&amp;nbsp;S. SSPNet: learning spatiotemporal saliency prediction networks for visual tracking. Inf Sci 2021;575:399-416."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Xue&amp;nbsp;H, Sun&amp;nbsp;M, Liang&amp;nbsp;Y. ECANet: explicit cyclic attention-based network for video saliency prediction. Neurocomputing 2022;468:233-44."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang&amp;nbsp;N, Nex&amp;nbsp;F, Kerle&amp;nbsp;N, Vosselman&amp;nbsp;G. LISU: low-light indoor scene understanding with joint learning of reflectance restoration. SPRS J Photogramm Remote Sens 2022;183:470-81."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Tang&amp;nbsp;G, Ni&amp;nbsp;J, Chen&amp;nbsp;Y, Cao&amp;nbsp;W, Yang&amp;nbsp;SX. An improved cycleGAN based model for low-light image enhancement. IEEE Sensors J 2023; doi: 10.1109/JSEN.2023.3296167."><meta data-n-head="ssr" name="citation_reference" content="citation_title=He&amp;nbsp;J, Li&amp;nbsp;M, Wang&amp;nbsp;Y, Wang&amp;nbsp;H. OVD-SLAM: an online visual SLAM for dynamic environments. IEEE Sensors J 2023;23:13210-9."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lu&amp;nbsp;X, Sun&amp;nbsp;H, Zheng&amp;nbsp;X. A feature aggregation convolutional neural network for remote sensing scene classification. IEEE Trans Geosci Remote Sens 2019;57:7894-906."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ma&amp;nbsp;D, Tang&amp;nbsp;P, Zhao&amp;nbsp;L. SiftingGAN: generating and sifting labeled samples to improve the remote sensing image scene classification baseline in vitro. IEEE Geosci Remote Sens Lett 2019;16:1046-50."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhang&amp;nbsp;X, Qiao&amp;nbsp;Y, Yang&amp;nbsp;Y, Wang&amp;nbsp;S. SMod: scene-specific-prior-based moving object detection for airport apron surveillance systems. IEEE Intell Transport Syst Mag 2023;15:58-69."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Tang&amp;nbsp;G, Ni&amp;nbsp;J, Shi&amp;nbsp;P, Li&amp;nbsp;Y, Zhu&amp;nbsp;J. An improved ViBe-based approach for moving object detection. Intell Robot 2022;2:130-44."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Lee CY, Badrinarayanan V, Malisiewicz T, Rabinovich A. Roomnet: end-to-end room layout estimation. arXiv. [Preprint.] August 7, 2017. Available from: https://arxiv.org/abs/1703.06241 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hsiao CW, Sun C, Sun M, Chen HT. Flat2layout: Flat representation for estimating layout of general room types. arXiv. [Preprint.] May 29, 2019. Available from: https://arxiv.org/abs/1905.12571 [Last accessed on 8 Aug 2023]."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Sarhan&amp;nbsp;S, Nasr&amp;nbsp;AA, Shams&amp;nbsp;MY. Multipose face recognition-based combined adaptive deep learning vector quantization. Comput Intell Neurosci 2020;2020:8821868."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Rublee E, Rabaud V, Konolige K, Bradski G. ORB: an efficient alternative to SIFT or SURF. In: 2011 International conference on computer vision; 2011 Nov 06-13; Barcelona, Spain. IEEE; 2012. p. 2564-71."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;K, Ma&amp;nbsp;S, Ren&amp;nbsp;F, Lu&amp;nbsp;J. SBAS: salient bundle adjustment for visual SLAM. IEEE Trans Instrum Meas 2021;70:1-9."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Gong&amp;nbsp;T, Gu&amp;nbsp;Y, Zhu&amp;nbsp;J, Fan&amp;nbsp;X. An improved deep residual network-based semantic simultaneous localization and mapping method for monocular vision robot. Comput Intell Neurosci 2020;2020:7490840."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Fu&amp;nbsp;Q, Yu&amp;nbsp;H, Wang&amp;nbsp;X, et al. Fast ORB-SLAM without keypoint descriptors. IEEE Trans Image Process 2022;31:1433-46."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Engel J, Schöps T, Cremers D. LSD-SLAM: large-scale direct monocular SLAM. In: Fleet D, Pajdla T, Schiele B, Tuytelaars, editors. Computer Vision – ECCV 2014. Cham: Springer; 2014. p. 834-49."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Engel&amp;nbsp;J, Koltun&amp;nbsp;V, Cremers&amp;nbsp;D. Direct sparse odometry. IEEE Trans Image Process 2018;40:611-25."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;Y, Zhang&amp;nbsp;S, Wang&amp;nbsp;J. Ceiling-view semi-direct monocular visual odometry with planar constraint. Remote Sens 2022;14:5447."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Forster&amp;nbsp;C, Zhang&amp;nbsp;Z, Gassner&amp;nbsp;M, Werlberger&amp;nbsp;M, Scaramuzza&amp;nbsp;D. SVO: semidirect visual odometry for monocular and multicamera systems. IEEE Trans Robot 2017;33:249-65."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Chen&amp;nbsp;Y, Ni&amp;nbsp;J, Mutabazi&amp;nbsp;E, Cao&amp;nbsp;W, Yang&amp;nbsp;SX. A variable radius side window direct SLAM method based on semantic information. Comput Intell Neurosci 2022;2022:4075910."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Liu&amp;nbsp;L. Image classification in htp test based on convolutional neural network model. Comput Intell Neurosci 2021;2021:6370509."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zheng&amp;nbsp;D, Li&amp;nbsp;L, Zheng&amp;nbsp;S, et al. A defect detection method for rail surface and fasteners based on deep convolutional neural network. Comput Intell Neurosci 2021;2021:2565500."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Gao X, Wang R, Demmel N, Cremers D. LDSO: direct sparse odometry with loop closure. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS); 2018 Oct 01-05; Madrid, Spain. IEEE; 2019. p. 2198-204."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Tang&amp;nbsp;C, Zheng&amp;nbsp;X, Tang&amp;nbsp;C. Adaptive discriminative regions learning network for remote sensing scene classification. Sensors 2023;23:1-5."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Song&amp;nbsp;Y, Feng&amp;nbsp;W, Dauphin&amp;nbsp;G, Long&amp;nbsp;Y, Quan&amp;nbsp;Y, Xing&amp;nbsp;M. Ensemble alignment subspace adaptation method for cross-scene classification. IEEE Geosci Remote Sensing Lett 2023;20:1-5."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Zhu&amp;nbsp;S, Wu&amp;nbsp;C, Du&amp;nbsp;B, Zhang&amp;nbsp;L. Adversarial divergence training for universal cross-scene classification. IEEE Trans Geosci Remote Sens 2023;61:1-12."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Ni&amp;nbsp;J, Shen&amp;nbsp;K, Chen&amp;nbsp;Y, Cao&amp;nbsp;W, Yang&amp;nbsp;SX. An improved deep network-based scene classification method for self-driving cars. IEEE Trans Instrum Meas 2022;71:1-14."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Mohapatra RK, Shaswat K, Kedia S. Offline handwritten signature verification using CNN inspired by inception V1 architecture. In: 2019 Fifth International Conference on Image Information Processing (ICIIP); 2019 Nov 15-17; Shimla, India. IEEE; 2020. p. 263-7."><meta data-n-head="ssr" name="citation_reference" content="citation_title=McCall&amp;nbsp;R, McGee&amp;nbsp;F, Mirnig&amp;nbsp;A, et al. A taxonomy of autonomous vehicle handover situations. Transp Res Part A Policy Pract 2019;124:507-22."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Wang&amp;nbsp;L, Guo&amp;nbsp;S, Huang&amp;nbsp;W, Xiong&amp;nbsp;Y, Qiao&amp;nbsp;Y. Knowledge guided disambiguation for large-scale scene classification with multi-resolution CNNs. IEEE Trans Image Process 2017;26:2055-68."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Hosny&amp;nbsp;KM, Kassem&amp;nbsp;MA, Fouad&amp;nbsp;MM. Classification of skin lesions into seven classes using transfer learning with AlexNet. J Digit Imaging 2020;33:1325-34."><meta data-n-head="ssr" name="citation_reference" content="citation_title=Alhichri&amp;nbsp;H, Alsuwayed&amp;nbsp;A, Bazi&amp;nbsp;Y, Ammour&amp;nbsp;N, Alajlan&amp;nbsp;NA. Classification of remote sensing images using EfficientNet-B3 CNN model with attention. IEEE Access 2021;9:14078-94."><meta data-n-head="ssr" name="citation_journal_title" content="Intelligence &amp; Robotics"><meta data-n-head="ssr" name="citation_publisher" content="OAE Publishing Inc."><meta data-n-head="ssr" name="citation_title" content="Deep learning-based scene understanding for autonomous robots: a survey"><meta data-n-head="ssr" name="citation_publication_date" content="2023/08/15"><meta data-n-head="ssr" name="citation_online_date" content="2023/08/15"><meta data-n-head="ssr" name="citation_doi" content="10.20517/ir.2023.22"><meta data-n-head="ssr" name="citation_volume" content="3"><meta data-n-head="ssr" name="citation_issue" content="3"><meta data-n-head="ssr" name="citation_firstpage" content="374"><meta data-n-head="ssr" name="citation_lastpage" content="401"><meta data-n-head="ssr" name="citation_author" content="Jianjun Ni"><meta data-n-head="ssr" name="citation_author" content="Yan Chen"><meta data-n-head="ssr" name="citation_author" content="Guangyi Tang"><meta data-n-head="ssr" name="citation_author" content="Jiamei Shi"><meta data-n-head="ssr" name="citation_author" content="Weidong Cao"><meta data-n-head="ssr" name="citation_author" content="Pengfei Shi"><meta data-n-head="ssr" name="prism.issn" content="ISSN 2770-3541 (Online)"><meta data-n-head="ssr" name="prism.publicationName" content="OAE Publishing Inc."><meta data-n-head="ssr" name="prism.publicationDate" content="2023-08-15"><meta data-n-head="ssr" name="prism.volume" content="3"><meta data-n-head="ssr" name="prism.section" content="Review"><meta data-n-head="ssr" name="prism.startingPag" content="374"><meta data-n-head="ssr" name="prism.url" content="https://www.oaepublish.com/articles/ir.2023.22"><meta data-n-head="ssr" name="prism.doi" content="doi:10.20517/ir.2023.22"><meta data-n-head="ssr" name="citation_journal_abbrev" content="ir"><meta data-n-head="ssr" name="citation_article_type" content="Review"><meta data-n-head="ssr" name="citation_language" content="en"><meta data-n-head="ssr" name="citation_doi" content="10.20517/ir.2023.22"><meta data-n-head="ssr" name="citation_id" content="ir.2023.22"><meta data-n-head="ssr" name="citation_issn" content="ISSN 2770-3541 (Online)"><meta data-n-head="ssr" name="citation_publication_date" content="2023-08-15"><meta data-n-head="ssr" name="citation_author_institution" content="Correspondence to: Prof. Jianjun Ni, School of Artificial Intelligence and Automation, Hohai University, No.200, North Jinling Road, Xinbei District, Changzhou 213022, Jiangsu, China. E-mail: njjhhuc@gmail.com; ORCID: 0000-0002-7130-8331"><meta data-n-head="ssr" name="citation_pdf_url" content="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022.pdf"><meta data-n-head="ssr" name="citation_fulltext_html_url" content="https://www.oaepublish.com/articles/ir.2023.22"><meta data-n-head="ssr" name="fulltext_pdf" content="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022.pdf"><meta data-n-head="ssr" name="twitter:type" content="article"><meta data-n-head="ssr" name="twitter:title" content="Deep learning-based scene understanding for autonomous robots: a survey"><meta data-n-head="ssr" name="twitter:description" content="Autonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed."><meta data-n-head="ssr" name="og:url" content="https://www.oaepublish.com/articles/ir.2023.22"><meta data-n-head="ssr" name="og:type" content="article"><meta data-n-head="ssr" name="og:site_name" content="Intelligence &amp; Robotics"><meta data-n-head="ssr" name="og:title" content="Deep learning-based scene understanding for autonomous robots: a survey"><meta data-n-head="ssr" name="og:description" content="Autonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed."><title>Deep learning-based scene understanding for autonomous robots: a survey</title><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="canonical" href="https://www.oaepublish.com/articles/ir.2023.22"><script data-n-head="ssr" src="https://accounts.google.com/gsi/client" async></script><script data-n-head="ssr" src="https://g.oaes.cc/oae/dist/relijs.js" async></script><script data-n-head="ssr" src="https://g.oaes.cc/oae/dist/baidu.js" async></script><link rel="preload" href="https://g.oaes.cc/oae/nuxt/ed95672.js" as="script"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/535cccf.js" as="script"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/css/8176b15.css" as="style"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/27419e7.js" as="script"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/css/9ca1cf5.css" as="style"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/08525cf.js" as="script"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/css/00f09b8.css" as="style"><link rel="preload" href="https://g.oaes.cc/oae/nuxt/d45592b.js" as="script"><link rel="stylesheet" href="https://g.oaes.cc/oae/nuxt/css/8176b15.css"><link rel="stylesheet" href="https://g.oaes.cc/oae/nuxt/css/9ca1cf5.css"><link rel="stylesheet" href="https://g.oaes.cc/oae/nuxt/css/00f09b8.css">
</head>

<body >
  <div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div data-fetch-key="data-v-0bee1158:0" data-v-0bee1158><div class="PcComment" data-v-20f04ba2 data-v-0bee1158><div class="ipad_bg" style="display:none;" data-v-20f04ba2></div> <div class="head_top" data-v-20f04ba2><div class="wrapper head_box" data-v-20f04ba2><span class="qk_jx" data-v-20f04ba2><img src="https://i.oaes.cc/upload/journal_logo/ir.png" alt data-v-20f04ba2></span> <a href="/ir" class="qk_a_name" data-v-20f04ba2><span class="title font20" data-v-20f04ba2>Intelligence &amp; Robotics</span></a> <i class="el-icon-caret-right sjbtn" style="color:rgb(0,71,187);" data-v-20f04ba2></i> <div class="top_img" data-v-20f04ba2><a href="" target="_blank" data-v-20f04ba2><img src="https://i.oaes.cc/uploads/20250623/ff9dcb103f1746dd9c8d2f3ec900479f.png" alt data-v-20f04ba2></a><a href="https://www.scopus.com/sourceid/21101199351" target="_blank" data-v-20f04ba2><img src="https://i.oaes.cc/uploads/20250606/5ac28259735646cc836f1a10e3c11d27.png" alt data-v-20f04ba2></a></div> <div class="oae_menu_box" data-v-20f04ba2><a href="/alljournals" data-v-20f04ba2><span data-v-20f04ba2>All Journals</span></a></div> <span class="search" data-v-20f04ba2><i class="icon-search icon_right font24" data-v-20f04ba2></i> <span data-v-20f04ba2>Search</span></span> <span class="go_oae" data-v-20f04ba2><a href="https://www.oaecenter.com/login" target="_blank" data-v-20f04ba2><i class="icon-login-line icon_right font24" data-v-20f04ba2></i> <span data-v-20f04ba2>Log In</span></a></span></div></div> <div class="cg" style="height: 41px" data-v-20f04ba2></div> <!----> <div class="head_text" style="border-bottom:3px solid rgb(0,71,187);" data-v-20f04ba2><div class="head_search wrapper" style="display:none;" data-v-20f04ba2><div class="box_btn" data-v-20f04ba2><div class="qk_miss" data-v-20f04ba2><img src="https://i.oaes.cc/uploads/20260213/7306c5708f5c4541a0c978663fd0b5a5.jpg" alt class="qk_fm" data-v-20f04ba2> <div class="miss_right" data-v-20f04ba2><div class="miss_btn" data-v-20f04ba2><!----></div> <div class="miss_btn" data-v-20f04ba2><span data-v-20f04ba2><span class="font_b" data-v-20f04ba2>Editor-in-Chief:</span> Simon X. Yang</span></div> <div class="miss_btn" data-v-20f04ba2><div class="text_index" data-v-20f04ba2><span class="font_b" data-v-20f04ba2>Indexing: </span> <span data-v-20f04ba2><a href="https://www.oaepublish.com/news/ir.852" target="_blank" data-v-20f04ba2>ESCI</a><span class="xing_d" data-v-20f04ba2>, </span></span><span data-v-20f04ba2><a href="https://www.scopus.com/sourceid/21101199351" target="_blank" data-v-20f04ba2>Scopus</a><span class="xing_d" data-v-20f04ba2>, </span></span><span data-v-20f04ba2><a href="https://scholar.google.com.hk/citations?view_op=list_works&amp;hl=zh-CN&amp;hl=zh-CN&amp;user=-Hx5OVYAAAAJ" target="_blank" data-v-20f04ba2>Google Scholar</a><span class="xing_d" data-v-20f04ba2>, </span></span><span data-v-20f04ba2><a href="https://app.dimensions.ai/discover/publication?and_facet_source_title=jour.1423782" target="_blank" data-v-20f04ba2>Dimensions</a><span class="xing_d" data-v-20f04ba2>, </span></span><span data-v-20f04ba2><a href="https://www.lens.org/lens/search/scholar/list?p=0&amp;n=10&amp;s=date_published&amp;d=%2B&amp;f=false&amp;e=false&amp;l=en&amp;authorField=author&amp;dateFilterField=publishedYear&amp;orderBy=%2Bdate_published&amp;presentation=false&amp;preview=true&amp;stemmed=true&amp;useAuthorId=false&amp;publicationType.must=journal%20article&amp;sourceTitle.must=Intelligence%20%26%20Robotics&amp;publisher.must=OAE%20Publishing%20Inc." target="_blank" data-v-20f04ba2>Lens</a><span class="xing_d" data-v-20f04ba2>, </span></span><span data-v-20f04ba2><a href="" target="_blank" data-v-20f04ba2>ASCI</a><span class="xing_d" data-v-20f04ba2>, </span></span></div> <div class="text_jour" data-v-20f04ba2><span class="font_b" data-v-20f04ba2>Journal Rank: </span><span data-v-20f04ba2>Impact factor 2.3 -Q2; CiteScore 3.7-Q2</span></div> <div data-v-20f04ba2><span class="font_b" data-v-20f04ba2>Median time to first editorial decision: </span><span data-v-20f04ba2>8 days</span></div></div> <!----> <div class="btn_box_t" data-v-20f04ba2><button type="button" class="el-button el-button--text " data-v-20f04ba2><!----><!----><span><a href="https://f.oaes.cc/index_ad/flyer/IR-flyer.pdf" target="_blank" data-v-20f04ba2><i class="icon-download font20" data-v-20f04ba2></i> Journal Flyer</a></span></button><!----></div></div></div> <div class="grid-content bg-purple search_box" data-v-20f04ba2><span data-v-20f04ba2><div role="tooltip" id="el-popover-9078" aria-hidden="true" class="el-popover el-popper" style="width:undefinedpx;display:none;"><!----> <!----> <div class="search_hot" data-v-20f04ba2><div class="title" data-v-20f04ba2><span class="text" data-v-20f04ba2>Hot Keywords</span></div> <div class="hot_list" data-v-20f04ba2><span data-v-20f04ba2>Intelligence</span><span data-v-20f04ba2>Robotics</span><span data-v-20f04ba2>Reinforcement Learning</span><span data-v-20f04ba2>Machine Learning</span><span data-v-20f04ba2>Unmanned Vehicles</span><span data-v-20f04ba2>UAV</span></div></div></div><span class="el-popover__reference-wrapper"><div class="el-input el-input--suffix" data-v-20f04ba2><!----><input type="text" autocomplete="off" placeholder="Keywords/Title/Author Name/DOI" class="el-input__inner"><!----><span class="el-input__suffix"><span class="el-input__suffix-inner"><i class="icon-search font24 el-input__icon" data-v-20f04ba2></i><!----><!----><!----><!----></span><!----></span><!----><!----></div></span></span></div></div></div> <div class="head_menu" data-v-20f04ba2><div class="wrapper" data-v-20f04ba2><div class="menu_box" data-v-20f04ba2><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" tabindex="-1" exact="" class="el-menu-item" style="color:;border-bottom-color:transparent;background-color:;" data-v-20f04ba2><a href="/ir" data-v-20f04ba2>Home</a></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" aria-haspopup="true" exact="" class="el-submenu" data-v-20f04ba2><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">About<i class="el-submenu__icon-arrow el-icon-arrow-down"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem" data-v-a0c70e7e data-v-20f04ba2><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/about_the_journal" data-v-a0c70e7e>About the Journal</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/aims_and_scope" data-v-a0c70e7e>Aims and Scope</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/editorial_policies" data-v-a0c70e7e>Editorial Policies</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/editor" data-v-a0c70e7e>Editorial Board</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/awards" data-v-a0c70e7e>Journal Awards</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/news" data-v-a0c70e7e>News</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/history" data-v-a0c70e7e>Journal History</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/partners" data-v-a0c70e7e>Partners</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/advertise" data-v-a0c70e7e>Advertise</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/contact_us" data-v-a0c70e7e>Contact Us</a></li></div></ul></div></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" aria-haspopup="true" exact="" class="el-submenu" data-v-20f04ba2><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">Publish with us<i class="el-submenu__icon-arrow el-icon-arrow-down"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem" data-v-a0c70e7e data-v-20f04ba2><li role="menuitem" aria-haspopup="true" class="el-submenu" data-v-a0c70e7e><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">For Authors<i class="el-submenu__icon-arrow el-icon-arrow-right"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem ts_item" data-v-a0c70e7e data-v-a0c70e7e><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/author_instructions" data-v-a0c70e7e>Author Instructions</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/article_processing_charges" data-v-a0c70e7e>Article Processing Charges</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/editorial_process" data-v-a0c70e7e>Editorial Process</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/manuscript_templates" data-v-a0c70e7e>Manuscript Templates</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="https://www.oaecenter.com/login?JournalId=ir" target="_blank" data-v-a0c70e7e>Submit a Manuscript</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/video_abstract_guidelines" data-v-a0c70e7e>Video Abstract Guidelines</a></li></div></ul></div></li><li role="menuitem" aria-haspopup="true" class="el-submenu" data-v-a0c70e7e><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">For Reviewers<i class="el-submenu__icon-arrow el-icon-arrow-right"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem ts_item" data-v-a0c70e7e data-v-a0c70e7e><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/peer_review_guidelines" data-v-a0c70e7e>Peer Review Guidelines</a></li></div></ul></div></li></div></ul></div></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" aria-haspopup="true" exact="" class="el-submenu" data-v-20f04ba2><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">Articles<i class="el-submenu__icon-arrow el-icon-arrow-down"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem" data-v-a0c70e7e data-v-20f04ba2><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/articles" data-v-a0c70e7e>All Articles</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/articles_videos" data-v-a0c70e7e>Articles With Video Abstracts</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/video_abstract_guidelines" data-v-a0c70e7e>Video Abstract Guidelines</a></li></div></ul></div></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" tabindex="-1" exact="" class="el-menu-item" style="color:;border-bottom-color:transparent;background-color:;" data-v-20f04ba2><a href="/ir/special_collections" data-v-20f04ba2>Special Collections</a></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" aria-haspopup="true" exact="" class="el-submenu" data-v-20f04ba2><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">Special Topics<i class="el-submenu__icon-arrow el-icon-arrow-down"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem" data-v-a0c70e7e data-v-20f04ba2><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/special_issues" data-v-a0c70e7e>All Special Topics</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/ongoing_special_issues" data-v-a0c70e7e>Ongoing Special Topics</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/completed_special_issues" data-v-a0c70e7e>Completed Special Topics</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/closed_special_issues" data-v-a0c70e7e>Closed Special Topics</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/special_issues_ebooks" data-v-a0c70e7e>Special Topic Ebooks</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/special_issue_guidelines" data-v-a0c70e7e>Special Topic Guidelines</a></li></div></ul></div></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" tabindex="-1" exact="" class="el-menu-item" style="color:;border-bottom-color:transparent;background-color:;" data-v-20f04ba2><a href="/ir/volumes" data-v-20f04ba2>Volumes</a></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" tabindex="-1" exact="" class="el-menu-item" style="color:;border-bottom-color:transparent;background-color:;" data-v-20f04ba2><a href="/ir/pre_onlines" data-v-20f04ba2>Pre-onlines</a></li></ul><ul role="menubar" class="el-menu-demo el-menu--horizontal el-menu" style="background-color:;" data-v-20f04ba2><li role="menuitem" aria-haspopup="true" exact="" class="el-submenu" data-v-20f04ba2><div class="el-submenu__title" style="border-bottom-color:transparent;color:;background-color:;">Features<i class="el-submenu__icon-arrow el-icon-arrow-down"></i></div><div class="el-menu--horizontal" style="display:none;"><ul role="menu" class="el-menu el-menu--popup el-menu--popup-" style="background-color:;"> <div class="menuItem" data-v-a0c70e7e data-v-20f04ba2><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/webinars" data-v-a0c70e7e>Webinars</a></li><li role="menuitem" tabindex="-1" class="el-menu-item" style="color:;background-color:;" data-v-a0c70e7e><a href="/ir/interviews" data-v-a0c70e7e>Interviews</a></li></div></ul></div></li></ul></div></div> <div class="wrapper menu_ipad" data-v-20f04ba2><div class="nav_box" data-v-20f04ba2><div class="nav_list colorH_ir" data-v-20f04ba2><a href="/ir" class="tab_item" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Home</span></a> <a href="/ir/articles" class="tab_item nuxt-link-active" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Articles</span></a> <a href="/ir/special_issues" class="tab_item" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Special Topics</span></a> <a href="/ir/volumes" class="tab_item" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Volumes</span></a> <a href="/ir/webinars" class="tab_item" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Webinars</span></a> <a href="/ir/videos" class="tab_item" data-v-20f04ba2><span class="tab_span" data-v-20f04ba2>Videos</span></a></div></div> <button type="button" class="el-button el-button--text" data-v-20f04ba2><!----><i class="icon-nav-line"></i><span>Menu</span></button> <!----></div></div></div></div> <div class="MoComment" data-v-21c7a6ce data-v-0bee1158><div class="head_top" data-v-21c7a6ce><a href="/" class="nuxt-link-active" data-v-21c7a6ce><span class="qk_jx" data-v-21c7a6ce><img src="https://i.oaes.cc/upload/journal_logo/ir.png" alt="" data-v-21c7a6ce></span></a> <div class="head_left" data-v-21c7a6ce><a href="/" class="tab_item nuxt-link-active" data-v-21c7a6ce><span class="title font18" data-v-21c7a6ce>Intelligence &amp; Robotics</span></a> <i class="el-icon-caret-right sjbtn" style="color:rgb(0,71,187);" data-v-21c7a6ce></i></div> <div class="head_right" data-v-21c7a6ce><a href="/ir/search" class="search" data-v-21c7a6ce><span data-v-21c7a6ce>Search</span></a> <span class="go_oae" style="background:rgb(0,71,187);" data-v-21c7a6ce><a href="https://www.oaecenter.com/login?JournalId=ir" target="_blank" data-v-21c7a6ce><span data-v-21c7a6ce>Submit</span></a></span></div></div> <div class="cg" style="height: 50px" data-v-21c7a6ce></div> <div class="fix_box" style="display:none;" data-v-21c7a6ce><div class="miss_right" data-v-21c7a6ce><div class="flex_tit" data-v-21c7a6ce><div class="top_img" data-v-21c7a6ce><a href="" target="_blank" data-v-21c7a6ce><img src="https://i.oaes.cc/uploads/20250623/ff9dcb103f1746dd9c8d2f3ec900479f.png" alt data-v-21c7a6ce></a><a href="https://www.scopus.com/sourceid/21101199351" target="_blank" data-v-21c7a6ce><img src="https://i.oaes.cc/uploads/20250606/5ac28259735646cc836f1a10e3c11d27.png" alt data-v-21c7a6ce></a></div></div> <div class="miss_btn wid70" data-v-21c7a6ce><!----> <div data-v-21c7a6ce><div data-v-21c7a6ce><span class="font_b" data-v-21c7a6ce>Editor-in-Chief:</span> Simon X. Yang</div></div></div> <div class="miss_btn" style="width:calc(100% - 74px);" data-v-21c7a6ce><!----></div> <div data-v-21c7a6ce><div data-v-21c7a6ce><span class="font_b" data-v-21c7a6ce>Journal Rank: </span> <span data-v-21c7a6ce>Impact factor 2.3 -Q2; CiteScore 3.7-Q2</span></div> <div data-v-21c7a6ce><span class="font_b" data-v-21c7a6ce>Median time to first editorial decision: </span>8 days</div></div></div></div> <div class="fix_box" data-v-21c7a6ce><div class="navigation colorH_ir" style="border-bottom:2px solid rgb(0,71,187);" data-v-21c7a6ce><div class="nav_box" data-v-21c7a6ce><div class="nav_list" data-v-21c7a6ce><a href="/ir" class="tab_item" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Home</span></a> <a href="/ir/articles" class="tab_item nuxt-link-active" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Articles</span></a> <a href="/ir/special_issues" class="tab_item" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Special Topics</span></a> <a href="/ir/volumes" class="tab_item" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Volumes</span></a> <a href="/ir/webinars" class="tab_item" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Webinars</span></a> <a href="/ir/videos" class="tab_item" data-v-21c7a6ce><span class="tab_span" data-v-21c7a6ce>Videos</span></a></div></div> <button type="button" class="el-button el-button--text" data-v-21c7a6ce><!----><!----><span><i class="icon-nav-line" data-v-21c7a6ce></i>Menu</span></button></div></div> <!----> <!----></div> <main data-v-0bee1158><div class="article_cont" data-v-5751490a data-v-0bee1158><!----><!----><!----> <div id="ipad_bg" class="ipad_bg" style="display:none;" data-v-5751490a></div> <div class="art_bread wrapper" data-v-5751490a><div aria-label="Breadcrumb" role="navigation" class="el-breadcrumb" data-v-5751490a><span class="el-breadcrumb__item" data-v-5751490a><span role="link" class="el-breadcrumb__inner is-link">Home</span><span role="presentation" class="el-breadcrumb__separator"></span></span> <span class="el-breadcrumb__item" data-v-5751490a><span role="link" class="el-breadcrumb__inner is-link">Articles</span><span role="presentation" class="el-breadcrumb__separator"></span></span> <span class="el-breadcrumb__item" data-v-5751490a><span role="link" class="el-breadcrumb__inner">Article</span><span role="presentation" class="el-breadcrumb__separator"></span></span></div></div> <div class="fixd_top" style="display:none;" data-v-5751490a><div class="left_art" data-v-5751490a><!----></div> <div class="content_b" data-v-5751490a><span class="PcComment" data-v-5751490a>Deep learning-based scene understanding for autonomous robots: a survey</span> <a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022_down.pdf?v=10" data-v-5751490a><span class="down_pdf" data-v-5751490a><span data-v-5751490a>Download PDF</span> <i class="el-icon-download" data-v-5751490a></i></span></a></div> <div class="right_art" data-v-5751490a><!----></div></div> <div class="wrapper pos_res" data-v-5751490a><button id="mathjaxRady" data-v-5751490a></button> <div class="line_list" data-v-5751490a></div> <div id="art_left_b" class="art_content" data-v-5751490a><div class="el-row" style="margin-left:-10px;margin-right:-10px;" data-v-5751490a><div class="el-col el-col-24 el-col-sm-24 el-col-md-18" style="padding-left:10px;padding-right:10px;" data-v-5751490a><div class="art_left" data-v-5751490a><a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022_down.pdf?v=10" class="MoComment" data-v-5751490a><span class="down_pdf_a" data-v-5751490a><span data-v-5751490a>Download PDF</span> <i class="el-icon-download" data-v-5751490a></i></span></a> <!----> <div class="ContentJournal" data-v-5751490a><div id="Article-content-left" class="Article-content view5287" data-v-5751490a><div class="article_block" data-v-5751490a><span class="font-999" data-v-5751490a>Review</span> <span data-v-5751490a> | </span> <span class="block-f17452" data-v-5751490a>Open Access</span> <span data-v-5751490a> | </span> <span class="font-999" data-v-5751490a>14 Aug 2023</span></div> <div class="tit_box mgt30" data-v-5751490a><h1 id="art_title" class="art_title2" data-v-5751490a><span class="title_ir" data-v-5751490a>Deep learning-based scene understanding for autonomous robots: a survey</span><!----></h1> <div class="art_seltte" data-v-5751490a><div class="el-dropdown" style="width:140px;" data-v-5751490a><button type="button" class="el-button el-button--primary" style="width:140px;padding:10px 6px;background:#4475e1 !important;border:1px solid #4475e1 !important;" data-v-5751490a><!----><!----><span><span style="display:flex;align-items:center;justify-content:center;" data-v-5751490a><img src="data:image/png;base64,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" alt="" style="width:24px;margin-right:6px;" data-v-5751490a> <span style="margin-top:-1px;" data-v-5751490a>Translate</span><i class="el-icon-arrow-down el-icon--right" style="margin:0 4px;font-size:18px;" data-v-5751490a></i></span></span></button> <ul class="el-dropdown-menu el-popper" style="display:none;" data-v-5751490a><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            English
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            中文
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Deutsch
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Français
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            日本語
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Русский язык
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            한국어
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Italiano
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Español
                          </div></li><li tabindex="-1" class="el-dropdown-menu__item" style="border-bottom:1px solid #ddd;" data-v-5751490a><!----><div data-v-5751490a>
                            Português
                          </div></li></ul></div></div></div> <div class="viewd_top font13" data-v-5751490a><span class="f1" data-v-5751490a><span style="color:#aa0c2f;" data-v-5751490a>Views:</span> <span id="articleViewCountLeft" data-v-5751490a>2944</span> | </span> <span class="f1" data-v-5751490a><span style="color:#aa0c2f;" data-v-5751490a>Downloads:</span> <span id="pdfDownloadCountLeft" data-v-5751490a>1438</span><span data-v-5751490a> | </span></span> <span class="f1" data-v-5751490a><span style="color:#aa0c2f;" data-v-5751490a>Cited:</span> 
                    <img alt="" src="data:image/png;base64,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" class="Crossref" data-v-5751490a> <a href="/articles//citation/" target="_blank" style="color:#4475e1;margin-left:1px;" data-v-5751490a>37</a> <!----></span></div> <div id="authorString" class="article-authors" data-v-5751490a><span class="authors_item" data-v-5751490a><div affNumList="" data-v-a32d723e data-v-5751490a><span class="pos_re" data-v-a32d723e><div role="tooltip" id="el-popover-8908" aria-hidden="true" class="el-popover el-popper" style="width:300px;display:none;"><!----><h3 class="font16 no_sup" style="color:#333;margin-bottom:20px;" data-v-a32d723e>Jianjun Ni<sup>1,2</sup></h3> <div class="Aff_current font14 no_sup" data-v-a32d723e><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>1</sup></label><addr-line>School of Artificial Intelligence and Automation, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>2</sup></label><addr-line>College of Information Science and Engineering, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div></div> <i class="close_btn el-icon-close" data-v-a32d723e></i>  <a href="https://scholar.google.com/scholar?q=Jianjun Ni" target="_blank" data-v-a32d723e><button type="button" class="el-button el-button--primary el-button--mini" data-v-a32d723e><!----><!----><span>Google Scholar</span></button></a></div><span class="el-popover__reference-wrapper"><span class="author_name" data-v-a32d723e>Jianjun Ni<sup>1,2</sup></span></span></span></div> <a href="http://orcid.org/0000-0002-7130-8331" target="_blank" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/orcid.a3b6f80.png" class="id_img" data-v-5751490a></a> <i data-v-5751490a> </i> <a href="/cdn-cgi/l/email-protection#157b7f7f7d7d6076557278747c793b767a78" data-v-5751490a><i class="iconfont icon-email" data-v-5751490a></i></a> <!----> <i data-v-5751490a> ,  </i></span><span class="authors_item" data-v-5751490a><div affNumList="" data-v-a32d723e data-v-5751490a><span class="pos_re" data-v-a32d723e><div role="tooltip" id="el-popover-2063" aria-hidden="true" class="el-popover el-popper" style="width:300px;display:none;"><!----><h3 class="font16 no_sup" style="color:#333;margin-bottom:20px;" data-v-a32d723e>Yan Chen<sup>1,2</sup></h3> <div class="Aff_current font14 no_sup" data-v-a32d723e><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>1</sup></label><addr-line>School of Artificial Intelligence and Automation, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>2</sup></label><addr-line>College of Information Science and Engineering, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div></div> <i class="close_btn el-icon-close" data-v-a32d723e></i>  <a href="https://scholar.google.com/scholar?q=Yan Chen" target="_blank" data-v-a32d723e><button type="button" class="el-button el-button--primary el-button--mini" data-v-a32d723e><!----><!----><span>Google Scholar</span></button></a></div><span class="el-popover__reference-wrapper"><span class="author_name" data-v-a32d723e>Yan Chen<sup>1,2</sup></span></span></span></div> <a href="https://orcid.org/0000-0003-4969-5723" target="_blank" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/orcid.a3b6f80.png" class="id_img" data-v-5751490a></a> <!----> <!----> <!----> <i data-v-5751490a> , ...  </i></span><span class="authors_item" data-v-5751490a><!----></span><span class="authors_item" data-v-5751490a><!----></span><span class="authors_item" data-v-5751490a><!----></span><span class="authors_item" data-v-5751490a><div affNumList="" data-v-a32d723e data-v-5751490a><span class="pos_re" data-v-a32d723e><div role="tooltip" id="el-popover-6493" aria-hidden="true" class="el-popover el-popper" style="width:300px;display:none;"><!----><h3 class="font16 no_sup" style="color:#333;margin-bottom:20px;" data-v-a32d723e>Pengfei Shi<sup>1,2</sup></h3> <div class="Aff_current font14 no_sup" data-v-a32d723e><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>1</sup></label><addr-line>School of Artificial Intelligence and Automation, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div><div data-v-a32d723e><div class="author_cont" data-v-a32d723e><label><sup>2</sup></label><addr-line>College of Information Science and Engineering, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div></div> <i class="close_btn el-icon-close" data-v-a32d723e></i>  <a href="https://scholar.google.com/scholar?q=Pengfei Shi" target="_blank" data-v-a32d723e><button type="button" class="el-button el-button--primary el-button--mini" data-v-a32d723e><!----><!----><span>Google Scholar</span></button></a></div><span class="el-popover__reference-wrapper"><span class="author_name" data-v-a32d723e>Pengfei Shi<sup>1,2</sup></span></span></span></div> <a href="https://orcid.org/0000-0002-2966-1676" target="_blank" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/orcid.a3b6f80.png" class="id_img" data-v-5751490a></a> <!----> <!----> <!----> <!----></span> <button type="button" class="el-button el-button--primary el-button--mini" data-v-5751490a><!----><i class="el-icon-plus"></i><span>Show Authors</span></button></div> <div class="article-header-info" data-v-5751490a><div data-v-5751490a> <i>Intell Robot</i> 2023;3(3):374-401.</div> <div class="mgt5" data-v-5751490a><a href="https://doi.org/10.20517/ir.2023.22" target="_blank" data-v-5751490a>10.20517/ir.2023.22</a> | 
                    <span class="btn_link" data-v-5751490a>© The Author(s) 2023.</span></div></div> <div class="top_btn_box" data-v-5751490a><div class="btn_item" data-v-5751490a><i class="el-icon-caret-right" data-v-5751490a></i><span data-v-5751490a>Author Information</span></div> <div class="btn_item" data-v-5751490a><i class="el-icon-caret-right" data-v-5751490a></i><span data-v-5751490a>Article Notes</span></div> <div class="btn_item" data-v-5751490a><i class="el-icon-caret-right" data-v-5751490a></i><span data-v-5751490a>Cite This Article</span></div></div> <div class="author_box" style="display:none;" data-v-5751490a><div data-v-5751490a><div data-v-5751490a><label><sup>1</sup></label><addr-line>School of Artificial Intelligence and Automation, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div><div data-v-5751490a><div data-v-5751490a><label><sup>2</sup></label><addr-line>College of Information Science and Engineering, Hohai University, Changzhou 213022, Jiangsu, China.</addr-line></div></div> <div class="CorrsPlus" data-v-5751490a><div data-v-5751490a><span id="cirrsMail" data-v-5751490a>Correspondence to: Prof. Jianjun Ni, School of Artificial Intelligence and Automation, Hohai University, No.200, North Jinling Road, Xinbei District, Changzhou 213022, Jiangsu, China. E-mail: <email><a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="244a4e4e4c4c5147644349454d480a474b49">[email&#160;protected]</a></email>; ORCID: 0000-0002-7130-8331</span></div></div></div> <div class="notes_box" style="display:none;" data-v-5751490a><div class="articleDate mag_top10" data-v-5751490a><span><b>Received:</b> 25 Apr 2023 | </span><span><b>First Decision:</b> 5 Jul 2023 | </span><span><b>Revised:</b> 15 Jul 2023 | </span><span><b>Accepted:</b> 4 Aug 2023 | </span><span><b>Published:</b> 15 Aug 2023</span></div> <div class="articleDate" data-v-5751490a><span><b>Academic Editors:</b> Simon X. Yang, Hongtian Chen | </span><span><b>Copy Editor:</b> Yanbin Bai | </span><span><b>Production Editor:</b> Yanbin Bai</span></div></div> <div class="article_bg" data-v-5751490a><h2 id="art_Abstract" data-v-5751490a>
                      Abstract<!----></h2> <div id="seo_des" class="article_Abstract mag_btn10" data-v-5751490a><p>Autonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed.</p></div> <!----> <h2 id="art_Keywords" data-v-5751490a>
                      Keywords<!----></h2> <div class="article_Abstract" data-v-5751490a><span id="seo_key" data-v-5751490a>Autonomous robots, scene understanding, deep learning, object detection, pose estimation</span></div></div> <div class="MoComment" data-v-5751490a><div class="top_banner" data-v-5751490a><div class="oae_header" data-v-5751490a>
                      Author's Talk
                    </div> <div class="line" data-v-5751490a></div> <div class="img_box" data-v-5751490a><img src="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" alt="" data-itemid="6014" data-itemhref="https://v.oaes.cc/uploads/20230816/a802fc5ba47f4c08ba0a95089779d731.mp4" data-itemimg="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" data-v-5751490a> <i data-itemid="6014" data-itemhref="https://v.oaes.cc/uploads/20230816/a802fc5ba47f4c08ba0a95089779d731.mp4" data-itemimg="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" class="bo_icon" data-v-5751490a></i></div></div> <!----> <div class="article_link" data-v-5751490a><span data-v-5751490a><a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022_down.pdf?v=10" data-v-5751490a><b data-v-5751490a><i class="icon-download icon_right4" data-v-5751490a></i> Download
                          PDF</b></a></span> <span data-v-5751490a><i class="comment-l icon-commentl iconfont icon_right4" data-v-5751490a></i> <!----><b data-v-5751490a>0</b></span> <span data-v-5751490a><span data-v-5751490a><div role="tooltip" id="el-popover-386" aria-hidden="true" class="el-popover el-popper" style="width:170px;display:none;"><!----><div class="icon_share" style="text-align:right;margin:0;" data-v-5751490a><a href="https://pinterest.com/pin/create/button/?url=&amp;media=&amp;description=https://www.oaepublish.com/articles/" target="_blank" class="pinterest-sign" data-v-5751490a><i class="iconfont icon-pinterest" data-v-5751490a></i></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.oaepublish.com/articles/" target="_blank" class="facebook-sign" data-v-5751490a><i aria-hidden="true" class="iconfont icon-facebook" data-v-5751490a></i></a> <a href="https://twitter.com/intent/tweet?url=https://www.oaepublish.com/articles/" target="_blank" class="twitter-sign" data-v-5751490a><i class="iconfont icon-tuite1" data-v-5751490a></i></a> <a href="https://www.linkedin.com/shareArticle?url=https://www.oaepublish.com/articles/" target="_blank" class="linkedin-sign" data-v-5751490a><i class="iconfont icon-linkedin" data-v-5751490a></i></a></div> </div><span class="el-popover__reference-wrapper"><button type="button" class="el-button colorddd el-button--text el-button--mini" data-v-5751490a><!----><!----><span><i class="icon-fenxiang iconfont icon_right4" data-v-5751490a></i> <!----><b data-v-5751490a>2</b></span></button></span></span></span> <span data-v-5751490a><span class="no_zan" data-v-5751490a><i class="icon-like-line icon_right4" data-v-5751490a></i> <!----><i class="num_n" data-v-5751490a><b data-v-5751490a>9</b></i></span></span></div></div> <div id="artDivBox" class="art_cont content_ir" data-v-5751490a><div id="sec11" class="article-Section"><h2 >1. INTRODUCTION</h2><p class="">In recent years, science and technology have developed rapidly, and the applications of autonomous robots become increasingly extensive <sup>[<a href="#b1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b1">1</a>–<a href="#b3" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b3">3</a>]</sup>. With the development of the technologies, the tasks for autonomous robots have become more complicated and challenging. To complete these tasks, one of the main requirements for autonomous robots is the strong capability of the robot to effectively perceive and understand the complicated three-dimensional (3D) environment in which it is positioned.</p><p class="">The ability of an autonomous robot to perceive and understand its own environment, akin to human perception, serves as the foundation for further autonomous interaction with the environment and human users. This problem is also a prominent topic in the field of computer vision, which has made great progress, and lots of research findings have been used for practical applications of autonomous robots. Many research findings in this field are based on two-dimensional (2D) images. However, the real world is a 3D environment, and there remains ample room for future research on the perception and understanding of 3D environments. The environment perception is the basis of scene understanding, which can provide stable and accurate information for scene understanding. On the other hand, scene understanding can provide richer and higher-level information for environment perception. In this paper, we will mainly discuss the scene understanding problems.</p><p class="">There are lots of research results in this field. Nevertheless, a significant portion of current research is focused on more idealized situations. However, the real world is a complicated scene with a number of issues that affect the accuracy of environmental perception and understanding, such as image interference, clutter occlusion, etc. Consequently, it is crucial to study the essential technologies that enable autonomous robots to perceive and comprehend their environment within complex 3D space, addressing both theoretical underpinnings and practical implementation.</p><p class="">This paper provides a survey on the deep learning-based scene understanding for autonomous robots. We provide a brief overview of the research methodologies used to study the perception and comprehension of the robotic environment, and then we concentrate on deep learning-based approaches to these issues. Other relevant surveys in the field of deep learning-based scene understanding can be used as supplements to this paper (see e.g., <sup>[<a href="#b4" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b4">4</a>,<a href="#b5" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b5">5</a>]</sup> and <sup>[<a href="#b6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b6">6</a>,<a href="#b7" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b7">7</a>]</sup>). The main differences between this paper and other surveys lie in its function as an overview of the state-of-the-art approaches in this field, owing to the continuous emergence of new approaches driven by the rapid development of deep learning-based scene understanding. In addition, this paper provides a selection of the latest related works from our research group.</p><p class="">The main contributions of this paper are summarized as follows: (1) The advancement of scene understanding for autonomous robots is thoroughly analyzed and reviewed; (2) A survey on the applications of deep learning methods in scene understanding for autonomous robots is given out; and (3) Some representative deep learning-based methods in the field of autonomous robot scene understanding are analyzed. At last, some possible future study directions in this field are discussed.</p><p class="">This paper is organized as follows. Section 2 provides a summary of the development of autonomous robots and their ability to perceive and comprehend their environment. In Section 3, the key issues of the scene understanding for autonomous robots are analyzed. Additionally, select representative deep learning-based methods based on deep learning techniques in the field of scene understanding are outlined and analyzed. The potential study directions of deep learning-based perception and comprehension of the environment for autonomous robots are given out in Section 4. Finally, conclusions are given out in Section 5.</p></div><div id="sec12" class="article-Section"><h2 >2. BACKGROUND AND SIGNIFICANCE OF THE SCENE UNDERSTANDING</h2><p class="">The global economy has witnessed rapid growth in recent years, paralleled by swift advancements in science and technology. The applications of robots are becoming more and more popular <sup>[<a href="#b8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b8">8</a>]</sup>. Autonomous robots are the representative of advanced technologies, which are the integration of the robotics, information technology, communication technology, and artificial intelligence. These robots have been more integrated into human society, not only creating huge economic benefits for society but also effectively improving individual living standards <sup>[<a href="#b9" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b9">9</a>]</sup>.</p><p class="">The autonomous robot industry is an important standard to evaluate the innovation and high-end manufacturing level of a country. The development of the autonomous robot has attracted growing attention from countries all over the world. A number of famous research institutions and companies across the globe have focused on the realm of autonomous robots.</p><p class="">The representative robotics research institutions include the Robotics and Mechatronics Center (RMC) of the German Aerospace Center, the Computer Science and Artificial Intelligence Laboratory (CSAIL) of Massachusetts Institute of Technology, the Humanoid Robotics Institute (HRI) of Waseda University, Shenyang Institute of Automation Chinese Academy of Sciences, the Robotics Institute of Shanghai Jiaotong University, and so on. There are lots of representative robotic enterprises, such as ABB (Switzerland), KUKA Robotics (Germany), Yaskawa Electric Corporation (Japan), iRobot (USA), AB Precision (UK), Saab Seaeye (Sweden), SIASUN (China), etc <sup>[<a href="#b10" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b10">10</a>]</sup>.</p><p class="">Due to the current technical limitations, the functions of common autonomous robots in daily life are still relatively simple. For example, the serving robot [see <a href="#Figure1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure1">Figure 1A</a>] and the sweeping robot [see <a href="#Figure1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure1">Figure 1B</a>] can only complete some simple tasks, such as moving according to the planned trajectory to the designated position. The expansion of the robot application range requires that the functions of robots are no longer limited to mechanized or programmed operations, narrow human-computer interactions, etc. There is an increasing need for autonomous robots to carry out more difficult tasks. Robots are anticipated to be able to do complicated tasks, such as picking up and dropping off goods or even operating tools autonomously by sensing their surroundings. Empowering autonomous robots with ample environmental perception and a comprehensive understanding of their intricate 3D surroundings stands as an essential prerequisite to satisfy the requirements for these more difficult jobs. For example, the logistics robot can make mobility control decisions after it can autonomously perceive and understand the traffic and road environment [see <a href="#Figure1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure1">Figure 1C</a>]. To operate effectively and securely in the unknown and complex underwater environment, the underwater search robot must be aware of its surroundings [see <a href="#Figure1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure1">Figure 1D</a>].</p><div class="Figure-block" id="Figure1"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure1" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-1.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 1. Applications of scene understanding for autonomous robots: (A) Service robots; (B) Sweeping robots; (C) Logistics robots; (D) Underwater search robots.</p></div></div><p class="">When an autonomous robot conducts a task in a complicated environment, it must first determine its current position and estimate its displacement pose change through a visual Simultaneous Localization and Mapping (SLAM) system. The robot also needs to assess the shape of the environment and comprehend the range of its surroundings. In addition, it is of utmost practical importance to research room layout estimation in complex cluttered environments. Next, the autonomous robot should perform the saliency detection, namely directing its attention toward the regions of interest, akin to human behavior. This is followed by target detection, a crucial step in identifying manipulable items and their locations within the environment. Notably, the study of functional availability detection of objects in 3D space is fundamentally important for robots to further perform complex operational tasks because autonomous robots need to understand the functional availability and even the usage of each part of the object to be interacted with. This facet is closely related to the 3D structure of the object. The main tasks of the scene understanding for the autonomous robot in a complicated environment are shown in <a href="#Figure2" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure2">Figure 2</a>.</p><div class="Figure-block" id="Figure2"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure2" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-2.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 2. The main tasks of the scene understanding for the autonomous robot.</p></div></div><p class="">All of these tasks introduced above are the research topics in the scene understanding of autonomous robots. In a word, scene understanding of autonomous robots is to analyze a scene by considering the geometric and semantic context of its contents and the intrinsic relationships between them. The process mainly involves matching signal information from sensors observing the scene with a model that humans use to understand the scene. On this basis, scene understanding is the semantic extraction and addition of sensor data, which is used to describe the scene for autonomous robots.</p><p class="">In the early research of scene understanding, parts-based representations for object description and scene understanding were the mainstream methods. In these methods, the basic information and hidden deeper information of images are reflected by extracting the low-level and middle-level visual features. And these early methods often realize semantic classification through feature modeling. There are many traditional feature representation methods that have been used widely in scene understanding. Scale-Invariant Feature Transform (SIFT) <sup>[<a href="#b11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b11">11</a>,<a href="#b12" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b12">12</a>]</sup> has rotation, scale, and affine invariant qualities. It has an excellent classification effect even for images with huge scale changes. GIST <sup>[<a href="#b13" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b13">13</a>]</sup> is an image global description feature based on fusing contextual data and a spatial envelope model, which can extract the spatial structural data of pictures using the energy spectrum. LSA (Latent Semantic Analysis) <sup>[<a href="#b14" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b14">14</a>]</sup> is used to address the issue of many words with a single meaning and multiple meanings of a word in text analysis. Other good manually designed features include Speeded Up Robust Features (SURF) <sup>[<a href="#b15" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b15">15</a>]</sup>, Histogram of Oriented Gradient (HOG) <sup>[<a href="#b16" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b16">16</a>,<a href="#b17" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b17">17</a>]</sup>, and so on. Based on these features, a number of traditional image scene semantic classification techniques were developed. For example, Vailaya <i>et al</i>. <sup>[<a href="#b18" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b18">18</a>]</sup> classified scene images using dual features of color moments and texture and derived global features of images in a Bayesian framework. Li <i>et al</i>. <sup>[<a href="#b19" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b19">19</a>]</sup> presented the target library-based image classification technique by decomposing an image into a number of objects and identifying the semantics of each object to realize the semantic classification of images.</p><p class="">With the rapid and substantial growth of hardware computing power, deep learning methods have gained rapid development <sup>[<a href="#b20" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b20">20</a>–<a href="#b22" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b22">22</a>]</sup>. Data-driven methods, especially those based on deep neural networks, have been proven to have outstanding advantages in feature learning and visual data description. The scene understanding of autonomous robots based on deep learning has been developed rapidly. Compared to traditional scene understanding methods, the methods based on deep neural networks can more flexibly use the adaptively extracted features to perform tasks such as object detection, semantic segmentation, and more. As a result, they achieve far better performance.</p><p class="">Environment perception and understanding of autonomous robots in complex 3D scenes is a hot topic both in computer vision and the robotic field. However, there are some differences between normal computer vision and the scene understanding for autonomous robots. Firstly, normal computer vision usually obtains data from static images or videos for analysis and pays more attention to the detection, recognition, and positioning of objects. Scene understanding for autonomous robots usually requires the combination of multiple sensor data and needs to consider the dynamic changes in the environment and the 3D perception and understanding of the environment in order to carry out tasks such as path planning and obstacle avoidance. Furthermore, this process often entails interactions with both the environment and individuals, leading to decision-making based on the interaction output. In contrast, the normal computer vision does not require such interactivity.</p><p class="">A lot of challenging, realistic issues still need to be resolved, and various methods have been used in this field, such as traditional image processing methods, traditional artificial intelligence methods, and so on. Among these methods, deep learning-based methods have achieved great success in this field for their distinct advantages, such as high accuracy, strong robustness, and low cost. This paper will focus on the deep learning-based methods used in the field of scene understanding for autonomous robots.</p></div><div id="sec13" class="article-Section"><h2 >3. DEEP LEARNING FOR SCENE UNDERSTANDING</h2><p class="">Deep neural networks, which serve as the foundational network for image classification, target recognition, image segmentation, target tracking, and video analysis, are used in the deep learning-based vision system. The network parameters are trained through big data, and the feature extraction and classification are realized end-to-end, avoiding complex feature engineering design. Deep learning-based methods have strong feature representation capabilities that can be used to transform the original image into low-level spatial features, middle-level semantic features, and high-level target features. Then, through feature combination, classification and prediction tasks can be achieved efficiently. In addition, learning-based methods have strong generality and make it simpler to complete multi-task learning and multi-modal learning tasks that incorporate video, text, and speech. This helps to advance the development of scene understanding for autonomous robots.</p><p class="">As introduced in Section 2, lots of issues should be solved in the field of scene understanding for autonomous robots. In this section, the detailed applications grounded in various deep learning methods will be introduced. The main applications for scene understanding based on deep learning summarized here stem from the extensive awareness of the authors, which can demonstrate the key issues and the latest advances in the field of scene understanding. The main applications of deep learning in scene understanding include object detection, semantic segmentation, pose estimation, and so on. These applications will be introduced in detail as follows. <a href="#Figure3" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure3">Figure 3</a> shows these deep learning-based models according to the time they are published. To describe easily without loss of generality, we do not distinguish the applications between normal computer vision and scene understanding for autonomous robots in this paper.</p><div class="Figure-block" id="Figure3"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure3" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-3.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 3. Some deep learning-based models in the field of scene understanding in recent years.</p></div></div><p class="">As we know, the datasets are very important for the scene understanding based on deep learning methods. Lots of works of literature have introduced various datasets in different tasks of scene understanding. So, before introducing the main applications of deep learning in this field, the most used datasets in the field of scene understanding are summarized and shown in <a href="#Table1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table1">Table 1</a>.</p><div id="Table1" class="Figure-block"><div class="table-note"><span class="">Table 1</span><p class="">The most used datasets of deep learning in the field of scene understanding</p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center">KITTI<sup>[<a href="#b23" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b23">23</a>]</sup></td><td style="class:table_top_border" align="left">The KITTI dataset contains 7, 481 training samples and 7, 518 test samples divided into three categories (i.e., Car, Pedestrian, and Cyclist). In addition, it is divided into three difficulty levels based on the scale, occlusion, and truncation levels of the objects in the context of autonomous driving (i.e., Easy, Moderate, and Hard)</td></tr></thead><tbody><tr><td style="class:table_top_border2" align="center">nuScenes<sup>[<a href="#b24" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b24">24</a>]</sup></td><td style="class:table_top_border2" align="left">The nuScenes dataset consists of 1000 challenging driving video sequences, each about 20 seconds long, with 30<inline-formula><tex-math id="M1">$$ k $$</tex-math></inline-formula> points per frame. It has 700, 150, and 150 annotated sequences for training, evaluation, and test segmentation, respectively</td></tr><tr><td style="class:table_top_border2" align="center">LINEMOD<sup>[<a href="#b25" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b25">25</a>]</sup></td><td style="class:table_top_border2" align="left">It is a dataset widely used for 6D object pose estimation. There are 13 objects in this dataset. For each object, there are about 1100-1300 images with annotations and only one object with annotation per image</td></tr><tr><td style="class:table_top_border2" align="center">FAST-YCB<sup>[<a href="#b26" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b26">26</a>]</sup></td><td style="class:table_top_border2" align="left">It consists of six realistic synthetic sequences, each containing the fast motion of a single object from the YCB model set in the desktop scene. Each sequence is rendered in bright static lighting conditions and provides <inline-formula><tex-math id="M2">$$ 1280\times720 $$</tex-math></inline-formula> RGB-D frames with accurate ground truth of 6D object pose and velocity</td></tr><tr><td style="class:table_top_border2" align="center">PASCAL VOC 2012<sup>[<a href="#b27" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b27">27</a>]</sup></td><td style="class:table_top_border2" align="left">It is a benchmark dataset that initially contains 1464 images for training, 1449 for validation, and 1456 for testing. In the original PASCAL VOC 2012 dataset, there are a total of 20 foreground object classes and one background class</td></tr><tr><td style="class:table_top_border2" align="center">Cityscapes<sup>[<a href="#b28" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b28">28</a>]</sup></td><td style="class:table_top_border2" align="left">The dataset has 5, 000 images captured from 50 different cities. Each image has <inline-formula><tex-math id="M3">$$ 2048\times10244 $$</tex-math></inline-formula> pixels, which have high-quality pixel-level labels of 19 semantic classes</td></tr><tr><td style="class:table_top_border2" align="center">DHF1K<sup>[<a href="#b29" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b29">29</a>]</sup></td><td style="class:table_top_border2" align="left">It contains the most common and diverse scenarios, with 1000 video samples and no publicly available ground-truth annotations. Only the first 700 annotated maps and videos are available in the DHF1K dataset, and the remaining 300 annotations are reserved for benchmarking</td></tr><tr><td style="class:table_top_border2 table_bottom_border" align="center">VOT-2017<sup>[<a href="#b30" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b30">30</a>]</sup></td><td style="class:table_top_border2 table_bottom_border" align="left">The VOT-2017 dataset can be used for target tracking of different tasks and contains 60 short sequences labeled with six different attributes</td></tr></tbody></table></div><div class="table_footer"></div></div><div id="sec26" class="article-Section"><h3 >3.1. 3D object detection</h3><p class="">Object detection is an image segmentation based on geometric and statistical features of the object. It combines object segmentation and recognition into one task, with the aim of determining the location and class of object appearances. Currently, 2D object detection has been relatively mature, especially with the emergence of Faster Regions with convolutional neural network Features (Faster RCNN), which has brought it to an unprecedented boom. For example, in the previous work of our research group <sup>[<a href="#b31" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b31">31</a>]</sup>, a deep neural network-based SSD framework is proposed to improve the feature representation capability of feature extraction networks. However, in the application scenarios of driverless, robotics, and augmented reality, 2D object detection can only provide the confidence of the position and corresponding category of the object in a 2D image (see <a href="#Figure4" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure4">Figure 4</a>), while the general 2D object detection cannot provide all the information needed for perceiving the environment.</p><div class="Figure-block" id="Figure4"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure4" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-4.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 4. 2D object detection visualization: (A) in the bedroom; (B) in the kitchen <sup>[<a href="#b31" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b31">31</a>]</sup>.</p></div></div><p class="">In the real world, objects have 3D shapes, and most applications require information about the length, width, height, and also the deflection angle of the target object. Therefore, research on methods related to 3D target detection is needed. In scene understanding for autonomous robots, object detection is a critical task to understand the position and class of the objects with which they interact. In real 3D complex scenes, the background information is very rich; therefore, object detection techniques can be used to understand the location and category of interactable objects by giving a 3D rectangular location candidate box and categorizing them according to their attribution possibilities.</p><p class="">3D object detection based on deep learning is a hot research topic in the field of environment perception and understanding. In the deep learning-based model, during the process of making the proposals in regional proposal networks in a bottom-up manner, the resulting proposals somehow deviate from the ground truth and appear densely in local communities. Due to the lack of a corresponding information compensation mechanism, the proposals generated by the general regional proposal networks give up a large amount of boundary information. To deal with this problem, Qian <i>et al</i>. <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup> proposed BADet, a 3D object detection model from point clouds, which can efficiently model the local boundary correlations of objects through local neighborhood graphs and significantly facilitate the complete boundaries of each individual proposal.</p><p class="">BADet consists of three key components, namely, a backbone and region generation network, a region feature aggregation module, and a boundary-aware graph neural network. Its network overview is shown in <a href="#Figure5" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure5">Figure 5</a>.</p><div class="Figure-block" id="Figure5"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure5" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-5.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 5. The network overview of the BADet, where RPN denotes the region proposal network <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup>.</p></div></div><p class="">In the backbone and region proposal network (RPN) of BADet, the original point cloud is voxelized into a volume mesh for multi-scale semantic feature abstraction and 3D proposal generation with the help of the backbone and a series of 3D sparse convolutions. Specifically, let <inline-formula><tex-math id="M4">$$ p $$</tex-math></inline-formula> be a point in a raw point cloud <inline-formula><tex-math id="M5">$$ P $$</tex-math></inline-formula> with 3D coordinates <inline-formula><tex-math id="M6">$$ (p_x , p_y , p_z ) $$</tex-math></inline-formula> and reflectance intensities <inline-formula><tex-math id="M7">$$ p_r $$</tex-math></inline-formula>, then</p><p class=""><div class="disp-formula"><label>(1)</label><tex-math id="E1"> $$ P = \left\{ {p^i = (p_x^i , p_y^i , p_z^i , p_r^i ) \in \Re ^4 , i = 1, 2, \cdots , N} \right\} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M8">$$ N $$</tex-math></inline-formula> indicates the number of points within <inline-formula><tex-math id="M9">$$ P $$</tex-math></inline-formula>. Let <inline-formula><tex-math id="M10">$$ [v_L , v_W , v_H ] \in \Re ^3 $$</tex-math></inline-formula> be the quantization step, then the voxelized coordinates of <inline-formula><tex-math id="M11">$$ p $$</tex-math></inline-formula> can be obtained, namely</p><p class=""><div class="disp-formula"><label>(2)</label><tex-math id="E2"> $$ V_p = \left( {\left\lfloor {\frac{{p_x }}{{v_L }}} \right\rfloor , \left\lfloor {\frac{{p_y }}{{v_W }}} \right\rfloor , \left\lfloor {\frac{{p_z }}{{v_H }}} \right\rfloor } \right) $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M12">$$ \left\lfloor {\cdot} \right\rfloor $$</tex-math></inline-formula> is the floor function. Therefore, the point cloud <inline-formula><tex-math id="M13">$$ P $$</tex-math></inline-formula> can be positioned into a feature map with a resolution of <inline-formula><tex-math id="M14">$$ L \times W \times H $$</tex-math></inline-formula>, subject to the quantization step <inline-formula><tex-math id="M15">$$ [v_L , v_W , v_H ] $$</tex-math></inline-formula>.</p><p class="">In the region feature aggregation module of BADet, multi-level semantic features are leveraged to obtain more informative RoI-wise representations. In the boundary-aware graph neural network, neighboring 3D proposals are used as inputs for graph construction within a given cutoff distance. Specifically, the local neighborhood graph <inline-formula><tex-math id="M16">$$ G(V, E) $$</tex-math></inline-formula> can be constructed as</p><p class=""><div class="disp-formula"><label>(3)</label><tex-math id="E3"> $$ E = \left\{ {(i, j)\left|\; {\left\| {x_i - x_j } \right\|_2 &lt; r} \right.} \right\} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M17">$$ V $$</tex-math></inline-formula> and <inline-formula><tex-math id="M18">$$ E $$</tex-math></inline-formula> are the nodes and edges, respectively; <inline-formula><tex-math id="M19">$$ r $$</tex-math></inline-formula> is the threshold; and <inline-formula><tex-math id="M20">$$ x_i $$</tex-math></inline-formula> denotes the 3D coordinates of a node of graph <inline-formula><tex-math id="M21">$$ G $$</tex-math></inline-formula>.</p><p class="">In <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup>, an overall loss <inline-formula><tex-math id="M22">$$ L $$</tex-math></inline-formula> is used, namely</p><p class=""><div class="disp-formula"><label>(4)</label><tex-math id="E4"> $$ L = L_{rpn} + L_{gnn} + L_{offset} + L_{seg} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M23">$$ L_{rpn} $$</tex-math></inline-formula> and <inline-formula><tex-math id="M24">$$ L_{gnn} $$</tex-math></inline-formula> are Focal Loss and Smooth-L1 Loss for the bounding box classification and regression, respectively; <inline-formula><tex-math id="M25">$$ L_{offset} $$</tex-math></inline-formula> is the center offset estimation loss, which is used to obtain better boundary-aware voxelwise representations, and <inline-formula><tex-math id="M26">$$ L_{seg} $$</tex-math></inline-formula> is the foreground segmentation loss.</p><p class="">To evaluate the performance of BADet, some comparison experiments are conducted on the KITTI and nuScenes datasets. The results of BADet are listed in <a href="#Table2" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table2">Table 2</a>.</p><div id="Table2" class="Figure-block"><div class="table-note"><span class="">Table 2</span><p class="">The results of BADet on KITTI test server and nuScenes dataset <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td align="center" style="class:table_top_border"></td><td colspan="3" align="center" style="class:table_top_border"><b>KITTI test server</b></td><td colspan="2" align="center" style="class:table_top_border"><b>nuScenes dataset</b></td></tr><tr><td align="center"></td><td align="center" style="class:table_top_border2"><b>Easy</b></td><td align="center" style="class:table_top_border2"><b>Moderate</b></td><td align="center" style="class:table_top_border2"><b>Hard</b></td><td align="center"></td><td align="center"></td></tr></thead><tfoot><tr><td align="left" colspan="6"><inline-formula><tex-math id="M27">$$AP_{3D}$$</tex-math></inline-formula> and <inline-formula><tex-math id="M28">$$AP_{BEV}$$</tex-math></inline-formula> mean the Average Precision (<i>AP</i>) with 40 recall positions on both BEV (Bird's Eye View) and 3D object detection leaderboard; <i>mAP</i> and <i>NDS</i> denote the mean Average Precision and nuScenes detection score, respectively.</td></tr></tfoot><tbody><tr><td align="center" style="class:table_top_border2"><inline-formula><tex-math id="M29">$$ AP_{3D} (\%) $$</tex-math></inline-formula></td><td align="center" style="class:table_top_border2">89.28</td><td align="center" style="class:table_top_border2">81.61</td><td align="center" style="class:table_top_border2">76.58</td><td align="center" style="class:table_top_border2"><inline-formula><tex-math id="M30">$$ mAP (\%) $$</tex-math></inline-formula></td><td align="center" style="class:table_top_border2">47.65</td></tr><tr><td align="center" style="class:table_bottom_border"><inline-formula><tex-math id="M31">$$ AP_{BEV} (\%) $$</tex-math></inline-formula></td><td align="center" style="class:table_bottom_border">95.23</td><td align="center" style="class:table_bottom_border">91.32</td><td align="center" style="class:table_bottom_border">86.48</td><td align="center" style="class:table_bottom_border"><inline-formula><tex-math id="M32">$$ NDS (\%) $$</tex-math></inline-formula></td><td align="center" style="class:table_bottom_border">58.84</td></tr></tbody></table></div><div class="table_footer"></div></div><p class="">The results in <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup> show that BADet outperforms all its competitors with remarkable margins on KITTI BEV detection leaderboard and ranks 1st in "Car" category of moderate difficulty.</p><p class="">3D object detection methods have developed rapidly with the development of deep learning techniques. In recent years, many scholars have been exploring new results in this field. For example, Shi <i>et al</i>. <sup>[<a href="#b33" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b33">33</a>]</sup> proposed the Part-A2 net to implement the 3D object detection using only LiDAR point cloud data. Li <i>et al</i>. <sup>[<a href="#b34" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b34">34</a>]</sup> proposed the TGNet, a new graph convolution structure, that can effectively learn expressive and compositional local geometric features from point clouds.</p><p class="">According to the type of input data, 3D object detection can be divided into single-modal methods and multi-modal methods. Single-modal 3D object detection refers to the use of data collected by one kind of sensor as input. The advantage of single-modal 3D target detection is that the input data are simple and the processing flow is clear; the disadvantage is that the input data may not be sufficient to describe the target information in 3D space. Multi-modal 3D object detection refers to the use of multiple data collected by multiple types of sensors as inputs. The advantage of multi-modal 3D target detection is that the input data are rich, and the complementarity of different modal data can be utilized to improve the accuracy and robustness of the detection. The disadvantage is the complexity of the input data and the need to deal with inconsistencies between different modal data. In the following, a summary of the deep learning-based 3D object detection models presented in the last five years is illustrated in <a href="#Table3" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table3">Table 3</a>, where the type of the input data of each method is given out.</p><div id="Table3" class="Figure-block"><div class="table-note"><span class="">Table 3</span><p class="">A summary of the deep learning-based 3D object detection models presented in the last five years</p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Structure</b></td><td style="class:table_top_border" align="center"><b>Reference</b></td><td style="class:table_top_border" align="center"><b>Input data type</b></td><td style="class:table_top_border" align="left"><b>Performances</b></td></tr></thead><tfoot><tr><td align="left" colspan="4"><i>AP</i> means the average precision. <i>MIoU</i>: the Mean Intersection over Union.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">SECOND</td><td style="class:table_top_border2" align="center">Yan <i>et al</i>. (2018) <sup>[<a href="#b35" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b35">35</a>]</sup></td><td style="class:table_top_border2" align="center">Single-modal</td><td style="class:table_top_border2" align="left"><inline-formula><tex-math id="M33">$$ AP $$</tex-math></inline-formula> of 83.13% on KITTI test set</td></tr><tr><td align="center">F-pointNet</td><td align="center">Qi <i>et al</i>. (2018) <sup>[<a href="#b36" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b36">36</a>]</sup></td><td align="center">Multi-modal</td><td align="left"><inline-formula><tex-math id="M34">$$ AP $$</tex-math></inline-formula> of 81.20% on KITTI test set</td></tr><tr><td align="center">F-ConvNet</td><td align="center">Wang <i>et al</i>. (2019) <sup>[<a href="#b37" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b37">37</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M35">$$ AP $$</tex-math></inline-formula> of 85.88% on KITTI test set</td></tr><tr><td align="center">Fast Point R-CNN</td><td align="center">Chen <i>et al</i>. (2019) <sup>[<a href="#b38" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b38">38</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M36">$$ AP $$</tex-math></inline-formula> of 84.28% on KITTI test set</td></tr><tr><td align="center">SA-SSD</td><td align="center">He <i>et al</i>. (2020) <sup>[<a href="#b39" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b39">39</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M37">$$ AP $$</tex-math></inline-formula> of 88.75% on KITTI test set</td></tr><tr><td align="center">TANet</td><td align="center">Liu <i>et al</i>. (2020) <sup>[<a href="#b40" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b40">40</a>]</sup></td><td align="center">Single-modal</td><td align="left">3D <inline-formula><tex-math id="M38">$$ mAP $$</tex-math></inline-formula> of 62.00% on KITTI test set</td></tr><tr><td align="center">TGNet</td><td align="center">Li <i>et al</i>. (2020) <sup>[<a href="#b34" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b34">34</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M39">$$ MIoU $$</tex-math></inline-formula> of 68.17% on Paris-Lille-3D datasets</td></tr><tr><td align="center">CenterPoint</td><td align="center">Yin <i>et al</i>. (2021) <sup>[<a href="#b41" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b41">41</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M40">$$ mAP $$</tex-math></inline-formula> of 58.0% on nuScenes test set</td></tr><tr><td align="center">Part-A2</td><td align="center">Shi <i>et al</i>. (2021) <sup>[<a href="#b33" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b33">33</a>]</sup></td><td align="center">Multi-modal</td><td align="left"><inline-formula><tex-math id="M41">$$ AP $$</tex-math></inline-formula> of 85.94% on KITTI test set</td></tr><tr><td align="center">RGBNet</td><td align="center">Wang <i>et al</i>. (2022) <sup>[<a href="#b42" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b42">42</a>]</sup></td><td align="center">Multi-modal</td><td align="left"><inline-formula><tex-math id="M42">$$ mAP $$</tex-math></inline-formula> of 70.2% on ScanNetV2 val set</td></tr><tr><td align="center">BADet</td><td align="center">Qian <i>et al</i>. (2022) <sup>[<a href="#b32" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b32">32</a>]</sup></td><td align="center">Single-modal</td><td align="left"><inline-formula><tex-math id="M43">$$ AP $$</tex-math></inline-formula> of 89.28% on KITTI test set</td></tr><tr><td style="class:table_bottom_border" align="center">DCLM</td><td style="class:table_bottom_border" align="center">Chen <i>et al</i>. (2023) <sup>[<a href="#b43" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b43">43</a>]</sup></td><td style="class:table_bottom_border" align="center">Multi-modal</td><td style="class:table_bottom_border" align="left"><inline-formula><tex-math id="M44">$$ mAP $$</tex-math></inline-formula> of 65.6% on SUN RGB-D dataset</td></tr></tbody></table></div><div class="table_footer"></div></div></div><div id="sec27" class="article-Section"><h3 >3.2. Pose estimation</h3><p class="">Pose estimation is a crucial component of autonomous robot technology. The pose estimation task deals with finding the position and orientation of an object with respect to a specific coordinate system. The vision-based pose estimation approaches employ a number of feature extraction techniques to obtain the spatial positional information of the target from the image.</p><p class="">There are two classical methods for pose estimation, namely, the feature-based techniques and the template matching methods. The traditional feature-based technique primarily extracts features from images and creates a relationship between the 2D pixel points and 3D coordinate points in space. The differences in lighting and background complexity have a significant impact on the feature extraction process. In addition, the feature-based methods struggle to handle sparse target texture features. The template matching method can effectively solve the pose estimation problem for the targets with weak texture features in images. However, the accuracy of the template matching method is determined by the number of samples in the template library. While its accuracy improves with the number of the template libraries, it also causes a decrease in problem-solving efficiency, making it unable to meet real-time requirements.</p><p class="">The development of deep learning has influenced pose estimation, and there are numerous study findings in this field. For example, Hoang <i>et al</i>. <sup>[<a href="#b44" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b44">44</a>]</sup> proposed the Voting and Attention-based model, which enhances the accuracy of object pose estimation by learning higher-level characteristics from the dependencies between the individual components of the object and object instances. The structure of this Voting and Attention-based network is shown in <a href="#Figure6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure6">Figure 6</a>.</p><div class="Figure-block" id="Figure6"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure6" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-6.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 6. Network architecture of the Voting and Attention-based model <sup>[<a href="#b44" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b44">44</a>]</sup>.</p></div></div><p class="">As shown in <a href="#Figure6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure6">Figure 6</a>, there are four main parts in the Voting and Attention-based model, namely the feature extraction module based on PointNet++ architecture, part proposals learning module (<inline-formula><tex-math id="M45">$$ \text{M}_{\text{p}} $$</tex-math></inline-formula>), object proposals learning module (<inline-formula><tex-math id="M46">$$ \text{M}_{\text{o}} $$</tex-math></inline-formula>), and the voting module in both <inline-formula><tex-math id="M47">$$ \text{M}_{\text{p}} $$</tex-math></inline-formula> and <inline-formula><tex-math id="M48">$$ \text{M}_{\text{o}} $$</tex-math></inline-formula> based on VoteNet.</p><p class="">In the <inline-formula><tex-math id="M49">$$ \text{M}_{\text{p}} $$</tex-math></inline-formula> module of the Voting and Attention-based model, the higher-order interactions between the proposed features can be explicitly modeled, which is formulated as non-local operations:</p><p class=""><div class="disp-formula"><label>(5)</label><tex-math id="E5"> $$ 	H_{part-part}=f\left( \theta\left( H\right) \phi\left( H\right) \right) g\left( H\right) . $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M50">$$ \theta(\cdot), \phi(\cdot), and g(\cdot) $$</tex-math></inline-formula> are the learnable transformation on the input feature map <inline-formula><tex-math id="M51">$$ H $$</tex-math></inline-formula>, and <inline-formula><tex-math id="M52">$$ f(\cdot) $$</tex-math></inline-formula> is the encoding function of the relationship between any two parts.</p><p class="">In addition, the compact generalized non-local network (CGNL) <sup>[<a href="#b45" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b45">45</a>]</sup> is used as the self-attentive module in <inline-formula><tex-math id="M53">$$ \text{M}_{\text{p}} $$</tex-math></inline-formula>. Specifically, the CGNL-based self-attentive module takes <inline-formula><tex-math id="M54">$$ K $$</tex-math></inline-formula> clusters <inline-formula><tex-math id="M55">$$ C=(C_1, C_2, ...C_K) $$</tex-math></inline-formula> as input. Then, votes from each cluster are processed by the Multi-Layer Perceptron (MLP) and passed to CGNL. The self-attention mechanism allows features from different clusters to interact with each other and find out who they should pay more attention to.</p><p class="">Similarly, in the <inline-formula><tex-math id="M56">$$ \text{M}_{\text{o}} $$</tex-math></inline-formula> module of the Voting and Attention-based model, the instance-to-instance correlation is modeled. Firstly, <inline-formula><tex-math id="M57">$$ K $$</tex-math></inline-formula> clusters from the high-dimensional features and a set of object centers are generated. Then, CGNL is used to model the rich interdependencies between clusters in feature space. The output is a new feature mapping:</p><p class=""><div class="disp-formula"><label>(6)</label><tex-math id="E6"> $$ 	H_{obj-obj}=CGNL\left( max\left(MLP\left( v_i\right) \right) \right) , i=1, \ldots, n $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M58">$$ v_i $$</tex-math></inline-formula> is the <inline-formula><tex-math id="M59">$$ i $$</tex-math></inline-formula>-th vote.</p><p class="">Finally, the new feature maps <inline-formula><tex-math id="M60">$$ H_{part-part} $$</tex-math></inline-formula> and <inline-formula><tex-math id="M61">$$ H_{obj-obj} $$</tex-math></inline-formula> are aggregated to the global information by an MLP layer after a max-pooling and concatenation operations.</p><p class="">In the Voting and Attention-based model, a multi-task loss is used for joint learning, namely</p><p class=""><div class="disp-formula"><label>(7)</label><tex-math id="E7"> $$ 	L=\lambda_1L_{part-vote}+\lambda_2L_{object-vote}+\lambda_3L_{pose} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M62">$$ \lambda_1 $$</tex-math></inline-formula>, <inline-formula><tex-math id="M63">$$ \lambda_2 $$</tex-math></inline-formula>, and <inline-formula><tex-math id="M64">$$ \lambda_3 $$</tex-math></inline-formula> are the weights of each task. The losses include voting partial loss <inline-formula><tex-math id="M65">$$ L_{part-vote} $$</tex-math></inline-formula>, object voting loss <inline-formula><tex-math id="M66">$$ L_{object-vote} $$</tex-math></inline-formula>, and pose loss <inline-formula><tex-math id="M67">$$ L_{pose} $$</tex-math></inline-formula>.</p><p class="">The pose estimation results based on the Voting and Attention-based model on nine objects in the Sil<inline-formula><tex-math id="M68">$$ \acute{e} $$</tex-math></inline-formula>ane dataset and two objects in the Fraunhofer IPA dataset are listed in <a href="#Table4" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table4">Table 4</a>, and some qualitative results are shown in <a href="#Figure7" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure7">Figure 7</a>.</p><div id="Table4" class="Figure-block"><div class="table-note"><span class="">Table 4</span><p class="">The pose estimation results based on the Voting and Attention-based model on nine objects in the Sil<inline-formula><tex-math id="M69">$$ \acute{e} $$</tex-math></inline-formula>ane dataset and two objects in the Fraunhofer IPA dataset <sup>[<a href="#b44" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b44">44</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td align="center" style="class:table_top_border"><b>Objects</b></td><td colspan="9" align="center" style="class:table_top_border"><b>Siléane dataset</b></td><td colspan="2" align="center" style="class:table_top_border"><b>Fraunhofer IPA dataset</b></td><td align="center" style="class:table_top_border"><b>Mean</b></td></tr></thead><tbody><tr><td align="center"></td><td align="center" style="class:table_top_border2">Brick</td><td align="center" style="class:table_top_border2">Bunny</td><td align="center" style="class:table_top_border2">C. stick</td><td align="center" style="class:table_top_border2">C.cup</td><td align="center" style="class:table_top_border2">Gear</td><td align="center" style="class:table_top_border2">Pepper</td><td align="center" style="class:table_top_border2">Tless 20</td><td align="center" style="class:table_top_border2">Tless 22</td><td align="center" style="class:table_top_border2">Tless 29</td><td align="center" style="class:table_top_border2">Gear shaft</td><td align="center" style="class:table_top_border2">Ring screw</td><td align="center"></td></tr><tr><td align="center" style="class:table_bottom_border"><inline-formula><tex-math id="M70">$$ AP $$</tex-math></inline-formula></td><td align="center" style="class:table_bottom_border">0.48</td><td align="center" style="class:table_bottom_border">0.61</td><td align="center" style="class:table_bottom_border">0.60</td><td align="center" style="class:table_bottom_border">0.52</td><td align="center" style="class:table_bottom_border">0.64</td><td align="center" style="class:table_bottom_border">0.39</td><td align="center" style="class:table_bottom_border">0.44</td><td align="center" style="class:table_bottom_border">0.37</td><td align="center" style="class:table_bottom_border">0.46</td><td align="center" style="class:table_bottom_border">0.65</td><td align="center" style="class:table_bottom_border">0.67</td><td align="center" style="class:table_bottom_border">0.53</td></tr></tbody></table></div><div class="table_footer"></div></div><div class="Figure-block" id="Figure7"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure7" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-7.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 7. Visualization for pose estimation results based on the Voting and Attention-based model: (A) 3D point cloud input; (B) True values of the poses; (C) Results obtained by the method in <sup>[<a href="#b44" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b44">44</a>]</sup>. The different color means the visualization of point-wise distance error, ranging from 0 (green) to greater than 0.2 times the diameter of the object (red).</p></div></div><p class="">The results in <a href="#Table4" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table4">Table 4</a> and <a href="#Figure7" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure7">Figure 7</a> show that the Voting and Attention model is very effective in improving the accuracy of the pose estimation, which can obtain an average precision of 53%.</p><p class="">In addition to the above voting-based model, there are lots of research results in pose estimation based on deep learning methods. For example, Chen <i>et al</i>. <sup>[<a href="#b46" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b46">46</a>]</sup> presented a probabilistic PnP (EPro-PnP) model for general end-to-end pose estimation, which is based on the method of locating 3D objects from a single RGB image via Perspective-n-Points (PnP). The EPro-PnP model can realize reliable end-to-end training for a PnP-based object pose estimation network by back-propagating the probability density of the pose to learn the 2D-3D association of the object.</p><p class="">Currently, there are five main types of methods for pose estimation, including feature-based methods, regression-based methods, projection-based methods, representation learning methods, and graph neural network methods. The feature-based method refers to restoring camera pose by establishing feature correspondence between images and scenes. A regression-based method uses a regressor to predict the camera pose. A projection-based method utilizes projection transformation to estimate the pose of a target from an image or video. A representation learning method utilizes deep neural networks to learn high-resolution representations of objects from images or videos, which can improve the accuracy and interpretability of pose estimation. Graph neural network methods use graph neural networks to learn structured representations of objects from images or videos, which can improve robustness of pose estimation. In the following, a summary of the deep learning-based pose estimation models presented in the last five years is illustrated in <a href="#Table5" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table5">Table 5</a>, where the type of each method is given out.</p><div id="Table5" class="Figure-block"><div class="table-note"><span class="">Table 5</span><p class="">A summary of the deep learning-based poss estimation models in the last five years</p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Structure</b></td><td style="class:table_top_border" align="center"><b>Reference</b></td><td style="class:table_top_border" align="center"><b>Type of the method</b></td><td style="class:table_top_border" align="left"><b>Performances</b></td></tr></thead><tfoot><tr><td align="left" colspan="4"><i>ADD</i> means average distance metric. <i>ADD-AUC</i> means area under the curve.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">V2V-PoseNet</td><td style="class:table_top_border2" align="center">Moon <i>et al</i>. (2018) <sup>[<a href="#b47" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b47">47</a>]</sup></td><td style="class:table_top_border2" align="center">Regression-based</td><td style="class:table_top_border2" align="left">Top1 in the HANDS 2017 frame-based dataset.</td></tr><tr><td align="center">CDPN</td><td align="center">Li <i>et al</i>. (2019) <sup>[<a href="#b48" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b48">48</a>]</sup></td><td align="center">Feature-based</td><td align="left">ADD of 89.86% on the LINEMOD dataset</td></tr><tr><td align="center">NOCS</td><td align="center">Wang <i>et al</i>. (2019) <sup>[<a href="#b49" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b49">49</a>]</sup></td><td align="center">Projection-based</td><td align="left"><inline-formula><tex-math id="M71">$$ mAP $$</tex-math></inline-formula> of 88.4% for 3D IoU on Occluded LINEMOD dataset</td></tr><tr><td align="center">DPVL</td><td align="center">Yu <i>et al</i>. (2020) <sup>[<a href="#b50" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b50">50</a>]</sup></td><td align="center">Representation learning</td><td align="left">Mean <inline-formula><tex-math id="M72">$$ ADD $$</tex-math></inline-formula> of 91.5% on the LINEMOD dataset</td></tr><tr><td align="center">G2L-Net</td><td align="center">Chen <i>et al</i>. (2020) <sup>[<a href="#b51" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b51">51</a>]</sup></td><td align="center">Graph neural network</td><td align="left">Mean <inline-formula><tex-math id="M73">$$ ADD $$</tex-math></inline-formula> of 98.7% on the LINEMOD dataset</td></tr><tr><td align="center">PVN3D</td><td align="center">He <i>et al</i>. (2020) <sup>[<a href="#b52" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b52">52</a>]</sup></td><td align="center">Projection-based</td><td align="left"><inline-formula><tex-math id="M74">$$ ADD $$</tex-math></inline-formula> of 99.4% on the LineMOD dataset</td></tr><tr><td align="center">FFB6D</td><td align="center">He <i>et al</i>. (2021) <sup>[<a href="#b53" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b53">53</a>]</sup></td><td align="center">Feature-based</td><td align="left">Mean <inline-formula><tex-math id="M75">$$ ADD $$</tex-math></inline-formula> of 99.7% on the LINEMOD dataset</td></tr><tr><td align="center">ROFT</td><td align="center">Piga <i>et al</i>. (2022) <sup>[<a href="#b26" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b26">26</a>]</sup></td><td align="center">Feature-based</td><td align="left"><inline-formula><tex-math id="M76">$$ ADD-AUC $$</tex-math></inline-formula> of 76.59% on the FAST-YCB dataset</td></tr><tr><td align="center">Voting and Attention</td><td align="center">Hoang <i>et al</i>. (2022) <sup>[<a href="#b44" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b44">44</a>]</sup></td><td align="center">Feature-based</td><td align="left"><inline-formula><tex-math id="M77">$$ AP $$</tex-math></inline-formula> of 53% on the Siléane dataset and Fraunhofer IPA dataset</td></tr><tr><td style="class:table_bottom_border" align="center">EPro-PnP</td><td style="class:table_bottom_border" align="center">Chen <i>et al</i>. (2022) <sup>[<a href="#b46" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b46">46</a>]</sup></td><td style="class:table_bottom_border" align="center">Projection-based</td><td style="class:table_bottom_border" align="left"><inline-formula><tex-math id="M78">$$ ADD $$</tex-math></inline-formula> of 95.80% on the LineMOD Dataset</td></tr></tbody></table></div><div class="table_footer"></div></div></div><div id="sec28" class="article-Section"><h3 >3.3. Semantic segmentation</h3><p class="">Semantic segmentation is a refined version of image classification. For an image, traditional image classification is to detect and recognize the objects that appear in the image, while semantic segmentation is to classify every pixel point in the image. In the field of autonomous robot environment perception and understanding, semantic segmentation is used to label each pixel in an image with its corresponding semantically meaningful category. Semantic segmentation can help robots recognize and understand surrounding objects and scenes. It is very useful for the semantic segmentation for the robot to find a specific object in the environment. For example, in the field of logistics robotics, semantic segmentation can help the autonomous robots perceive and understand road conditions, traffic signs, pedestrians, and vehicles, which can improve the safety and efficiency of logistics robotics.</p><p class="">The traditional semantic segmentation algorithms are mainly grayscale segmentation, conditional random fields, etc. Grayscale segmentation algorithms recursively segment images into sub-regions until labels can be assigned and then combine adjacent sub-regions with the same labels by merging them. The conditional random field is a type of statistical modeling method for structured prediction.</p><p class="">With the continuous development of deep learning techniques, deep learning has been widely applied in semantic segmentation tasks and achieved impressive results. There are a series of classical deep learning-based models for semantic segmentation, such as Full convolution network (FCN) <sup>[<a href="#b54" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b54">54</a>]</sup>, SegNet <sup>[<a href="#b55" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b55">55</a>]</sup>, DeepLab series <sup>[<a href="#b56" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b56">56</a>,<a href="#b57" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b57">57</a>]</sup>, RefineNet <sup>[<a href="#b58" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b58">58</a>]</sup>, DenseASPP <sup>[<a href="#b59" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b59">59</a>]</sup>, etc. Recently, some improvements have been proposed based on those classical models. For example, Yin <i>et al</i>. <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup> presented a multi-sensor fusion for lane marking semantic segmentation (FusionLane) based on the DeepLabV3+ network. The workflow of the FusionLane model is shown in <a href="#Figure8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure8">Figure 8</a>.</p><div class="Figure-block" id="Figure8"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure8" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-8.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 8. The workflow of the FusionLane model <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup>. CBEV denotes the camera bird's eye view data. LBEV denotes the points cloud bird's eye view data. C-Region denotes the obtained semantic segmentation result on CBEV.</p></div></div><p class="">As shown in <a href="#Figure8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure8">Figure 8</a>, firstly, the DeepLabV3+ network is used to achieve semantic segmentation on camera BEV (CBEV) data (called as C-Region). Then, the C-Region and LiDAR point cloud BEV (LBEV) data are input into the FusionLane model to realize the lane marking semantic segmentation. Unlike other methods that mainly focus on the analysis of camera images, the semantic segmentation data used in FusionLane is a BEV image converted from the LiDAR point cloud instead of the images captured by the camera to obtain the accurate location information of the segmentation results.</p><p class="">The network contains two data input branches: the camera data and the point cloud data. The data from the two branches need to be preprocessed to meet the network input requirements. For the camera data, the front view is converted into CBEV. In CBEV, one pixel represents an area of <inline-formula><tex-math id="M79">$$ 5cm\times 5cm $$</tex-math></inline-formula> in real space. Then, the CBEV image is semantically segmented using the trained DeepLabV3+ network to obtain the C-Region input data. For the point cloud data, it is projected into the 3D BEV with three channels. The values of the three channels are calculated as follows:</p><p class=""><div class="disp-formula"><label>(8)</label><tex-math id="E8"> $$ 	F\left( x, y\right) =\frac{\sum\nolimits_1^ni}{n}\times255 $$ </tex-math></div></p><p class=""><div class="disp-formula"><label>(9)</label><tex-math id="E9"> $$ 	S\left( x, y\right) =\frac{\sum\nolimits_1^n\left( h+2\right) }{n}\times255 $$ </tex-math></div></p><p class=""><div class="disp-formula"><label>(10)</label><tex-math id="E10"> $$ 	T\left( x, y\right) =255\times\frac{2}{\pi}\times arctan\sqrt{\frac{\sum\nolimits_1^n\left( h-\sum\nolimits_1^n\frac{h}{n}\right) ^2}{n}} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M80">$$ F\left( x, y\right) $$</tex-math></inline-formula>, <inline-formula><tex-math id="M81">$$ S\left( x, y\right) $$</tex-math></inline-formula>, and <inline-formula><tex-math id="M82">$$ T\left( x, y\right) $$</tex-math></inline-formula> denote the values of the first channel, the second channel, and the third channel, respectively; <inline-formula><tex-math id="M83">$$ i \in [0, 1] $$</tex-math></inline-formula> is the reflection intensity value of each point falling within the grid corresponding to the pixel; <inline-formula><tex-math id="M84">$$ h \in [ - 2, - 1] $$</tex-math></inline-formula> is the height value of each laser spot falling within the grid, and <inline-formula><tex-math id="M85">$$ arctan $$</tex-math></inline-formula> is used as the normalization function.</p><p class="">In the FusionLane model of <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup>, an encoder-decoder network model is proposed, and the LSTM structure is added to the network to assist the semantic segmentation of the lane marking. At last, the KITTI dataset is used to test the performance of the FusionLane model, which is processed and divided into seven categories. The experimental results are listed in <a href="#Table6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table6">Table 6</a>, and some segmentation results based on the FusionLane network are shown in <a href="#Figure9" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure9">Figure 9</a>.</p><div id="Table6" class="Figure-block"><div class="table-note"><span class="">Table 6</span><p class="">Some comparison experiment results of the semantic segmentation on the KITTI dataset <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Methods</b></td><td style="class:table_top_border" align="center"><b>Background</b></td><td style="class:table_top_border" align="center"><b>Solid line</b></td><td style="class:table_top_border" align="center"><b>Dotted line</b></td><td style="class:table_top_border" align="center"><b>Arrow</b></td><td style="class:table_top_border" align="center"><b>Prohibited area</b></td><td style="class:table_top_border" align="center"><b>Stop line</b></td><td style="class:table_top_border" align="center"><b>Other point</b></td><td style="class:table_top_border" align="center"><b>MIoU</b></td><td style="class:table_top_border" align="center"><b>PA</b> (%)</td></tr></thead><tfoot><tr><td align="left" colspan="10"><i>IoU</i>: the evaluation metrics include the Intersection over Union on each category; <i>MIoU</i>: the Mean Intersection over Union; <i>PA</i>: the Pixel Accuracy.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">DeepLabv3+(LBEV)</td><td style="class:table_top_border2" align="center">0.9419</td><td style="class:table_top_border2" align="center">0.2587</td><td style="class:table_top_border2" align="center">0.2648</td><td style="class:table_top_border2" align="center">0.2793</td><td style="class:table_top_border2" align="center">0.1915</td><td style="class:table_top_border2" align="center">0.3586</td><td style="class:table_top_border2" align="center">0.2770</td><td style="class:table_top_border2" align="center">0.3674</td><td style="class:table_top_border2" align="center">91.31</td></tr><tr><td align="center">DeepLabv3+(CBEV)</td><td align="center">0.9106</td><td align="center">0.6287</td><td align="center">0.7012</td><td align="center">0.5821</td><td align="center">0.6935</td><td align="center">0.5294</td><td align="center">-</td><td align="center">0.6743</td><td align="center">85.76</td></tr><tr><td style="class:table_bottom_border" align="center">FusionLane</td><td style="class:table_bottom_border" align="center"><b>1.0000</b></td><td style="class:table_bottom_border" align="center"><b>0.7477</b></td><td style="class:table_bottom_border" align="center"><b>0.7838</b></td><td style="class:table_bottom_border" align="center"><b>0.7526</b></td><td style="class:table_bottom_border" align="center"><b>0.7979</b></td><td style="class:table_bottom_border" align="center"><b>0.9053</b></td><td style="class:table_bottom_border" align="center"><b>0.9867</b></td><td style="class:table_bottom_border" align="center"><b>0.8535</b></td><td style="class:table_bottom_border" align="center"><b>99.92</b></td></tr></tbody></table></div><div class="table_footer"></div></div><div class="Figure-block" id="Figure9"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure9" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-9.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 9. The segmentation results based on the FusionLane network for some scenarios <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup>.</p></div></div><p class="">The results in <a href="#Table6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table6">Table 6</a> show that DeepLabV3+ has a low <inline-formula><tex-math id="M86">$$ IoU $$</tex-math></inline-formula> for all scenarios except "Background". However, it can be seen that the FusionLane model achieves the best results in all metrics compared to the traditional DeepLabV3+ model. The results in <a href="#Table6" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table6">Table 6</a> and <a href="#Figure9" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure9">Figure 9</a> show that relying on a single kind of sensor, whether camera or LiDAR, cannot give sufficiently accurate semantic segmentation results. Effective fusion of data from different sensors can be considered a viable approach to solving the problem.</p><p class="">In addition to the above DeepLab-based model, there are lots of good semantic segmentation models based on deep learning methods. For example, Hu <i>et al</i>. <sup>[<a href="#b61" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b61">61</a>]</sup> presented the FANet model, which is based on an improved self-attention mechanism, to capture the rich spatial context at a small computational cost. Sun <i>et al</i>. <sup>[<a href="#b62" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b62">62</a>]</sup> proposed the FuseSeg model, a new RGB and thermal data fusion network, to achieve superior semantic segmentation performance in urban scenes.</p><p class="">More and more scholars have researched many results in this field. According to the type of network structure, semantic segmentation can be divided into encoder-decoder structure, attention mechanism, graph neural network, generative adversarial network (GAN), and transfer learning. The semantic segmentation method based on encoder-decoder utilizes the encoder-decoder structure to learn and predict the semantic category of each pixel from an image. The method based on GAN uses a generator and a discriminator to conduct confrontation learning. Attention mechanism is a technique that simulates the process of human visual attention. It can calculate the correlation between different positions or channels, give different weights, and highlight the parts of interest while suppressing irrelevant parts. A graph neural network is a deep neural network that can process graph-structured data, which can update the features of nodes and edges through graph convolution operations. Transfer learning is a machine learning technology that can use the knowledge of one domain (source domain) to help the learning of another domain (target domain), thus reducing the dependence on the labeled data of the target domain. A summary of the deep learning-based 3D semantic segmentation models presented in the last five years is illustrated in <a href="#Table7" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table7">Table 7</a>, where the type of the network structure of each method is given out.</p><div id="Table7" class="Figure-block"><div class="table-note"><span class="">Table 7</span><p class="">A summary of the deep learning-based models of semantic segmentation in the last five years</p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Structure</b></td><td style="class:table_top_border" align="center"><b>Reference</b></td><td style="class:table_top_border" align="center"><b>Network structure</b></td><td style="class:table_top_border" align="left"><b>Performances</b></td></tr></thead><tbody><tr><td style="class:table_top_border2" align="center">DenseASPP</td><td style="class:table_top_border2" align="center">Yang <i>et al</i>. (2018) <sup>[<a href="#b63" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b63">63</a>]</sup></td><td style="class:table_top_border2" align="center">Encoder-decoder</td><td style="class:table_top_border2" align="left"><inline-formula><tex-math id="M87">$$ MIoU $$</tex-math></inline-formula> score of 80.6% on Cityscapes datasets</td></tr><tr><td align="center">EncNet</td><td align="center">Zhang <i>et al</i>. (2018) <sup>[<a href="#b64" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b64">64</a>]</sup></td><td align="center">Attention mechanism</td><td align="left"><inline-formula><tex-math id="M88">$$ MIoU $$</tex-math></inline-formula> score of 85.9% on PASCAL VOC 2012</td></tr><tr><td align="center">DANet</td><td align="center">Fu <i>et al</i>. (2019) <sup>[<a href="#b65" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b65">65</a>]</sup></td><td align="center">Graph neural network</td><td align="left"><inline-formula><tex-math id="M89">$$ MIoU $$</tex-math></inline-formula> score of 81.5% on Cityscapes test set</td></tr><tr><td align="center">APCNet</td><td align="center">He <i>et al</i>. (2019) <sup>[<a href="#b66" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b66">66</a>]</sup></td><td align="center">Attention mechanism</td><td align="left">A new record 84.2% on PASCAL VOC 2012 test set</td></tr><tr><td align="center">CANet</td><td align="center">Zhang <i>et al</i>. (2019) <sup>[<a href="#b67" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b67">67</a>]</sup></td><td align="center">Attention mechanism</td><td align="left"><inline-formula><tex-math id="M90">$$ MIoU $$</tex-math></inline-formula> score of 57.1% on PASCAL-5i test set</td></tr><tr><td align="center">EfficientFCN</td><td align="center">Liu <i>et al</i>. (2020) <sup>[<a href="#b68" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b68">68</a>]</sup></td><td align="center">GAN</td><td align="left"><inline-formula><tex-math id="M91">$$ MIoU $$</tex-math></inline-formula> score of 55.3% on PASCAL Context test set</td></tr><tr><td align="center">FuseSeg</td><td align="center">Sun <i>et al</i>. (2021) <sup>[<a href="#b62" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b62">62</a>]</sup></td><td align="center">Encoder-decoder</td><td align="left"><inline-formula><tex-math id="M92">$$ MIoU $$</tex-math></inline-formula> score of 54.5% on the dataset released in <sup>[<a href="#b69" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b69">69</a>]</sup></td></tr><tr><td align="center">MaskFormer</td><td align="center">Cheng <i>et al</i>. (2021) <sup>[<a href="#b70" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b70">70</a>]</sup></td><td align="center">Transformer learning</td><td align="left"><inline-formula><tex-math id="M93">$$ MIoU $$</tex-math></inline-formula> score of 55.6% on the ADE20K dataset</td></tr><tr><td align="center">FANet</td><td align="center">Hu <i>et al</i>. (2021) <sup>[<a href="#b61" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b61">61</a>]</sup></td><td align="center">Encoder-decoder</td><td align="left"><inline-formula><tex-math id="M94">$$ MIoU $$</tex-math></inline-formula> score of 75.5% on Cityscapes test set</td></tr><tr><td align="center">FusionLane</td><td align="center">Yin <i>et al</i>. (2022) <sup>[<a href="#b60" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b60">60</a>]</sup></td><td align="center">Encoder-decoder</td><td align="left"><inline-formula><tex-math id="M95">$$ MIoU $$</tex-math></inline-formula> score of 85.35% on KITTI test set</td></tr><tr><td style="class:table_bottom_border" align="center">BCINet</td><td style="class:table_bottom_border" align="center">Zhou <i>et al</i>. (2023) <sup>[<a href="#b71" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b71">71</a>]</sup></td><td style="class:table_bottom_border" align="center">Encoder-decoder</td><td style="class:table_bottom_border" align="left"><inline-formula><tex-math id="M96">$$ MIoU $$</tex-math></inline-formula> score of 52.95% on the NYUv2 dataset</td></tr></tbody></table></div><div class="table_footer"></div></div></div><div id="sec29" class="article-Section"><h3 >3.4. Saliency prediction</h3><p class="">The human visual system selectively attends to salient parts of a scene and performs a detailed understanding of the most salient regions. The detection of salient regions corresponds to important objects and events in a scene and their mutual relationships. In the field of scene understanding for autonomous robots, the task of the saliency prediction is to mimic the characteristics of human vision to focus on obvious or interested targets by acquiring 3D environment information containing color and depth through sensors. In detail, the saliency prediction needs to identify and segment the most salient objects from the acquired 3D environment information and pay attention to the focal objects.</p><p class="">The traditional saliency prediction problem is commonly known as the task of capturing rare and unique elements from images. Traditionally, salient prediction methods can be classified into three types: (1) Block-based detection models. In this type of method, the linear subspace method is used instead of actual image segmentation, and the significant regions are selected by measuring the feature pair ratio and geometric properties of the region. (2) Region-based detection models. This type of method divides the image into multiple regions, and the saliency of each region is regarded as the sum of the product of its contrast and the weight of all the other regions. (3) Detection model based on external cues of the image. This model utilizes accurate annotations (ground-truth) obtained from the training set, video sequences, similar images, and other sources to make the results more accurate. The performance of the saliency prediction based on similar images will be improved if a large number of data sets are available. In general, the traditional methods use a large amount of saliency a priori information for saliency detection, mainly relying on hand-crafted features. These hand-crafted features have some shortcomings; for example, they may not be able to describe complex image scenes and object structures, cannot adapt to new scenes and objects, and have poor generalization ability. So, the saliency detection based on traditional methods has hit a bottleneck.</p><p class="">Recently, deep learning-based methods have been used widely in various image tasks (e.g., target detection, semantic segmentation, edge detection, etc.), which provide new ideas for saliency prediction and show surprising effect enhancement in some studies. For example, Lou <i>et al</i>. <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup> proposed the TranSalNet network model. Its basic workflow is shown in <a href="#Figure10" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure10">Figure 10</a>.</p><div class="Figure-block" id="Figure10"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure10" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-10.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 10. The schematic overview of the TranSalNet network <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup>.</p></div></div><p class="">As shown in <a href="#Figure10" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure10">Figure 10</a>, the convolutional neural network (CNN)-based encoding is used to extract features for saliency prediction. The outputs of the CNN encoding are three sets of multi-scale feature maps with <inline-formula><tex-math id="M97">$$ \frac{w}{8} \times \frac{h}{8} $$</tex-math></inline-formula>, <inline-formula><tex-math id="M98">$$ \frac{w}{16} \times \frac{h}{16} $$</tex-math></inline-formula>, and <inline-formula><tex-math id="M99">$$ \frac{w}{32} \times \frac{h}{32} $$</tex-math></inline-formula>, respectively. Then, these feature maps are input into the transformer encoders to enhance the long-range and contextual information. At last, a CNN decoder is used to fuse the enhanced feature maps from the three transformer encoders. The CNN decoder used in <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup> is a full CNN network with seven blocks. The processes from block1 to block6 are as follows:</p><p class=""><div class="disp-formula"><label>(11)</label><tex-math id="E11"> $$ X_i^f = \left\{ \begin{array}{l} X_i^c , \begin{array}{*{20}c} {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {\begin{array}{*{20}c} {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {i = 1} \hfill \\ \end{array}} \hfill \\ \end{array} \\ {\rm{ReLU(Upsamle(\hat X}}_{i - 1}^f {\rm{)}} \odot X_i^c {\rm{), }}\begin{array}{*{20}c} {} &amp; {} &amp; {i = 2, 3} \\ \end{array} \\ {\rm{Upsamle(\hat X}}_{i - 1}^f {\rm{)}}, \begin{array}{*{20}c} {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {} \hfill &amp; {i = 4, 5, 6} \hfill \\ \end{array} \\ \end{array} \right. $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M100">$$ X_i^f $$</tex-math></inline-formula> and <inline-formula><tex-math id="M101">$$ {\hat X}_{i}^f $$</tex-math></inline-formula> are the input and output of the <inline-formula><tex-math id="M102">$$ i $$</tex-math></inline-formula>-th block. The output of the block7 <inline-formula><tex-math id="M103">$$ \hat y $$</tex-math></inline-formula> is the predicted saliency map, namely</p><p class=""><div class="disp-formula"><label>(12)</label><tex-math id="E12"> $$ \hat y = {\rm{Sigmoid}}({\rm{Conv}}_{3 \times 3} (X_6^f )) $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M104">$$ \rm{Sigmoid}(\cdot) $$</tex-math></inline-formula> is the sigmoid activation function; <inline-formula><tex-math id="M105">$$ {\rm{Conv}}_{3 \times 3} $$</tex-math></inline-formula> denotes the <inline-formula><tex-math id="M106">$$ 3 \times 3 $$</tex-math></inline-formula> convolution operation; and <inline-formula><tex-math id="M107">$$ X_6^f $$</tex-math></inline-formula> is the output of the block6.</p><p class="">In the TranSalNet network model, a linear combination of four losses is used as the loss function, namely</p><p class=""><div class="disp-formula"><label>(13)</label><tex-math id="E13"> $$ L = \omega _1 L_{NSS} + \omega _2 L_{KLD} + \omega _3 L_{CC} + \omega _4 L_{SIM} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M108">$$ L_{NSS} $$</tex-math></inline-formula> is the Normalized Scanpath Saliency loss; <inline-formula><tex-math id="M109">$$ L_{KLD} $$</tex-math></inline-formula> is the Kullback–Leibler divergence loss; <inline-formula><tex-math id="M110">$$ L_{CC} $$</tex-math></inline-formula> is the Linear Correlation Coefficient loss; and <inline-formula><tex-math id="M111">$$ L_{SIM} $$</tex-math></inline-formula> is the Similarity loss. <inline-formula><tex-math id="M112">$$ \omega _1 $$</tex-math></inline-formula>, <inline-formula><tex-math id="M113">$$ \omega _2 $$</tex-math></inline-formula>, <inline-formula><tex-math id="M114">$$ \omega _3 $$</tex-math></inline-formula>, and <inline-formula><tex-math id="M115">$$ \omega _4 $$</tex-math></inline-formula> are the weights of each loss.</p><p class="">Some results of the TranSalNet network model are listed in <a href="#Table8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table8">Table 8</a>, where TranSalNet_Res and TranSalNet_Dense denote the CNN encoders used in the TranSalNet network, ResNet_50 and DenseNet_161, respectively. Here, two public datasets are as follows: (1) MIT1003 <sup>[<a href="#b73" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b73">73</a>]</sup>: This dataset contains 300 natural images and eye movement data from 39 observers and is the most influential and widely used dataset in the field of image human eye focus detection. (2) CAT2000 <sup>[<a href="#b74" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b74">74</a>]</sup>: This dataset includes 4000 images, 200 in each of 20 categories, covering different types of scenes such as cartoon, art, object, low-resolution image, indoor, outdoor, chaotic, random, and line drawings. Some saliency maps generated by the two models are shown in <a href="#Figure11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure11">Figure 11</a>.</p><div id="Table8" class="Figure-block"><div class="table-note"><span class="">Table 8</span><p class="">Some results of the TranSalNet network model on MIT1003 and CAT2000 datasets <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" rowspan="3" align="center"><b>Model name</b></td><td style="class:table_top_border" colspan="6" align="center"><b>MIT1003</b></td><td style="class:table_top_border" colspan="6" align="center"><b>CAT2000</b></td></tr><tr><td style="class:table_top_border2" colspan="3" align="center"><b>Perception metrics</b></td><td style="class:table_top_border2" colspan="3" align="center"><b>Non-perception metrics</b></td><td style="class:table_top_border2" colspan="3" align="center"><b>Perception metrics</b></td><td style="class:table_top_border2" colspan="3" align="center"><b>Non-perception metrics</b></td></tr><tr><td style="class:table_top_border2" align="center"><b>CC</b></td><td style="class:table_top_border2" align="center"><b>SIM</b></td><td style="class:table_top_border2" align="center"><b>NSS</b></td><td style="class:table_top_border2" align="center"><b>sAUC</b></td><td style="class:table_top_border2" align="center"><b>AUC</b></td><td style="class:table_top_border2" align="center"><b>KLD</b></td><td style="class:table_top_border2" align="center"><b>CC</b></td><td style="class:table_top_border2" align="center"><b>SIM</b></td><td style="class:table_top_border2" align="center"><b>NSS</b></td><td style="class:table_top_border2" align="center"><b>sAUC</b></td><td style="class:table_top_border2" align="center"><b>AUC</b></td><td style="class:table_top_border2" align="center"><b>KLD</b></td></tr></thead><tbody><tr><td style="class:table_top_border2" align="center">TranSalNet-Res</td><td style="class:table_top_border2" align="center">0.7595</td><td style="class:table_top_border2" align="center">0.6145</td><td style="class:table_top_border2" align="center">2.8501</td><td style="class:table_top_border2" align="center">0.7546</td><td style="class:table_top_border2" align="center">0.9093</td><td style="class:table_top_border2" align="center">0.7779</td><td style="class:table_top_border2" align="center">0.8786</td><td style="class:table_top_border2" align="center">0.7492</td><td style="class:table_top_border2" align="center">2.4154</td><td style="class:table_top_border2" align="center">0.6054</td><td style="class:table_top_border2" align="center">0.8811</td><td style="class:table_top_border2" align="center">0.5036</td></tr><tr><td style="class:table_bottom_border" align="center">TranSalNet-Dense</td><td style="class:table_bottom_border" align="center">0.7743</td><td style="class:table_bottom_border" align="center">0.6279</td><td style="class:table_bottom_border" align="center">2.9214</td><td style="class:table_bottom_border" align="center">0.7547</td><td style="class:table_bottom_border" align="center">0.9116</td><td style="class:table_bottom_border" align="center">0.7862</td><td style="class:table_bottom_border" align="center">0.8823</td><td style="class:table_bottom_border" align="center">0.7512</td><td style="class:table_bottom_border" align="center">2.4290</td><td style="class:table_bottom_border" align="center">0.6099</td><td style="class:table_bottom_border" align="center">0.8820</td><td style="class:table_bottom_border" align="center">0.4715</td></tr></tbody></table></div><div class="table_footer"></div></div><div class="Figure-block" id="Figure11"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure11" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-11.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 11. Results of saliency maps generated by TranSalNet_Res and TranSalNet_Dense <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup>. The images from (A) to (C) are from the MIT1003 dataset, and the images from (D) to (F) are from the CAT2000 dataset.</p></div></div><p class="">The results in <a href="#Table8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table8">Table 8</a> and <a href="#Figure11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure11">Figure 11</a> prove that the TranSalNet architecture presented in <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup> is effective in the saliency prediction tasks. In addition, the results in <a href="#Table8" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table8">Table 8</a> and <a href="#Figure11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure11">Figure 11</a> show that the performance of the TranSalNet could be further enhanced by replacing ResNet-50 with DenseNet-161.</p><p class="">In addition to the above TranSalNet model, there are other saliency prediction models based on deep learning, which also have obtained good results in this field. For example, Zou <i>et al</i>. <sup>[<a href="#b75" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b75">75</a>]</sup> proposed the STA3D model, where the S3D network is used as an encoder and the prediction network with spatial dimensional upsampling and temporal dimensional compression is used as a decoder, to solve the difficulty of video significance prediction in the continuous frame with a fixed offset.</p><p class="">At present, there are five types of methods for saliency prediction, including gradient-based methods, perturbation-based methods, SHAP value-based methods, attention mechanism-based methods, and transfer learning-based methods. The gradient-based method utilizes the gradient information of neural networks to calculate the contribution of each pixel in the input image to the output saliency map. The perturbation-based method evaluates the importance of each pixel by randomly or regularly perturbing the input image. The method based on SHAP values utilizes shapely additive explanations to quantify the impact of each pixel on the output saliency map. The saliency prediction, based on attention mechanisms, utilizes an attention mechanism to simulate the process of human visual attention, thereby improving the accuracy and interpretability of saliency prediction. Transfer learning is used to solve the problem of data shortage and domain differences in saliency prediction, which can improve the generalization ability and adaptability of saliency prediction. A summary of the deep learning-based models in the last five years is illustrated in <a href="#Table9" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table9">Table 9</a>, where the type of each method is given out.</p><div id="Table9" class="Figure-block"><div class="table-note"><span class="">Table 9</span><p class="">A summary of the deep learning-based saliency prediction models in the last five years</p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Structure</b></td><td style="class:table_top_border" align="center"><b>Reference</b></td><td style="class:table_top_border" align="center"><b>Type of methods</b></td><td style="class:table_top_border" align="left"><b>Performances</b></td></tr></thead><tfoot><tr><td align="left" colspan="4"><i>MAE</i> means mean absolute error. <inline-formula><tex-math id="M116">$$AUC_{J}$$</tex-math></inline-formula> means the area under the receiver operating characteristic curve. <i>EAO</i> means expected average overlap.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">ASNet</td><td style="class:table_top_border2" align="center">Wang <i>et al</i>. (2018) <sup>[<a href="#b76" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b76">76</a>]</sup></td><td style="class:table_top_border2" align="center">Gradient-based</td><td style="class:table_top_border2" align="left"><inline-formula><tex-math id="M117">$$ MAE $$</tex-math></inline-formula> scores of 0.072 on the PASCAL-S dataset</td></tr><tr><td align="center">RGB-D-SOD</td><td align="center">Huang <i>et al</i>. (2019) <sup>[<a href="#b77" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b77">77</a>]</sup></td><td align="center">Perturbation-based</td><td align="left"><inline-formula><tex-math id="M118">$$ AUC $$</tex-math></inline-formula> of 0.874 on the NJU400 dataset</td></tr><tr><td align="center">AF-RGB-D</td><td align="center"><i>et al</i>. (2019) <sup>[<a href="#b78" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b78">78</a>]</sup></td><td align="center">SHAP value-based methods</td><td align="left"><inline-formula><tex-math id="M119">$$ MAE $$</tex-math></inline-formula> scores of 0.0462 on the STEREO dataset</td></tr><tr><td align="center">CMP-SOI</td><td align="center">Zhang <i>et al</i>. (2020) <sup>[<a href="#b79" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b79">79</a>]</sup></td><td align="center">Gradient-based</td><td align="left"><inline-formula><tex-math id="M120">$$ AUC_{J} $$</tex-math></inline-formula> of 0.8839 on the ODI dataset</td></tr><tr><td align="center">DevsNet</td><td align="center">Fang <i>et al</i>. (2020) <sup>[<a href="#b80" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b80">80</a>]</sup></td><td align="center">Gradient-based</td><td align="left"><inline-formula><tex-math id="M121">$$ MAE $$</tex-math></inline-formula> scores of 0.016 on the UVSD dataset</td></tr><tr><td align="center">AMDFNet</td><td align="center">Li <i>et al</i>. (2021) <sup>[<a href="#b81" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b81">81</a>]</sup></td><td align="center">Gradient-based</td><td align="left"><inline-formula><tex-math id="M122">$$ MAE $$</tex-math></inline-formula> scores of 0.019 on the RGBD135 dataset</td></tr><tr><td align="center">SSPNet</td><td align="center">Lee <i>et al</i>. (2021) <sup>[<a href="#b82" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b82">82</a>]</sup></td><td align="center">Gradient-based</td><td align="left"><inline-formula><tex-math id="M123">$$ EAO $$</tex-math></inline-formula> of 0.285 on the VOT-2017 dataset</td></tr><tr><td align="center">STA3D</td><td align="center">Zou <i>et al</i>. (2021) <sup>[<a href="#b75" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b75">75</a>]</sup></td><td align="center">Gradient-based</td><td align="left"><inline-formula><tex-math id="M124">$$ AUC_{J} $$</tex-math></inline-formula> of 0.927 on the Hollywood2-actions dataset</td></tr><tr><td align="center">ECANet</td><td align="center">Xue <i>et al</i>. (2022) <sup>[<a href="#b83" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b83">83</a>]</sup></td><td align="center">Attention mechanism-based</td><td align="left"><inline-formula><tex-math id="M125">$$ AUC_{J} $$</tex-math></inline-formula> of 0.903 on the DHF1K dataset</td></tr><tr><td style="class:table_bottom_border" align="center">TranSalNet</td><td style="class:table_bottom_border" align="center">Lou <i>et al</i>. (2022) <sup>[<a href="#b72" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b72">72</a>]</sup></td><td style="class:table_bottom_border" align="center">Transformer learning-based</td><td style="class:table_bottom_border" align="left"><inline-formula><tex-math id="M126">$$ AUC $$</tex-math></inline-formula> of 0.9116 on the MIT1003 dataset</td></tr></tbody></table></div><div class="table_footer"></div></div></div><div id="sec210" class="article-Section"><h3 >3.5. Other applications</h3><p class="">In addition to the applications mentioned above, there are many other applications of deep learning methods in autonomous robot environment perception and understanding, such as image enhancement <sup>[<a href="#b84" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b84">84</a>,<a href="#b85" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b85">85</a>]</sup>, visual SLAM <sup>[<a href="#b1" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b1">1</a>,<a href="#b86" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b86">86</a>]</sup>, scene classification <sup>[<a href="#b87" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b87">87</a>,<a href="#b88" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b88">88</a>]</sup>, moving object detection <sup>[<a href="#b89" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b89">89</a>,<a href="#b90" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b90">90</a>]</sup>, and layout estimation <sup>[<a href="#b91" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b91">91</a>,<a href="#b92" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b92">92</a>]</sup>. In this section, some recent jobs of our group related to this review will be introduced in detail as follows.</p><div id="sec32" class="article-Section"><h4 >3.5.1. Visual SLAM</h4><p class="">When a robot enters an unknown environment, vision SLAM technology can be used to solve the problem of the robots about where they are. It estimates the current position, pose, and travel trajectory of the robot in the 3D scene by the changes in the visual data acquired during the robot's travel. In order to implement vision SLAM, there are three main methods: feature-based methods, direct methods, and semi-direct methods.</p><p class="">With feature-based visual SLAM methods, feature points are found and matched. Then, the poses of robots are calculated, and maps are built from geometric relationships. Scalar Transformation (SIFT) <sup>[<a href="#b11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b11">11</a>]</sup>, Accelerated Robust Feature (SURF) <sup>[<a href="#b93" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b93">93</a>]</sup>, and Fast Rotational Abbreviation (ORB) <sup>[<a href="#b94" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b94">94</a>]</sup> are the most frequently used feature extraction techniques. The most widely used method for visual SLAM is ORB-SLAM <sup>[<a href="#b95" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b95">95</a>,<a href="#b96" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b96">96</a>]</sup>. To overcome the problem of high computational complexity in the traditional ORB-SLAM, Fu <i>et al</i>. <sup>[<a href="#b97" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b97">97</a>]</sup> proposed the Fast ORB-SLAM that is light-weight and efficient as it tracks keypoints between adjacent frames without computing descriptors.</p><p class="">Direct methods do not rely on one-to-one matching of points. These types of methods minimize the photometric error function of the pixels by extracting pixels with significant gradients and optimizing the inter-frame pose. The classical direct methods include Large Scale Direct Monocular SLAM (LSD-SLAM) <sup>[<a href="#b98" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b98">98</a>]</sup>, Direct Sparse Range (DSO) <sup>[<a href="#b99" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b99">99</a>]</sup>, etc. Recently, Wang <i>et al</i>. <sup>[<a href="#b100" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b100">100</a>]</sup> introduced a new ceiling-view visual odometry method that introduces plane constraints as additional conditions and achieves better accuracy.</p><p class="">Semi-direct methods, such as SVO <sup>[<a href="#b101" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b101">101</a>]</sup>, employ a similar structure to the feature-based methods, which combine the tracking of the direct method with the motion optimization of feature-based methods. Both feature-based and semi-direct methods rely on highly repeatable low-level geometric feature extractors. Both of them are inappropriate for surfaces with little texture or many repetitive features.</p><p class="">Direct methods, on the other hand, can be applied to a wider variety of scenes. However, compared to feature-based methods, direct methods are less robust. The performance of the direct visual SLAM system under the influence of various camera imaging perturbations will be reduced obviously. To deal with this problem, our group proposed an improved Direct Sparse Odometry with Loop Closure (LDSO) method <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup>, which is shown in <a href="#Figure12" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure12">Figure 12</a>.</p><div class="Figure-block" id="Figure12"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure12" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-12.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 12. The framework of the improved LDSO method based on the variable radius side window <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup>.</p></div></div><p class="">In the framework of the improved LDSO shown in <a href="#Figure12" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure12">Figure 12</a>, the region surrounding each pixel is divided into blocks when a new frame is introduced, using the side window approach. Then, a CNN structure is created by this multiple-layer superposition of pixel information fusion <sup>[<a href="#b22" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b22">22</a>,<a href="#b103" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b103">103</a>]</sup>. The middle layer shows the presence of semi-static items. In the later layers, the radius of the side windows of the pixels belonging to the semi-static objects is increased. Points with an adequate gradient intensity and corners are chosen using dynamic grid searches. The robustness of the system is increased by the addition of points in direct SLAM. To accomplish edge protection, the fusion method is used with a side window mechanism. Finally, to lessen the weight of semi-static objects, the radius of the adjustment side windows is modified in accordance with the semantic information based on a pre-trained Yolov5 model <sup>[<a href="#b104" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b104">104</a>]</sup>.</p><p class="">In the experiments to test the performance of the improved LDSO in <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup>, two public datasets are used: the KITTI dataset (outdoor datasets) and the TUM RGB-D dataset (indoor datasets). To test the improved LDSO under different camera sensor noises, Gaussian noise and Salt-and-Pepper noise are added to the two datasets. Some results of visual SLAM based on the improved LDSO are shown in <a href="#Table10" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table10">Table 10</a> and <a href="#Table11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table11">Table 11</a>, where RMSE<inline-formula><tex-math id="M127">$$ _{\rm{ATE}} $$</tex-math></inline-formula> means the root mean squared error of absolute trajectory error. The comparison results on the KITTI dataset with Salt-and-Pepper noise are not given out because the general LDSO is entirely inoperable on the datasets under Salt-and-Pepper noise. The results of the point cloud map constructed on the sequence 'KITTI_07' in the KITTI dataset are shown in <a href="#Figure13" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure13">Figure 13</a>. The results in <a href="#Table10" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table10">Tables 10</a> and <a href="#Table11" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table11">11</a> show that the improved LDSO in <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup> can work efficiently in both the indoor and the outdoor datasets under different noises, while the general LDSO will fail to track (see <a href="#Figure13" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure13">Figure 13</a>).</p><div id="Table10" class="Figure-block"><div class="table-note"><span class="">Table 10</span><p class="">RMSE<inline-formula><tex-math id="M128">$$ _{\rm{ATE}} $$</tex-math></inline-formula> on the KITTI dataset with Gaussian noise <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" rowspan="2" align="center"><b>Method</b></td><td style="class:table_top_border" colspan="12" align="center"><b>Gaussian noise</b></td></tr><tr><td style="class:table_top_border2" align="center"><b>KITTI_00</b></td><td style="class:table_top_border2" align="center"><b>KITTI_01</b></td><td style="class:table_top_border2" align="center"><b>KITTI_02</b></td><td style="class:table_top_border2" align="center"><b>KITTI_03</b></td><td style="class:table_top_border2" align="center"><b>KITTI_04</b></td><td style="class:table_top_border2" align="center"><b>KITTI_05</b></td><td style="class:table_top_border2" align="center"><b>KITTI_06</b></td><td style="class:table_top_border2" align="center"><b>KITTI_07</b></td><td style="class:table_top_border2" align="center"><b>KITTI_08</b></td><td style="class:table_top_border2" align="center"><b>KITTI_09</b></td><td style="class:table_top_border2" align="center"><b>KITTI_10</b></td><td style="class:table_top_border2" align="center"><b>Average</b></td></tr></thead><tfoot><tr><td align="left" colspan="13">‘–’ means tracking failure. The average value is calculated based on the number of successes.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">LDSO <sup>[<a href="#b105" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b105">105</a>]</sup></td><td style="class:table_top_border2" align="center">22.543</td><td style="class:table_top_border2" align="center">23.052</td><td style="class:table_top_border2" align="center">169.247</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">44.010</td><td style="class:table_top_border2" align="center">58.729</td><td style="class:table_top_border2" align="center">53.481</td><td style="class:table_top_border2" align="center">130.993</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">16.277</td><td style="class:table_top_border2" align="center">64.792</td></tr><tr><td style="class:table_bottom_border" align="center">Improved LDSO</td><td style="class:table_bottom_border" align="center">17.772</td><td style="class:table_bottom_border" align="center">13.023</td><td style="class:table_bottom_border" align="center">120.380</td><td style="class:table_bottom_border" align="center">2.133</td><td style="class:table_bottom_border" align="center">1.093</td><td style="class:table_bottom_border" align="center">5.740</td><td style="class:table_bottom_border" align="center">13.491</td><td style="class:table_bottom_border" align="center">1.973</td><td style="class:table_bottom_border" align="center">102.206</td><td style="class:table_bottom_border" align="center">52.664</td><td style="class:table_bottom_border" align="center">14.042</td><td style="class:table_bottom_border" align="center">31.320</td></tr></tbody></table></div><div class="table_footer"></div></div><div id="Table11" class="Figure-block"><div class="table-note"><span class="">Table 11</span><p class="">RMSE<inline-formula><tex-math id="M129">$$ _{\rm{ATE}} $$</tex-math></inline-formula> on the TUM RGB-D dataset <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" rowspan="2" align="center"><b>Method</b></td><td style="class:table_top_border" colspan="5" align="center"><b>Gaussian noise</b></td><td style="class:table_top_border" colspan="5" align="center"><b>Salt-and-pepper noise</b></td></tr><tr><td style="class:table_top_border2" align="center"><b>fr1_xyz</b></td><td style="class:table_top_border2" align="center"><b>fr2_xyz</b></td><td style="class:table_top_border2" align="center"><b>fr2_rpy</b></td><td style="class:table_top_border2" align="center"><b>fr1_desk</b></td><td style="class:table_top_border2" align="center"><b>fr1_desk2</b></td><td style="class:table_top_border2" align="center"><b>fr1_xyz</b></td><td style="class:table_top_border2" align="center"><b>fr2_xyz</b></td><td style="class:table_top_border2" align="center"><b>fr2_rpy</b></td><td style="class:table_top_border2" align="center"><b>fr1_desk</b></td><td style="class:table_top_border2" align="center"><b>fr1_desk2</b></td></tr></thead><tfoot><tr><td align="left" colspan="11">‘–’ means tracking failure.</td></tr></tfoot><tbody><tr><td style="class:table_top_border2" align="center">LDSO <sup>[<a href="#b105" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b105">105</a>]</sup></td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">0.096</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">0.518</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">–</td><td style="class:table_top_border2" align="center">0.841</td><td style="class:table_top_border2" align="center">–</td></tr><tr><td style="class:table_bottom_border" align="center">Improved LDSO</td><td style="class:table_bottom_border" align="center">0.156</td><td style="class:table_bottom_border" align="center">0.01</td><td style="class:table_bottom_border" align="center">0.06</td><td style="class:table_bottom_border" align="center">0.801</td><td style="class:table_bottom_border" align="center">0.756</td><td style="class:table_bottom_border" align="center">0.129</td><td style="class:table_bottom_border" align="center">0.011</td><td style="class:table_bottom_border" align="center">0.058</td><td style="class:table_bottom_border" align="center">0.796</td><td style="class:table_bottom_border" align="center">0.871</td></tr></tbody></table></div><div class="table_footer"></div></div><div class="Figure-block" id="Figure13"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure13" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-13.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 13. Sample outputs of the sequence 'KITTI_07' in the KITTI dataset <sup>[<a href="#b102" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b102">102</a>]</sup>: (A) and (B) are the outputs on the sequence with Gaussian noise and Salt-and-Pepper noise, respectively.</p></div></div></div><div id="sec33" class="article-Section"><h4 >3.5.2. Scene classification</h4><p class="">Scene classification is one of the key technologies of scene understanding for autonomous robots, which can provide the basis for decision-making of the robots. The task of the scene classification for an autonomous robot refers to the information of its surroundings obtained by the on-board sensors, and then the state of the current position is recognized.</p><p class="">Lots of researchers have conducted studies on scene classification. For example, Tang <i>et al</i>. <sup>[<a href="#b106" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b106">106</a>]</sup> proposed an adaptive discriminative region learning network for remote sensing scene classification, which locates discriminative regions effectively for solving the problems of scene classification, such as scale-variation of objects and redundant and noisy areas. Song <i>et al</i>. <sup>[<a href="#b107" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b107">107</a>]</sup> used an ensemble alignment subspace adaptation method for the cross-scene classification. It can settle the problem of both foreign objects in the same spectrum and different spectra. Zhu <i>et al</i>. <sup>[<a href="#b108" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b108">108</a>]</sup> proposed a domain adaptation cross-scene classification approach to simultaneously classify the target common categories and detect the target private categories based on the divergence of different classifiers.</p><p class="">The methods for the scene classification can be divided into two main types. One of them is based on the underlying visual features. This type of method has some shortcomings. For example, the accuracy of the scene classification is low when only the low-level visual features are used to represent the contents of the scene. The other type of the scene classification method is based on the deep learning technologies. To deal with the problem of the scene classification of the road scene, our group presented an improved deep network-based model <sup>[<a href="#b109" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b109">109</a>]</sup>. The structure of the proposed model is shown in <a href="#Figure14" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure14">Figure 14</a>.</p><div class="Figure-block" id="Figure14"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure14" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-14.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 14. The structure of the proposed deep network for the road scene classification <sup>[<a href="#b109" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b109">109</a>]</sup>.</p></div></div><p class="">As shown in <a href="#Figure14" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure14">Figure 14</a>, there are four main parts in the proposed scene classification model, namely, (1) The improved Faster RCNN-based local feature extraction module; (2) The improved Inception_V1-based global feature extraction module; (3) The feature fusion module; (4) The classification network.</p><p class="">In the improved Faster RCNN-based local feature extraction module, the VGG16 Net is used to get the feature map of the whole image first. Then, a residual attention module is used to further deal with redundant information in images. The operation on the feature map based on the residual attention module is:</p><p class=""><div class="disp-formula"><label>(14)</label><tex-math id="E14"> $$ F_{output} (i, j) = F_{input} (i, j) \otimes a_{ij}+F_{input} (i, j) $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M130">$$ F_{output} $$</tex-math></inline-formula> and <inline-formula><tex-math id="M131">$$ F_{input} $$</tex-math></inline-formula> are the output and the input feature value of the residual attention module, respectively; <inline-formula><tex-math id="M132">$$ a_{ij} $$</tex-math></inline-formula> is the attention weight; <inline-formula><tex-math id="M133">$$ \otimes $$</tex-math></inline-formula> is the dot product operation.</p><p class="">The output of the residual attention module is input into the RPN to generate region proposals. The output Region-of-Interests (ROIs) of the RPN is processed by a ROI pooling network to get a fixed-size proposal feature map, which is finally input into a fully connected layer for the object classification and generating the positions of the objects.</p><p class="">In the global feature extraction module, the Inception_V1 is used as the baseline network, which has nine Inception blocks. One Inception block has four branches. To deal with the shortcomings of the general Inception_V1 <sup>[<a href="#b110" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b110">110</a>]</sup>, the Inception_V1 is improved in the proposed model in <sup>[<a href="#b109" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b109">109</a>]</sup>, where a mixed activation function is presented by alternately using the ELU and Leaky ReLU functions for the Inception networks. The Leaky ReLU function is denoted by:</p><p class=""><div class="disp-formula"><label>(15)</label><tex-math id="E15"> $$ y_i = \left\{ \begin{array}{l} x_i, \text{ if } x_i \ge 0 \\ \alpha x_i, \text{ if } \alpha x_i &lt; 0 \\ \end{array} \right. $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M134">$$ \alpha $$</tex-math></inline-formula> is a fixed parameter.</p><p class="">The ELU function is denoted by:</p><p class=""><div class="disp-formula"><label>(16)</label><tex-math id="E16"> $$ y_i = \left\{ \begin{array}{l} x_i, \; \text{if}\; \; x_i \ge 0 \\ e^{x_i} - 1, \; \text{if}\; \; x_i&lt;0 \\ \end{array} \right. $$ </tex-math></div></p><p class="">In the feature fusion module, the local feature vectors and the global feature vectors are appended to get the fused feature <inline-formula><tex-math id="M135">$$ F $$</tex-math></inline-formula>, namely</p><p class=""><div class="disp-formula"><label>(17)</label><tex-math id="E17"> $$ F = \left[ {L, G} \right] $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M136">$$ L{\rm{ = }}\left[ {l _{\rm{1}}, l _2, \cdots, l _N } \right] $$</tex-math></inline-formula> and <inline-formula><tex-math id="M137">$$ G{\rm{ = }}\left[ {g _{\rm{1}}, g_2, \cdots, g_N } \right] $$</tex-math></inline-formula> denote the local feature vectors and the global feature vectors, respectively; <inline-formula><tex-math id="M138">$$ N $$</tex-math></inline-formula> is the feature dimension.</p><p class="">At last, the fused feature vector is input to the classification network for the scene classification. The loss function used in this classification network is as follows:</p><p class=""><div class="disp-formula"><label>(18)</label><tex-math id="E18"> $$ Loss_{cls} = \frac{1}{S}\sum\limits_i {\left( { - \sum\limits_{j = 1}^C {y_{ij} \log \left( {p_{ij} } \right)} } \right)} $$ </tex-math></div></p><p class="">where <inline-formula><tex-math id="M139">$$ C $$</tex-math></inline-formula> is the number of scene classification; <inline-formula><tex-math id="M140">$$ S $$</tex-math></inline-formula> is the number of the samples; <inline-formula><tex-math id="M141">$$ p_{ij} $$</tex-math></inline-formula> is the probability that the <inline-formula><tex-math id="M142">$$ i $$</tex-math></inline-formula>-th sample belongs to the <inline-formula><tex-math id="M143">$$ j $$</tex-math></inline-formula>-th category, and <inline-formula><tex-math id="M144">$$ y_{ij} $$</tex-math></inline-formula> is the indicator variable.</p><p class="">To test the performance of the proposed road scene classification model, our group proposed a special dataset based on two public datasets: KITTI <sup>[<a href="#b111" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b111">111</a>]</sup> and Place365 <sup>[<a href="#b112" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b112">112</a>]</sup>. The results of the comparison experiments are listed in <a href="#Table12" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table12">Table 12</a>, and some scene classification results based on different models are shown in <a href="#Figure15" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure15">Figure 15</a>.</p><div id="Table12" class="Figure-block"><div class="table-note"><span class="">Table 12</span><p class="">The experimental results of scene classification based on different deep networks <sup>[<a href="#b109" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b109">109</a>]</sup></p></div><div class="table-responsive article-table"><table class="a-table"><thead><tr><td style="class:table_top_border" align="center"><b>Network</b></td><td style="class:table_top_border" align="center"><b>Total accuracy</b></td><td style="class:table_top_border" align="center"><b>Standard deviation</b></td><td style="class:table_top_border" align="center"><b>On sunny days</b></td><td style="class:table_top_border" align="center"><b>On rainy days</b></td><td style="class:table_top_border" align="center"><b>At night</b></td></tr></thead><tbody><tr><td style="class:table_top_border2" align="center">AlexNet <sup>[<a href="#b113" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b113">113</a>]</sup></td><td style="class:table_top_border2" align="center">84.20%</td><td style="class:table_top_border2" align="center">5.22%</td><td style="class:table_top_border2" align="center">90.20%</td><td style="class:table_top_border2" align="center">81.70%</td><td style="class:table_top_border2" align="center">80.70%</td></tr><tr><td align="center">EfficientNet <sup>[<a href="#b114" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b114">114</a>]</sup></td><td align="center">87.07%</td><td align="center">8.31%</td><td align="center">96.30%</td><td align="center">80.00%</td><td align="center">85.30%</td></tr><tr><td align="center">Inception_V1 <sup>[<a href="#b110" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b110">110</a>]</sup></td><td align="center">90.53%</td><td align="center">2.51%</td><td align="center">93.40%</td><td align="center">88.70%</td><td align="center">89.50%</td></tr><tr><td style="class:table_bottom_border" align="center">Ours</td><td style="class:table_bottom_border" align="center"><b>94.76%</b></td><td style="class:table_bottom_border" align="center"><b>1.62%</b></td><td style="class:table_bottom_border" align="center"><b>96.50%</b></td><td style="class:table_bottom_border" align="center"><b>93.30%</b></td><td style="class:table_bottom_border" align="center"><b>94.50%</b></td></tr></tbody></table></div><div class="table_footer"></div></div><div class="Figure-block" id="Figure15"><div xmlns="http://www.w3.org/1999/xhtml" class="article-figure-image"><a href="/articles/ir.2023.22/image/Figure15" class="Article-img" alt="" target="_blank"><img alt="Deep learning-based scene understanding for autonomous robots: a survey" src="https://image.oaes.cc/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022-15.jpg" class="" title="" alt="" /></a></div><div class="article-figure-note"><p class="figure-note"></p><p class="figure-note">Figure 15. Some scene classification results based on different models <sup>[<a href="#b109" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="b109">109</a>]</sup>.</p></div></div><p class="">It can be seen that our proposed model can improve the accuracy to 94.76%, which is 4.67% (Relative value) higher than the general Inception_V1 (the second-best model). In addition, our proposed model has good scene classification performance under some challenging tasks, such as the task on a rainy day or at night (see <a href="#Table12" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Table12">Table 12</a> and <a href="#Figure15" class="Link_style" data-jats-ref-type="bibr" data-jats-rid="Figure15">Figure 15</a> for details).</p></div></div></div><div id="sec14" class="article-Section"><h2 >4. FUTURE DIRECTIONS</h2><p class="">With the developments of the artificial intelligence technologies and deep learning methods, great progress has been made in the research of scene understanding for autonomous robots. However, there are still a lot of difficulties in using deep learning to perceive and understand the surroundings for autonomous robots. There are some problems that should be further studied as follows:</p><p class="">(1) Light-weight models: With the continuous improvement of the computing power of hardware devices, the scene understanding method based on deep learning technology has achieved great success. However, it is difficult to run large-scale models on autonomous robots with limited processing, memory, and power resources. How to design a practical light-weight deep learning model while keeping the desired accuracy is a challenging task. Meanwhile, it also needs to develop efficient compact representation models for 3D data.</p><p class="">(2) Multi-task learning: A valuable but less explored direction for scene understanding is to jointly train models on multiple terminal tasks. For example, semantic contour detection technology could jointly detect target contours and recognize the semantic information of the contours. This multi-task learning method is useful for model learning without decreasing the performance of any single task.</p><p class="">(3) Transfer learning: Common tasks such as object detection, semantic segmentation, and scene classification usually have many annotated examples for training. However, there is a lack of large datasets for tasks such as layout estimation, affordance prediction, and physics-based reasoning. How to optimally fine-tune an existing model to the desired task so that the knowledge is properly transferred from the source domain to the target domain is a good research direction in this field.</p><p class="">(4) Multi-modal fusion: Building a cross-modal adaptive fusion network will allow us to more fully fuse the sparse information in the point cloud space with the dense information in the image space. Based on these multi-modal fusion methods, the accuracy of the scene understanding can be further improved. In this field, how to fuse different modal information efficiently is a good research direction.</p><p class="">(5) The specific datasets: To improve the performance of the deep learning-based models, some specific datasets should be constructed for the applications of the robots in different environments. For example, how to make the autonomous underwater vehicle (AUV) work efficiently is still a challenging task. The main reason is that the underwater environments are complex; for example, the illumination is low, and the reference objects are fewer. To build a specific dataset for special robots is arduous, but it is very meaningful.</p><p class="">(6) Application extensions: With the popularization of robot applications and the important role that robots play in various fields, we need to take a step forward in researching the applications of scene understanding for autonomous robots. In addition to the applications mentioned above, such as target detection and pose estimation, we need to focus on more application extensions, such as physics-based reasoning, affordance prediction, full 3D reconstruction, etc.</p><p class="">The scene understanding of autonomous robots is the first prerequisite for autonomous robots to complete complex tasks. On this basis, robots can become smarter to further improve social productivity, produce huge social benefits, and improve people's life quality. Therefore, there are many problems that need to be solved efficiently. The deep learning-based methods for the robotic scene understanding are still on the way.</p></div><div id="sec15" class="article-Section"><h2 >5. CONCLUSIONS</h2><p class="">This study analyzes the most recent advancements in deep learning-based environment perception and understanding methods for autonomous robots. Firstly, this paper provides a summary of recent advances in the ability of autonomous robots to perceive and understand their environments. The typical application techniques for perceiving and understanding the surroundings by autonomous robots are discussed. Then, the research and application of deep learning-based methods in the field of scene understanding for autonomous robots are further discussed in this study, which also presents exemplary techniques for the use of robot environment perception and understanding. Lastly, the main issues and difficulties of deep learning-based autonomous robot scene understanding are examined.</p><p class="">It is obvious that the deep learning method will become one of the most popular research topics in the field of autonomous robot scene understanding, including theoretical and applied research. Deep learning-based technologies will further improve the intelligence and autonomy of robots. With a better perception and understanding of the environment, the robots will be able to solve complex tasks instead of just performing some simple and single commands. At present, many fundamental problems of robot scene understanding based on deep learning have been explored with exciting results, which show the potential of deep learning. But there are still many questions that need to be further studied.</p></div><div id="sec16" class="article-Section"><h2 >DECLARATIONS</h2><div id="sec21" class="article-Section"><h3 >Authors' contributions</h3><p class="">Funding acquisition: Ni J</p><p class="">Project administration: Ni J, Shi P</p><p class="">Writing-original draft: Chen Y, Tang G</p><p class="">Writing-review and editing: Shi J, Cao W</p></div><div id="sec22" class="article-Section"><h3 >Availability of data and materials</h3><p class="">Not applicable.</p></div><div id="sec23" class="article-Section"><h3 >Financial support and sponsorship</h3><p class="">This work has been supported by the National Natural Science Foundation of China (61873086) and the Science and Technology Support Program of Changzhou (CE20215022).</p></div><div id="sec24" class="article-Section"><h3 >Conflicts of interest</h3><p class="">The authors declared that they have no conflicts of interest related to this work.</p></div><div id="sec25" class="article-Section"><h3 >Ethical approval and consent to participate</h3><p class="">Not applicable.</p></div><div id="sec26" class="article-Section"><h3 >Consent for publication</h3><p class="">Not applicable.</p></div><div id="sec27" class="article-Section"><h3 >Copyright</h3><p class="">© The Author(s) 2023.</p></div></div></div> <!----> <div class="art_list" data-v-5751490a></div> <div class="article_references" data-v-5751490a><div class="ReferencesBox" data-v-5751490a><h2 id="References" class="bg_d" data-v-5751490a><span data-v-5751490a><a href="/articles//reference" data-v-5751490a>REFERENCES</a></span> <span class="icon" data-v-5751490a><i class="el-icon-arrow-down" data-v-5751490a></i> <i class="el-icon-arrow-up hidden" data-v-5751490a></i></span></h2> <div class="references_list heightHide" data-v-5751490a><div id="b1" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>1. </span> <span data-v-5751490a>Ni&nbsp;J, Wang&nbsp;X, Gong&nbsp;T, Xie&nbsp;Y. An improved adaptive ORB-SLAM method for monocular vision robot under dynamic environments. <i>Int J Mach Learn Cyber</i> 2022;13:3821-36.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s13042-022-01627-2" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b2" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>2. </span> <span data-v-5751490a>Li&nbsp;J, Xu&nbsp;Z, Zhu&nbsp;D, et al. Bio-inspired intelligence with applications to robotics: a survey. <i>Intell Robot</i> 2021;1:58-83.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.20517/ir.2021.08" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b3" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>3. </span> <span data-v-5751490a>Ni&nbsp;J, Tang&nbsp;M, Chen&nbsp;Y, Cao&nbsp;W. An improved cooperative control method for hybrid unmanned aerial-ground system in multitasks. <i>Int J Aerosp Eng</i> 2020; doi: 10.1155/2020/9429108.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2020/9429108" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b4" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>4. </span> <span data-v-5751490a>Zhao&nbsp;ZQ, Zheng&nbsp;P, Xu&nbsp;ST, Wu&nbsp;X. Object Detection with Deep Learning: A Review. <i>IEEE Trans Neural Netw Learn Syst</i> 2019;30:3212-32.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TNNLS.2018.2876865" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/30703038" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b5" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>5. </span> <span data-v-5751490a>Garcia-Garcia&nbsp;A, Orts-Escolano&nbsp;S, Oprea&nbsp;S, Villena-Martinez&nbsp;V, Martinez-Gonzalez&nbsp;P, Garcia-rodriguez&nbsp;J. A survey on deep learning techniques for image and video semantic segmentation. <i>Appl Soft Comput</i> 2018;70:41-65.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.asoc.2018.05.018" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b6" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>6. </span> <span data-v-5751490a>Wang&nbsp;L, Huang&nbsp;Y. A survey of 3D point cloud and deep learning-based approaches for scene understanding in autonomous driving. <i>IEEE Intell Transport Syst Mag</i> 2022;14:135-54.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/MITS.2021.3109041" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b7" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>7. </span> <span data-v-5751490a>Naseer&nbsp;M, Khan&nbsp;S, Porikli&nbsp;F. Indoor scene understanding in 2.5/3D for autonomous agents: a survey. <i>IEEE Access</i> 2019;7:1859-87.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ACCESS.2018.2886133" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b8" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>8. </span> <span data-v-5751490a>Zhu&nbsp;M, Ferstera&nbsp;A, Dinulescu&nbsp;S, et al. A peristaltic soft, wearable robot for compression therapy and massage. <i>IEEE Robot Autom</i> 2023;8:4665-72.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LRA.2023.3287773" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b9" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>9. </span> <span data-v-5751490a>Sun&nbsp;P, Shan&nbsp;R, Wang&nbsp;S. An intelligent rehabilitation robot with passive and active direct switching training: improving intelligence and security of human-robot interaction systems. <i>IEEE Robot Automat Mag</i> 2023;30:72-83.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/MRA.2022.3228490" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b10" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>10. </span> <span data-v-5751490a>Wang&nbsp;TM, Tao&nbsp;Y, Liu&nbsp;H. Current researches and future development trend of intelligent robot: a review. <i>Int J Autom Comput</i> 2018;15:525-46.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s11633-018-1115-1" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b11" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>11. </span> <span data-v-5751490a>Lowe&nbsp;DG. Distinctive image features from scale-invariant keypoints. <i>Int J comput vis</i> 2004;60:91-110.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1023/B:VISI.0000029664.99615.94" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b12" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>12. </span> <span data-v-5751490a>Zhou&nbsp;H, Yuan&nbsp;Y, Shi&nbsp;C. Object tracking using SIFT features and mean shift. <i>Comput Vis Image Underst</i> 2009;113:345-52.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.cviu.2008.08.006" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b13" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>13. </span> <span data-v-5751490a>Oliva&nbsp;A, Torralba&nbsp;A. Modeling the shape of the scene: a holistic representation of the spatial envelope. <i>Int J comput vis</i> 2001;42:145-75.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1023/A:1011139631724" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b14" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>14. </span> <span data-v-5751490a>Hofmann&nbsp;T. Unsupervised learning by probabilistic latent semantic analysis. <i>Mach Learn</i> 2001;42:177-96.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1023/A:1007617005950" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b15" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>15. </span> <span data-v-5751490a>Sarhan&nbsp;S, Nasr&nbsp;AA, Shams&nbsp;MY. Multipose face recognition-based combined adaptive deep learning vector quantization. <i>Comput Intell Neurosci</i> 2020;2020:8821868.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2020/8821868" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/33029115" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b16" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>16. </span> <span data-v-5751490a>Liu&nbsp;B, Wu&nbsp;H, Su&nbsp;W, Zhang&nbsp;W, Sun&nbsp;J. Rotation-invariant object detection using Sector-ring HOG and boosted random ferns. <i>Vis Comput</i> 2018;34:707-19.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s00371-017-1408-3" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b17" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>17. </span> <span data-v-5751490a>Wang X, Han TX, Yan S. An HOG-LBP human detector with partial occlusion handling. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 32-39.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCV.2009.5459207" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b18" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>18. </span> <span data-v-5751490a>Vailaya&nbsp;A, Figueiredo&nbsp;MA, Jain&nbsp;AK, Zhang&nbsp;HJ. Image classification for content-based indexing. <i>XX</i> 2001;10:117-30.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/83.892448" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b19" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>19. </span> <span data-v-5751490a>Li LJ, Su H, Xing EP, Fei-Fei L. Object bank: a high-level image representation for scene classification &amp; semantic feature sparsification. In: Proceedings of the 23rd International Conference on Neural Information Processing Systems; 2010. p. 1378–86. Available from: <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2010/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html" xmlns:xlink="http://www.w3.org/1999/xlink">https://proceedings.neurips.cc/paper/2010/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b20" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>20. </span> <span data-v-5751490a>Zhang&nbsp;L, Li&nbsp;W, Yu&nbsp;L, Sun&nbsp;L, Dong&nbsp;X, Ning&nbsp;X. GmFace: an explicit function for face image representation. <i>Displays</i> 2021;68:102022.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.displa.2021.102022" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b21" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>21. </span> <span data-v-5751490a>Ning&nbsp;X, Gong&nbsp;K, Li&nbsp;W, Zhang&nbsp;L, Bai&nbsp;X, et al. Feature refinement and filter network for person re-identification. <i>IEEE Trans Circuits Syst Video Technol</i> 2021;31:3391-402.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TCSVT.2020.3043026" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b22" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>22. </span> <span data-v-5751490a>Ni&nbsp;J, Chen&nbsp;Y, Chen&nbsp;Y, Zhu&nbsp;J, Ali&nbsp;D, Cao&nbsp;W. A survey on theories and applications for self-driving cars based on deep learning methods. <i>Appl Sci</i> 2020;10:2749.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.3390/app10082749" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b23" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>23. </span> <span data-v-5751490a>Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition. Providence; 2012 Jun 16-21; RI, USA. IEEE; 2012. p. 3354-61.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2012.6248074" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b24" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>24. </span> <span data-v-5751490a>Caesar H, Bankiti V, Lang AH, et al. nuScenes: a multimodal dataset for autonomous driving. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11618-28.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR42600.2020.01164" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b25" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>25. </span> <span data-v-5751490a>Hinterstoisser S, Lepetit V, Ilic S, et al. Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes. In: Lee KM, Matsushita Y, Rehg JM, Hu Z, editors. Computer Vision - ACCV; 2013. p. 548-62.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/978-3-642-37331-2-42" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b26" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>26. </span> <span data-v-5751490a>Piga&nbsp;NA, Onyshchuk&nbsp;Y, Pasquale&nbsp;G, Pattacini&nbsp;U, Natale&nbsp;L. ROFT: Real-time tptical flow-aided 6D object pose and velocity tracking. <i>IEEE Robot Autom Lett</i> 2022;7:159-66.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LRA.2021.3119379" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b27" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>27. </span> <span data-v-5751490a>Everingham&nbsp;M, Gool&nbsp;LV, Williams&nbsp;CKI, Winn&nbsp;JM, Zisserman&nbsp;A. The pascal visual object classes (VOC) challenge. <i>Int J Comput Vis</i> 2010;88:303-38.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s11263-009-0275-4" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b28" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>28. </span> <span data-v-5751490a>Cordts M, Omran M, Ramos S, et al. The cityscapes dataset for semantic urban scene understanding. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016 Jun 27-30; Las Vegas, NV, USA. IEEE; 2016. p. 3213-23.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2016.350" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b29" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>29. </span> <span data-v-5751490a>Wang W, Shen J, Guo F, Cheng MM, Borji A. Revisiting video saliency: a large-scale benchmark and a new model. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 4894-903.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00514" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b30" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>30. </span> <span data-v-5751490a>Kristan M, Leonardis A, Matas J, et al. The visual object tracking VOT2017 challenge results. In: 2017 IEEE International Conference on Computer Vision Workshops (ICCVW); 2017 Oct 22-29; Venice, Italy. IEEE; 2017. p. 1949-72.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCVW.2017.230" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b31" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>31. </span> <span data-v-5751490a>Ni&nbsp;J, Shen&nbsp;K, Chen&nbsp;Y, Yang&nbsp;SX. An improved SSD-like deep network-based object detection method for indoor scenes. <i>IEEE Trans Instrum Meas</i> 2023;72:1-15.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TIM.2023.3244819" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/37323850" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b32" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>32. </span> <span data-v-5751490a>Qian&nbsp;R, Lai&nbsp;X, Li&nbsp;X. BADet: boundary-aware 3D object detection from point clouds. <i>Pattern Recognit</i> 2022;125:108524.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.patcog.2022.108524" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b33" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>33. </span> <span data-v-5751490a>Shi&nbsp;S, Wang&nbsp;Z, Shi&nbsp;J, Wang&nbsp;X, Li&nbsp;H. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. <i>IEEE Trans Pattern Anal Mach Intell</i> 2021;43:2647-64.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TPAMI.2020.2977026" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/32142423" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b34" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>34. </span> <span data-v-5751490a>Li&nbsp;Y, Ma&nbsp;L, Zhong&nbsp;Z, Cao&nbsp;D, Li&nbsp;J. TGNet: geometric graph CNN on 3-D point cloud segmentation. <i>IEEE Trans Geosci Remote Sens</i> 2020;58:3588-600.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TGRS.2019.2958517" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b35" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>35. </span> <span data-v-5751490a>Yan&nbsp;Y, Mao&nbsp;Y, Li&nbsp;B. SECOND: sparsely embedded convolutional detection. <i>Sensors</i> 2018;18:3337.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.3390/s18103337" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b36" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>36. </span> <span data-v-5751490a>Qi CR, Liu W, Wu C, Su H, Guibas LJ. Frustum pointnets for 3D object detection from RGB-D data. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018 Jun 18-23; Salt Lake City, UT, USA; IEEE; 2018. p. 918-27.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00102" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b37" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>37. </span> <span data-v-5751490a>Wang Z, Jia K. Frustum convNet: sliding frustums to aggregate local point-wise features for amodal 3D object detection. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS); 2019 Nov 03-08; Macau, China. IEEE; 2019. p. 1742-49.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/IROS40897.2019.8968513" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b38" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>38. </span> <span data-v-5751490a>Chen Y, Liu S, Shen X, Jia J. Fast point R-CNN. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea. IEEE; 2019. p. 9774-83.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCV.2019.00987" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b39" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>39. </span> <span data-v-5751490a>He C, Zeng H, Huang J, Hua XS, Zhang L. Structure aware single-stage 3D object detection from point cloud. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11870-9.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR42600.2020.01189" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b40" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>40. </span> <span data-v-5751490a>Liu Z, Zhao X, Huang T, Hu R, Zhou Y, Bai X. TANet: robust 3D object detection from point clouds with triple attention. In: 34th AAAI Conference on Artificial Intelligence, AAAI; 2020 Feb 7-12; New York, NY, United states. California: AAAI; 2020. p. 11677-84.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1609/aaai.v34i07.6837" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b41" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>41. </span> <span data-v-5751490a>Yin T, Zhou X, Krahenbuhl P. Center-based 3D Object Detection and Tracking. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021. Virtual, Online, United states; 2021. pp. 11779 – 11788.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR46437.2021.01161" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b42" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>42. </span> <span data-v-5751490a>Wang H, Shi S, Yang Z, et al. RBGNet: ray-based Grouping for 3D Object Detection. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2022 Jun 18-24; New Orleans, LA, USA. IEEE; 2022. p. 1100-09.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR52688.2022.00118" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b43" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>43. </span> <span data-v-5751490a>Chen&nbsp;Y, Ni&nbsp;J, Tang&nbsp;G, Cao&nbsp;W, Yang&nbsp;SX. An improved dense-to-sparse cross-modal fusion network for 3D object detection in RGB-D images. <i>Multimed Tools Appl</i> 2023; doi: 10.1007/s11042-023-15845-5.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s11042-023-15845-5" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b44" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>44. </span> <span data-v-5751490a>Hoang&nbsp;DC, Stork&nbsp;JA, Stoyanov&nbsp;T. Voting and attention-based pose relation learning for object pose estimation from 3D point clouds. <i>IEEE Robot Autom Lett</i> 2022;7:8980-7.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LRA.2022.3189158" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b45" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>45. </span> <span data-v-5751490a>Yue K, Sun M, Yuan Y, Zhou F, Ding E, Xu F. Compact generalized non-local network. arXiv. [Preprint.] November 1, 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.13125" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/1810.13125</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b46" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>46. </span> <span data-v-5751490a>Chen H, Wang P, Wang F, Tian W, Xiong L, Li H. Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation. arXiv. [Preprint.] August 11, 2022 Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2203.13254" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/2203.13254</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b47" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>47. </span> <span data-v-5751490a>Moon G, Chang JY, Lee KM. V2V-poseNet: voxel-to-voxel prediction network for accurate 3D hand and human pose estimation from a single depth map. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 5079-88.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00533" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b48" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>48. </span> <span data-v-5751490a>Li Z, Wang G, Ji X. CDPN: coordinates-based disentangled pose network for real-time RGB-based 6-DoF object pose estimation. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea (South). IEEE; 2020. p. 7677-86.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCV.2019.00777" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b49" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>49. </span> <span data-v-5751490a>Wang H, Sridhar S, Huang J, Valentin J, Song S, Guibas LJ. Normalized object coordinate space for category-level 6D object pose and size estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 2637-46.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2019.00275" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b50" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>50. </span> <span data-v-5751490a>Yu X, Zhuang Z, Koniusz P, Li H. 6DoF object pose estimation via differentiable proxy voting loss. arXiv. [Preprint.] Febuary 11, 2020. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.03923" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/2002.03923</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b51" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>51. </span> <span data-v-5751490a>Chen W, Jia X, Chang HJ, Duan J, Leonardis A. G2L-net: global to local network for real-time 6D pose estimation with embedding vector features. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 4232-41.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR42600.2020.00429" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b52" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>52. </span> <span data-v-5751490a>He Y, Sun W, Huang H, Liu J, Fan H, Sun J. PVN3D: a deep point-wise 3D keypoints voting network for 6DoF pose estimation. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19 Seattle, WA, USA. IEEE; 2020. pp. 11629-38.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR42600.2020.01165" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b53" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>53. </span> <span data-v-5751490a>He Y, Huang H, Fan H, Chen Q, Sun J. FFB6D: a full flow bidirectional fusion network for 6D pose estimation. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2021 Jun 20-25; Nashville, TN, USA. IEEE; 2021. p. 3002-12.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR46437.2021.00302" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b54" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>54. </span> <span data-v-5751490a>Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. arXiv. [Preprint.] November 14, 2014. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1411.4038" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/1411.4038</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b55" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>55. </span> <span data-v-5751490a>Badrinarayanan&nbsp;V, Kendall&nbsp;A, Cipolla&nbsp;R. SegNet: a deep convolutional encoder-decoder architecture for image segmentation. <i>IEEE Trans Pattern Anal Mach Intell</i> 2017;39:2481-95.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TPAMI.2016.2644615" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/28060704" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b56" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>56. </span> <span data-v-5751490a>Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. Semantic image segmentation with deep convolutional nets and fully connected CRFs. arXiv. [Preprint.] December 22, 2014. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.7062" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/1412.7062</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b57" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>57. </span> <span data-v-5751490a>Chen&nbsp;LC, Papandreou&nbsp;G, Kokkinos&nbsp;I, Murphy&nbsp;K, Yuille&nbsp;AL. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. <i>IEEE Trans Pattern Anal Mach Intell</i> 2017;40:834-48.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TPAMI.2017.2699184" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/28463186" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b58" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>58. </span> <span data-v-5751490a>Lin G, Milan A, Shen C, Reid I. Refinenet: multi-path refinement networks for high-resolution semantic segmentation. In: 2017IEEE conference on computer vision and pattern recognition (CVPR); 2017. p. 1925-34.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2017.549" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b59" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>59. </span> <span data-v-5751490a>Zeng L, Zhang S, Wang P, Li Z, Hu Y, Xie T. Defect detection algorithm for magnetic particle inspection of aviation ferromagnetic parts based on improved DeepLabv3+. <i>Meas Sci Technol</i> 2023;34: 065401. Measurement Science and Technology 2023;34: 065401.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1088/1361-6501/acb9ae" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b60" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>60. </span> <span data-v-5751490a>Yin&nbsp;R, Cheng&nbsp;Y, Wu&nbsp;H, Song&nbsp;Y, Yu&nbsp;B, Niu&nbsp;R. Fusionlane: multi-sensor fusion for lane marking semantic segmentation using deep neural networks. <i>IEEE Trans Intell Transport Syst</i> 2022;23:1543-53.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TITS.2020.3030767" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b61" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>61. </span> <span data-v-5751490a>Hu&nbsp;P, Perazzi&nbsp;F, Heilbron&nbsp;FC, et al. Real-time semantic segmentation with fast attention. <i>IEEE Robot Autom Lett</i> 2021;6:263-70.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LRA.2020.3039744" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b62" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>62. </span> <span data-v-5751490a>Sun&nbsp;Y, Zuo&nbsp;W, Yun&nbsp;P, Wang&nbsp;H, Liu&nbsp;M. FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion. <i>IEEE Trans Automat Sci Eng</i> 2021;18:1000-11.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TASE.2020.2993143" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b63" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>63. </span> <span data-v-5751490a>Yang M, Yu K, Zhang C, Li Z, Yang K. DenseASPP for semantic segmentation in street scenes. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 3684-92.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00388" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b64" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>64. </span> <span data-v-5751490a>Zhang H, Dana K, Shi J, et al. Context encoding for semantic segmentation. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 7151-60.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00747" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b65" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>65. </span> <span data-v-5751490a>Fu J, Liu J, Tian H, et al. Dual attention network for scene segmentation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 3141-9.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2019.00326" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b66" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>66. </span> <span data-v-5751490a>He J, Deng Z, Zhou L, Wang Y, Qiao Y. Adaptive pyramid context network for semantic segmentation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 7511-20.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2019.00770" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b67" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>67. </span> <span data-v-5751490a>Zhang C, Lin G, Liu F, Yao R, Shen C. CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 5212-21.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2019.00536" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b68" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>68. </span> <span data-v-5751490a>Liu J, He J, Zhang J, Ren JS, Li H. EfficientFCN: holistically-guided decoding for semantic segmentation. In: Vedaldi A, Bischof H, Brox T, Frahm JM, editors. Computer Vision – ECCV 2020. Cham: Springer; 2020. p. 1-17.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/978-3-030-58574-7-1" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b69" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>69. </span> <span data-v-5751490a>Ha Q, Watanabe K, Karasawa T, Ushiku Y, Harada T. MFNet: towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes. In: 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS); 2017 Sep 24-28; Vancouver, BC, Canada. IEEE; 2017. p. 5108-15.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/IROS.2017.8206396" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b70" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>70. </span> <span data-v-5751490a>Cheng&nbsp;B, Schwing&nbsp;AG, Kirillov&nbsp;A. Per-pixel classification is not all you need for semantic segmentation. <i>Signal Process Image Commun</i> 2021;88:17864-75.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.image.2020.115950" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b71" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>71. </span> <span data-v-5751490a>Zhou&nbsp;W, Yue&nbsp;Y, Fang&nbsp;M, Qian&nbsp;X, Yang&nbsp;R, Yu&nbsp;L. BCINet: bilateral cross-modal interaction network for indoor scene understanding in RGB-D images. <i>Inf Fusion</i> 2023;94:32-42.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.inffus.2023.01.016" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b72" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>72. </span> <span data-v-5751490a>Lou&nbsp;J, Lin&nbsp;H, Marshall&nbsp;D, Saupe&nbsp;D, Liu&nbsp;H. TranSalNet: towards perceptually relevant visual saliency prediction. <i>Neurocomputing</i> 2022;494:455-67.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.neucom.2022.04.080" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b73" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>73. </span> <span data-v-5751490a>Judd T, Ehinger K, Durand F, Torralba A. Learning to predict where humans look. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 2106-13.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCV.2009.5459462" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b74" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>74. </span> <span data-v-5751490a>Ishikura&nbsp;K, Kurita&nbsp;N, Chandler&nbsp;DM, Ohashi&nbsp;G. Saliency detection based on multiscale extrema of local perceptual color differences. <i>IEEE Trans Image Process</i> 2018;27:703-17.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TIP.2017.2767288" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/29185988" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b75" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>75. </span> <span data-v-5751490a>Zou&nbsp;W, Zhuo&nbsp;S, Tang&nbsp;Y, Tian&nbsp;S, Li&nbsp;X, Xu&nbsp;C. STA3D: spatiotemporally attentive 3D network for video saliency prediction. <i>Pattern Recognit Lett</i> 2021;147:78-84.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.patrec.2021.04.010" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b76" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>76. </span> <span data-v-5751490a>Wang W, Shen J, Dong X, Borji A. Salient object detection driven by fixation prediction. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 1711-20.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/CVPR.2018.00184" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b77" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>77. </span> <span data-v-5751490a>Huang&nbsp;R, Xing&nbsp;Y, Wang&nbsp;Z. RGB-D salient object detection by a CNN with multiple layers fusion. <i>IEEE Signal Process Lett</i> 2019;26:552-6.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LSP.2019.2898508" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b78" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>78. </span> <span data-v-5751490a>Wang&nbsp;N, Gong&nbsp;X. Adaptive fusion for RGB-D salient object detection. <i>IEEE Access</i> 2019;7:55277-84.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ACCESS.2019.2913107" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b79" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>79. </span> <span data-v-5751490a>Zhang&nbsp;J, Yu&nbsp;M, Jiang&nbsp;G, Qi&nbsp;Y. CMP-based saliency model for stereoscopic omnidirectional images. <i>Digit Signal Process</i> 2020;101:102708.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.dsp.2020.102708" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b80" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>80. </span> <span data-v-5751490a>Fang&nbsp;Y, Zhang&nbsp;C, Min&nbsp;X, et al. DevsNet: deep video saliency network using short-term and long-term cues. <i>Pattern Recognit</i> 2020;103:107294.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.patcog.2020.107294" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b81" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>81. </span> <span data-v-5751490a>Li&nbsp;F, Zheng&nbsp;J, fang&nbsp;Zhang Y, Liu&nbsp;N, Jia&nbsp;W. AMDFNet: adaptive multi-level deformable fusion network for RGB-D saliency detection. <i>Neurocomputing</i> 2021;465:141-56.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.neucom.2021.08.116" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b82" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>82. </span> <span data-v-5751490a>Lee&nbsp;H, Kim&nbsp;S. SSPNet: learning spatiotemporal saliency prediction networks for visual tracking. <i>Inf Sci</i> 2021;575:399-416.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.ins.2021.06.042" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b83" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>83. </span> <span data-v-5751490a>Xue&nbsp;H, Sun&nbsp;M, Liang&nbsp;Y. ECANet: explicit cyclic attention-based network for video saliency prediction. <i>Neurocomputing</i> 2022;468:233-44.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.neucom.2021.10.024" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b84" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>84. </span> <span data-v-5751490a>Zhang&nbsp;N, Nex&nbsp;F, Kerle&nbsp;N, Vosselman&nbsp;G. LISU: low-light indoor scene understanding with joint learning of reflectance restoration. <i>SPRS J Photogramm Remote Sens</i> 2022;183:470-81.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.isprsjprs.2021.11.010" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b85" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>85. </span> <span data-v-5751490a>Tang&nbsp;G, Ni&nbsp;J, Chen&nbsp;Y, Cao&nbsp;W, Yang&nbsp;SX. An improved cycleGAN based model for low-light image enhancement. <i>IEEE Sensors J</i> 2023; doi: 10.1109/JSEN.2023.3296167.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/JSEN.2023.3296167" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b86" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>86. </span> <span data-v-5751490a>He&nbsp;J, Li&nbsp;M, Wang&nbsp;Y, Wang&nbsp;H. OVD-SLAM: an online visual SLAM for dynamic environments. <i>IEEE Sensors J</i> 2023;23:13210-9.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/JSEN.2023.3270534" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b87" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>87. </span> <span data-v-5751490a>Lu&nbsp;X, Sun&nbsp;H, Zheng&nbsp;X. A feature aggregation convolutional neural network for remote sensing scene classification. <i>IEEE Trans Geosci Remote Sens</i> 2019;57:7894-906.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TGRS.2019.2917161" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b88" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>88. </span> <span data-v-5751490a>Ma&nbsp;D, Tang&nbsp;P, Zhao&nbsp;L. SiftingGAN: generating and sifting labeled samples to improve the remote sensing image scene classification baseline <i>in vitro</i>. <i>IEEE Geosci Remote Sens Lett</i> 2019;16:1046-50.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LGRS.2018.2890413" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b89" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>89. </span> <span data-v-5751490a>Zhang&nbsp;X, Qiao&nbsp;Y, Yang&nbsp;Y, Wang&nbsp;S. SMod: scene-specific-prior-based moving object detection for airport apron surveillance systems. <i>IEEE Intell Transport Syst Mag</i> 2023;15:58-69.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/MITS.2021.3122926" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b90" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>90. </span> <span data-v-5751490a>Tang&nbsp;G, Ni&nbsp;J, Shi&nbsp;P, Li&nbsp;Y, Zhu&nbsp;J. An improved ViBe-based approach for moving object detection. <i>Intell Robot</i> 2022;2:130-44.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.20517/ir.2022.07" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b91" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>91. </span> <span data-v-5751490a>Lee CY, Badrinarayanan V, Malisiewicz T, Rabinovich A. Roomnet: end-to-end room layout estimation. arXiv. [Preprint.] August 7, 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.06241" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/1703.06241</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b92" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>92. </span> <span data-v-5751490a>Hsiao CW, Sun C, Sun M, Chen HT. Flat2layout: Flat representation for estimating layout of general room types. arXiv. [Preprint.] May 29, 2019. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.12571" xmlns:xlink="http://www.w3.org/1999/xlink">https://arxiv.org/abs/1905.12571</ext-link> [Last accessed on 8 Aug 2023].</span></p> <div class="refrences" data-v-5751490a><!----> <!----> <!----></div></div><div id="b93" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>93. </span> <span data-v-5751490a>Sarhan&nbsp;S, Nasr&nbsp;AA, Shams&nbsp;MY. Multipose face recognition-based combined adaptive deep learning vector quantization. <i>Comput Intell Neurosci</i> 2020;2020:8821868.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2020/8821868" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/33029115" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b94" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>94. </span> <span data-v-5751490a>Rublee E, Rabaud V, Konolige K, Bradski G. ORB: an efficient alternative to SIFT or SURF. In: 2011 International conference on computer vision; 2011 Nov 06-13; Barcelona, Spain. IEEE; 2012. p. 2564-71.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICCV.2011.6126544" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b95" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>95. </span> <span data-v-5751490a>Wang&nbsp;K, Ma&nbsp;S, Ren&nbsp;F, Lu&nbsp;J. SBAS: salient bundle adjustment for visual SLAM. <i>IEEE Trans Instrum Meas</i> 2021;70:1-9.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TIM.2021.3105243" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/33776080" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b96" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>96. </span> <span data-v-5751490a>Ni&nbsp;J, Gong&nbsp;T, Gu&nbsp;Y, Zhu&nbsp;J, Fan&nbsp;X. An improved deep residual network-based semantic simultaneous localization and mapping method for monocular vision robot. <i>Comput Intell Neurosci</i> 2020;2020:7490840.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2020/7490840" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/32104171" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b97" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>97. </span> <span data-v-5751490a>Fu&nbsp;Q, Yu&nbsp;H, Wang&nbsp;X, et al. Fast ORB-SLAM without keypoint descriptors. <i>IEEE Trans Image Process</i> 2022;31:1433-46.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TIP.2021.3136710" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/34951846" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b98" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>98. </span> <span data-v-5751490a>Engel J, Schöps T, Cremers D. LSD-SLAM: large-scale direct monocular SLAM. In: Fleet D, Pajdla T, Schiele B, Tuytelaars, editors. Computer Vision – ECCV 2014. Cham: Springer; 2014. p. 834-49.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/978-3-319-10605-2-54" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b99" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>99. </span> <span data-v-5751490a>Engel&nbsp;J, Koltun&nbsp;V, Cremers&nbsp;D. Direct sparse odometry. <i>IEEE Trans Image Process</i> 2018;40:611-25.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TPAMI.2017.2658577" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b100" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>100. </span> <span data-v-5751490a>Wang&nbsp;Y, Zhang&nbsp;S, Wang&nbsp;J. Ceiling-view semi-direct monocular visual odometry with planar constraint. <i>Remote Sens</i> 2022;14:5447.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.3390/rs14215447" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b101" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>101. </span> <span data-v-5751490a>Forster&nbsp;C, Zhang&nbsp;Z, Gassner&nbsp;M, Werlberger&nbsp;M, Scaramuzza&nbsp;D. SVO: semidirect visual odometry for monocular and multicamera systems. <i>IEEE Trans Robot</i> 2017;33:249-65.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TRO.2016.2623335" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b102" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>102. </span> <span data-v-5751490a>Chen&nbsp;Y, Ni&nbsp;J, Mutabazi&nbsp;E, Cao&nbsp;W, Yang&nbsp;SX. A variable radius side window direct SLAM method based on semantic information. <i>Comput Intell Neurosci</i> 2022;2022:4075910.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2022/4075910" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/36045974" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b103" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>103. </span> <span data-v-5751490a>Liu&nbsp;L. Image classification in htp test based on convolutional neural network model. <i>Comput Intell Neurosci</i> 2021;2021:6370509.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1155/2021/6370509" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/34659394" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b104" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>104. </span> <span data-v-5751490a>Zheng&nbsp;D, Li&nbsp;L, Zheng&nbsp;S, et al. A defect detection method for rail surface and fasteners based on deep convolutional neural network. <i>Comput Intell Neurosci</i> 2021;2021:2565500.</span></p> <div class="refrences" data-v-5751490a><!----> <a href="http://www.ncbi.nlm.nih.gov/pubmed/34381497" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b105" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>105. </span> <span data-v-5751490a>Gao X, Wang R, Demmel N, Cremers D. LDSO: direct sparse odometry with loop closure. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS); 2018 Oct 01-05; Madrid, Spain. IEEE; 2019. p. 2198-204.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/IROS.2018.8593376" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b106" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>106. </span> <span data-v-5751490a>Tang&nbsp;C, Zheng&nbsp;X, Tang&nbsp;C. Adaptive discriminative regions learning network for remote sensing scene classification. <i>Sensors</i> 2023;23:1-5.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.3390/s23020773" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b107" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>107. </span> <span data-v-5751490a>Song&nbsp;Y, Feng&nbsp;W, Dauphin&nbsp;G, Long&nbsp;Y, Quan&nbsp;Y, Xing&nbsp;M. Ensemble alignment subspace adaptation method for cross-scene classification. <i>IEEE Geosci Remote Sensing Lett</i> 2023;20:1-5.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/LGRS.2023.3256348" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b108" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>108. </span> <span data-v-5751490a>Zhu&nbsp;S, Wu&nbsp;C, Du&nbsp;B, Zhang&nbsp;L. Adversarial divergence training for universal cross-scene classification. <i>IEEE Trans Geosci Remote Sens</i> 2023;61:1-12.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TGRS.2023.3274781" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b109" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>109. </span> <span data-v-5751490a>Ni&nbsp;J, Shen&nbsp;K, Chen&nbsp;Y, Cao&nbsp;W, Yang&nbsp;SX. An improved deep network-based scene classification method for self-driving cars. <i>IEEE Trans Instrum Meas</i> 2022;71:1-14.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/TIM.2022.3146923" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b110" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>110. </span> <span data-v-5751490a>Mohapatra RK, Shaswat K, Kedia S. Offline handwritten signature verification using CNN inspired by inception V1 architecture. In: 2019 Fifth International Conference on Image Information Processing (ICIIP); 2019 Nov 15-17; Shimla, India. IEEE; 2020. p. 263-7.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ICIIP47207.2019.8985925" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b111" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>111. </span> <span data-v-5751490a>McCall&nbsp;R, McGee&nbsp;F, Mirnig&nbsp;A, et al. A taxonomy of autonomous vehicle handover situations. <i>Transp Res Part A Policy Pract</i> 2019;124:507-22.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1016/j.tra.2018.05.005" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div><div id="b112" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>112. </span> <span data-v-5751490a>Wang&nbsp;L, Guo&nbsp;S, Huang&nbsp;W, Xiong&nbsp;Y, Qiao&nbsp;Y. Knowledge guided disambiguation for large-scale scene classification with multi-resolution CNNs. <i>IEEE Trans Image Process</i> 2017;26:2055-68.</span></p> <div class="refrences" data-v-5751490a><!----> <a href="http://www.ncbi.nlm.nih.gov/pubmed/28252402" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b113" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>113. </span> <span data-v-5751490a>Hosny&nbsp;KM, Kassem&nbsp;MA, Fouad&nbsp;MM. Classification of skin lesions into seven classes using transfer learning with AlexNet. <i>J Digit Imaging</i> 2020;33:1325-34.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1007/s10278-020-00371-9" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/32607904" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>PubMed</span></button></a> <!----></div></div><div id="b114" class="references_item" data-v-5751490a><p data-v-5751490a><span data-v-5751490a>114. </span> <span data-v-5751490a>Alhichri&nbsp;H, Alsuwayed&nbsp;A, Bazi&nbsp;Y, Ammour&nbsp;N, Alajlan&nbsp;NA. Classification of remote sensing images using EfficientNet-B3 CNN model with attention. <i>IEEE Access</i> 2021;9:14078-94.</span></p> <div class="refrences" data-v-5751490a><a href="https://dx.doi.org/10.1109/ACCESS.2021.3051085" target="_blank" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>DOI</span></button></a> <!----> <!----></div></div></div> <div class="line" data-v-5751490a></div></div> <div class="article_cite cite_layout" data-v-5751490a><div id="cite" data-v-5751490a></div> <div class="el-row" style="margin-left:-10px;margin-right:-10px;" data-v-5751490a><div class="el-col el-col-24 el-col-xs-24 el-col-sm-16" style="padding-left:10px;padding-right:10px;" data-v-5751490a><div class="left_box" data-v-5751490a><div data-v-5751490a><h2 style="margin-top:0 !important;padding-top:0;" data-v-5751490a>
                              Cite This Article
                            </h2> <div class="cite_article" data-v-5751490a><div class="cite_article_sec" data-v-5751490a>
                                Review
                              </div> <div class="cite_article_open" style="color:#aa0c2f;" data-v-5751490a><img src="https://g.oaes.cc/oae/nuxt/img/open_icon.bff5dde.png" alt="" style="width:10px;" data-v-5751490a>
                                Open Access
                              </div> <div class="cite_article_tit" data-v-5751490a><span data-v-5751490a>Deep learning-based scene understanding for autonomous robots: a survey</span></div> <div class="cite_article_editor" data-v-5751490a><span data-v-5751490a>Jianjun  Ni<a href='http://orcid.org/0000-0002-7130-8331' target='_blank'><img src='https://i.oaes.cc/images/orcid.png' class='author_id' alt='Jianjun  Ni'></a>, ... Pengfei  Shi<a href='https://orcid.org/0000-0002-2966-1676' target='_blank'><img src='https://i.oaes.cc/images/orcid.png' class='author_id' alt='Pengfei  Shi'></a></span></div></div></div> <div class="color_000" data-v-5751490a><h2 data-v-5751490a>How to Cite</h2> <!----></div> <div data-v-5751490a><h2 data-v-5751490a>Download Citation</h2> <div class="font_12" data-v-5751490a>
                              If you have the appropriate software installed,
                              you can download article citation data to the
                              citation manager of your choice. Simply select
                              your manager software from the list below and
                              click on download.
                            </div></div> <div class="line" data-v-5751490a></div> <div data-v-5751490a><h2 data-v-5751490a>Export Citation File:</h2> <div style="margin-top:10px;" data-v-5751490a><label role="radio" aria-checked="true" tabindex="0" class="el-radio is-checked" style="display:block;margin-bottom:20px;" data-v-5751490a><span class="el-radio__input is-checked"><span class="el-radio__inner"></span><input type="radio" aria-hidden="true" tabindex="-1" autocomplete="off" value="1" checked="checked" class="el-radio__original"></span><span class="el-radio__label">RIS<!----></span></label> <label role="radio" tabindex="0" class="el-radio" style="display:block;margin-bottom:20px;" data-v-5751490a><span class="el-radio__input"><span class="el-radio__inner"></span><input type="radio" aria-hidden="true" tabindex="-1" autocomplete="off" value="2" class="el-radio__original"></span><span class="el-radio__label">BibTeX<!----></span></label> <label role="radio" tabindex="0" class="el-radio" style="display:block;margin-bottom:20px;" data-v-5751490a><span class="el-radio__input"><span class="el-radio__inner"></span><input type="radio" aria-hidden="true" tabindex="-1" autocomplete="off" value="3" class="el-radio__original"></span><span class="el-radio__label">EndNote<!----></span></label></div></div> <div class="line" data-v-5751490a></div> <div data-v-5751490a><h2 data-v-5751490a>Type of Import</h2> <div style="margin-top:10px;" data-v-5751490a><label role="radio" aria-checked="true" tabindex="0" class="el-radio is-checked" style="display:block;margin-bottom:20px;" data-v-5751490a><span class="el-radio__input is-checked"><span class="el-radio__inner"></span><input type="radio" aria-hidden="true" tabindex="-1" autocomplete="off" value="1" checked="checked" class="el-radio__original"></span><span class="el-radio__label">Direct Import<!----></span></label> <label role="radio" tabindex="0" class="el-radio" style="display:block;margin-bottom:20px;" data-v-5751490a><span class="el-radio__input"><span class="el-radio__inner"></span><input type="radio" aria-hidden="true" tabindex="-1" autocomplete="off" value="2" class="el-radio__original"></span><span class="el-radio__label">Indirect Import<!----></span></label></div></div> <div data-v-5751490a><button type="button" class="el-button el-button--primary" data-v-5751490a><!----><!----><span>Download</span></button></div></div></div> <div class="el-col el-col-24 el-col-xs-24 el-col-sm-8" style="padding-left:10px;padding-right:10px;" data-v-5751490a><div class="grid-content bg-purple" data-v-5751490a><div data-v-5751490a><h3 style="margin-top:0;" data-v-5751490a>
                              Tips on Downloading Citation
                            </h3> <div class="color_666 font_12" data-v-5751490a>
                              This feature enables you to download the
                              bibliographic information (also called citation
                              data, header data, or metadata) for the articles
                              on our site.
                            </div></div> <div data-v-5751490a><h3 data-v-5751490a>Citation Manager File Format</h3> <div class="color_666 font_12" data-v-5751490a>
                              Use the radio buttons to choose how to format
                              the bibliographic data you're harvesting.
                              Several citation manager formats are available,
                              including EndNote and BibTex.
                            </div></div> <div data-v-5751490a><h3 data-v-5751490a>Type of Import</h3> <div class="color_666 font_12" data-v-5751490a>
                              If you have citation management software
                              installed on your computer your Web browser
                              should be able to import metadata directly into
                              your reference database.<br data-v-5751490a><br data-v-5751490a> <b data-v-5751490a>Direct Import:</b> When the Direct Import
                              option is selected (the default state), a
                              dialogue box will give you the option to Save or
                              Open the downloaded citation data. Choosing Open
                              will either launch your citation manager or give
                              you a choice of applications with which to use
                              the metadata. The Save option saves the file
                              locally for later use.<br data-v-5751490a><br data-v-5751490a> <b data-v-5751490a>Indirect Import:</b> When the Indirect Import
                              option is selected, the metadata is displayed
                              and may be copied and pasted as needed.
                            </div></div></div></div></div></div> <!----> <h2 id="about" data-v-5751490a>About This Article</h2> <h3 class="btn_h3" data-v-5751490a>
                    Special Topic
                  </h3> <div class="News_abstract mag_top10 font12 font-999" data-v-5751490a>
                    This article belongs to the Special Topic
                    <a href="/specials/ir.1243" data-v-5751490a><span data-v-5751490a>Scene Understanding for Autonomous Robotics</span></a></div> <!----> <div data-v-5751490a><b data-v-5751490a>Disclaimer/Publisher’s Note:</b> All statements, opinions, and data contained in this publication are solely those of the individual author(s) and contributor(s) and do not necessarily reflect those of OAE and/or the editor(s). OAE and/or the editor(s) disclaim any responsibility for harm to persons or property resulting from the use of any ideas, methods, instructions, or products mentioned in the content.
                  </div> <h3 id="copyright" class="btn_h3" data-v-5751490a>Copyright</h3> <div class="CorrsAdd" data-v-5751490a><div class="cor_left" data-v-5751490a><a href="https://creativecommons.org/licenses/by/4.0/" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/ccb_4.229daa2.png" class="media-object" data-v-5751490a></a></div> <div class="cor_right font12" data-v-5751490a>© The Author(s) 2023. <b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License (<a target="_blank"href="https://creativecommons.org/licenses/by/4.0/" xmlns:xlink="http://www.w3.org/1999/xlink">https://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, sharing, adaptation, distribution and reproduction in any medium or format, for any purpose, even commercially, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</div></div> <!----> <!----> <!----> <!----> <!----> <div class="clearfix" data-v-5751490a></div> <div class="line_box" data-v-5751490a></div> <!----> <h2 id="data" data-v-5751490a>Data &amp; Comments</h2> <h3 class="btn_h3" data-v-5751490a>Data</h3> <div class="article_viewnum" data-v-5751490a><div class="viewnum_item" data-v-5751490a><div class="item_top" data-v-5751490a><b data-v-5751490a>Views</b></div> <div class="item_ctn" data-v-5751490a>2944</div></div> <div class="viewnum_item" data-v-5751490a><div class="item_top" data-v-5751490a><b data-v-5751490a>Downloads</b></div> <div class="item_ctn" data-v-5751490a>1438</div></div> <div class="viewnum_item" data-v-5751490a><div class="item_top" data-v-5751490a><b data-v-5751490a>Citations</b></div> <div class="item_ctn" data-v-5751490a><img alt="" src="data:image/png;base64,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" class="Crossref" data-v-5751490a> <a href="/articles//citation/" target="_blank" style="color:#4475e1;" data-v-5751490a>37</a></div></div> <!----> <div class="viewnum_item" data-v-5751490a><div class="item_top" data-v-5751490a><b data-v-5751490a>Comments</b></div> <div class="item_ctn" data-v-5751490a>0</div></div> <div class="viewnum_item" data-v-5751490a><div class="item_top" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/like.08d1ca4.png" data-v-5751490a></div> <span class="num" data-v-5751490a><b data-v-5751490a>9</b></span></div></div> <div class="article_comments" data-v-5751490a><h3 id="comment" class="btn_h3" data-v-5751490a>Comments</h3> <p data-v-5751490a>
                      Comments must be written in English. Spam, offensive
                      content, impersonation, and private information will not
                      be permitted. If any comment is reported and identified
                      as inappropriate content by OAE staff, the comment will
                      be removed without notice. If you have any queries or
                      need any help, please contact us at
                      <a href="/cdn-cgi/l/email-protection#8bf8fefbfbe4f9ffcbe4eaeefbfee9e7e2f8e3a5e8e4e6" data-v-5751490a><span class="__cf_email__" data-cfemail="fc8f898c8c938e88bc939d998c899e90958f94d29f9391">[email&#160;protected]</span></a>.
                    </p> <div class="commentlist" data-v-5751490a> <div style="height:70px;display:none;" data-v-5751490a></div> <div id="comment_input" class="commentinput" data-v-5751490a><div class="contain contain_log" data-v-5751490a><div class="user_icon" data-v-5751490a><img src="https://g.oaes.cc/oae/nuxt/img/comments_u25.e5672eb.png" alt="" data-v-5751490a></div> <div class="el-textarea height30" data-v-5751490a><textarea autocomplete="off" placeholder="Write your comments here…" class="el-textarea__inner"></textarea><!----></div> <div class="contain_right" data-v-5751490a><button disabled="disabled" type="button" class="el-button postbtn el-button--text el-button--mini is-disabled" data-v-5751490a><!----><!----><span>Post</span></button> <button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span>Login</span></button> <div class="el-badge item" data-v-5751490a><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span><i class="icon-comment" data-v-5751490a></i></span></button><sup class="el-badge__content is-fixed">0</sup></div> <button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span><i class="icon-like-line font20" data-v-5751490a></i></span></button> <span data-v-5751490a><div role="tooltip" id="el-popover-1713" aria-hidden="true" class="el-popover el-popper" style="width:170px;display:none;"><!----><div class="icon_share" style="text-align:right;margin:0;" data-v-5751490a><a href="https://pinterest.com/pin/create/button/?url=&amp;media=&amp;description=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="pinterest-sign" data-v-5751490a><i class="iconfont icon-pinterest" data-v-5751490a></i></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="facebook-sign" data-v-5751490a><i aria-hidden="true" class="iconfont icon-facebook" data-v-5751490a></i></a> <a href="https://twitter.com/intent/tweet?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="twitter-sign" data-v-5751490a><i class="iconfont icon-tuite1" data-v-5751490a></i></a> <a href="https://www.linkedin.com/shareArticle?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="linkedin-sign" data-v-5751490a><i class="iconfont icon-linkedin" data-v-5751490a></i></a></div> </div><span class="el-popover__reference-wrapper"><button type="button" class="el-button el-button--text el-button--mini" data-v-5751490a><!----><!----><span><i class="iconfont icon-zhuanfa" data-v-5751490a></i></span></button></span></span></div></div></div></div></div></div></div></div></div></div> <div class="hidden-sm-and-down box_right el-col el-col-24 el-col-md-6" style="padding-left:10px;padding-right:10px;" data-v-5751490a><div class="art_right pad_l_10" data-v-5751490a><div class="top_banner" data-v-5751490a><div class="oae_header" data-v-5751490a>
                Author's Talk
              </div> <div class="line" data-v-5751490a></div> <div class="img_box" data-v-5751490a><img src="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" alt="" data-itemid="6014" data-itemhref="https://v.oaes.cc/uploads/20230816/a802fc5ba47f4c08ba0a95089779d731.mp4" data-itemimg="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" data-v-5751490a> <i data-itemid="6014" data-itemhref="https://v.oaes.cc/uploads/20230816/a802fc5ba47f4c08ba0a95089779d731.mp4" data-itemimg="https://i.oaes.cc/uploads/20240205/5aa534648ac046f0ba42673856c3bb89.jpg" class="bo_icon" data-v-5751490a></i></div></div> <!----> <a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022_down.pdf?v=10" data-v-5751490a><div class="down_pdf" data-v-5751490a><span data-v-5751490a>Download PDF</span> <i class="el-icon-download" data-v-5751490a></i></div></a> <div class="right_btn" data-v-5751490a><div class="btn_item" data-v-5751490a><a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022.xml?v=10" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/xml.6704117.png" data-v-5751490a> <span class="name" data-v-5751490a>Download XML</span> <span class="num" data-v-5751490a><b data-v-5751490a>6</b> downloads
                  </span></a></div> <div class="btn_item" data-v-5751490a><a href="/articles//cite/" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/cite.7c6f3cb.png" data-v-5751490a> <span class="name" data-v-5751490a>Cite This Article</span> <span class="num" data-v-5751490a><b data-v-5751490a>34</b> clicks
                  </span></a></div> <div class="btn_item" data-v-5751490a><a href="https://f.oaes.cc/ris/6014.ris" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/cita.fa0c5fb.png" data-v-5751490a> <span class="name" data-v-5751490a>Export Citation</span> <span class="num" data-v-5751490a><b data-v-5751490a>7</b>
                    clicks
                  </span></a></div> <div class="btn_item" data-v-5751490a><img alt="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACIAAAAkCAYAAADsHujfAAADL0lEQVRYhcWXS08aURTHz8wd7EAcHo0EU3RpKjuNjYmm9EO4tLuGlZ/BNsb0I7gqpE1DYGMwXVm36kqjJtYFamINLqoCcQbBoMAwzZn0moHC3OHh+E/IvC7n/uace86ZywEArKysBAOBwKKmae8dDocEfVK1WlVUVf0qy/LnhYUF2cwqF41Ggz6f7xch5GW/AJqlquofRVHeRiKR83ZjeK/X++kpIVCEkFcej+dnLBZrOw8PAPNPCUElCMJrSZJ+tAURBGHQDhDUwMBAOJlMplqC2AVB5XQ65xKJRPQ/EE3T9BM7jy6XK5JIJJYbQOyGoEdRFD/G4/EPDR55jh9KFMUv8Xj8HZ5zqVRK61P4u1KtVisrivJGoHQsCYIAgUAA3G43lMtluL6+1o9UHMfB8PAwSJIE9Xod8vk8KIrCtEsIcXo8nkVLICMjIzA1NQUOh6Ph/sXFBRwcHOiAExMTmBENzy8vL2FnZwffmjXFHLe6umpKgpOEw+G2z+/v7zHWbZ/ncjnY2toC1gszF+vk5KSpATMIlN/vh9HRUWZCmBY0n88Hg4O9F95gMMgcY7pGcGH2Q7iAWaExBbGaUSxR95vJNDSlUqkvIFbs8M2kxuubmxsrqcdUNpttad94/dhrjO6j56qqwtnZWU8QaCOTybS0b7xmpu/x8XFPXjk5OdErMDN9WQPu7u5gb2+vKwhZluHo6MhSU7VU4jE8Xq8XxsfHLUOgF7CiWvWmYNXw/v6+Th4KhZhji8UibG5udpR1lkFQ2OCwo05PT2PXbDnm6uoKtre3oVKpWLaL/cryZwAVhgkb2ezsLAwNDT3ex+w4PDyEdDrdcSEsFAqdg9A/bmxswNjYmN4UEWx3d7erAlitVvVQdhQao/AFTk9PdQ+hN7oVZhZ+SHXlEaN6qTGYWegNeI59DRV6EUNKHdGzR7oRzokQuD6obPcIQmAzxYptlFCr1UqEEFv2vxTi9va2+VEGe03Sjg0WXRMtIFBrfKVSWVZVNfeUnnh4eNC3Fs3h+KdznueXyfr6enFmZua7KIquer0e4jjuRb8A0AvYEjAcLWrNbwD4RgiZX1paKvwFjW5L4jSVRrUAAAAASUVORK5CYII=" data-v-5751490a> <span class="name" data-v-5751490a>Like This Article</span> <span class="num" data-v-5751490a><b data-v-5751490a>9</b>
                  likes
                </span></div></div> <!----> <div class="right_list" data-v-5751490a><div class="oae_header" data-v-5751490a>Share This Article</div> <div class="icon_share" data-v-5751490a><a href="https://pinterest.com/pin/create/button/?url=&amp;media=&amp;description=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="pinterest-sign" data-v-5751490a><i class="iconfont icon-pinterest" data-v-5751490a></i></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="facebook-sign" data-v-5751490a><i aria-hidden="true" class="iconfont icon-facebook" data-v-5751490a></i></a> <a href="https://twitter.com/intent/tweet?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="twitter-sign" data-v-5751490a><i class="iconfont icon-tuite1" data-v-5751490a></i></a> <a href="https://www.linkedin.com/shareArticle?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="linkedin-sign" data-v-5751490a><i class="iconfont icon-linkedin" data-v-5751490a></i></a></div> <div class="Journal_qrcode m-top30" data-v-5751490a><img title="https://www.oaepublish.com/articles/ir.2023.22" alt="" src="https://api.qrserver.com/v1/create-qr-code/?size=80x80&amp;data=https://www.oaepublish.com/articles/ir.2023.22" class="code" data-v-5751490a> <span class="tip" data-v-5751490a>Scan the QR code for reading!</span></div></div> <div class="right_list" data-v-5751490a><div class="oae_header" data-v-5751490a>See Updates</div> <div class="checkNew" data-v-5751490a><img alt="" src="https://g.oaes.cc/oae/nuxt/img/CROSSMARK_Color_horizontal.a6fa1ee.svg" data-v-5751490a></div></div> <div id="boxFixed" class="right_tab" data-v-5751490a><div class="a_tab_top" data-v-5751490a><div class="top_item tab_active1" data-v-5751490a>
                  Contents
                </div> <div class="top_item" data-v-5751490a>
                  Figures
                </div> <div class="top_item" data-v-5751490a>
                  Related
                </div></div> <div class="tab_ctn" data-v-5751490a><div class="ctn_item" data-v-5751490a></div></div> <!----></div></div></div></div></div></div> <div class="ipad_menu" data-v-5751490a><span class="el-icon-s-unfold" data-v-5751490a></span></div> <div class="ipad_con" style="display:none;" data-v-5751490a><div id="Share-con" class="Journal_right_Share publish-content" data-v-5751490a><h2 data-v-5751490a>Share This Article</h2> <div class="line_btn" data-v-5751490a></div> <div class="shareicon-Con" data-v-5751490a><a href="https://pinterest.com/pin/create/button/?url=&amp;media=&amp;description=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="pinterest-sign" data-v-5751490a><i class="iconfont icon-pinterest" data-v-5751490a></i></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="facebook-sign" data-v-5751490a><i aria-hidden="true" class="iconfont icon-facebook" data-v-5751490a></i></a> <a href="https://twitter.com/intent/tweet?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="twitter-sign" data-v-5751490a><i class="iconfont icon-tuite1" data-v-5751490a></i></a> <a href="https://www.linkedin.com/shareArticle?url=https://www.oaepublish.com/articles/ir.2023.22" target="_blank" class="linkedin-sign" data-v-5751490a><i class="iconfont icon-linkedin" data-v-5751490a></i></a> <a href="javascript:void(0);" id="wx_wd" class="weixin-sign" data-v-5751490a><i aria-hidden="true" class="iconfont icon-weixin" data-v-5751490a></i></a> <div class="wx_code" style="display: none" data-v-5751490a><div class="arrow" style="top: 50%; z-idnex: 9999" data-v-5751490a></div> <img src="https://api.qrserver.com/v1/create-qr-code/?size=80x80&amp;data=https://www.oaepublish.comarticle/view/426" alt class="code" data-v-5751490a></div> <a id="wx_ph" onclick="call('wechatFriend')" target="_blank" class="weixin-sign" style="display: none" data-v-5751490a><i aria-hidden="true" class="fab2 fa-weixin" data-v-5751490a></i></a></div></div> <div class="ipad_btn" data-v-5751490a><ul class="btn_list" data-v-5751490a><div class="btn_item" data-v-5751490a><a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022_down.pdf?v=10" data-v-5751490a><img alt src="https://g.oaes.cc/oae/nuxt/img/pdf.c310b0c.png" data-v-5751490a> <span class="name" data-v-5751490a>Download PDF (1438)</span></a></div> <div class="btn_item" data-v-5751490a><a href="https://f.oaes.cc/xmlpdf/1fac2add-6180-4dc1-83e1-e46b3dc9e6c5/ir3022.xml?v=10" data-v-5751490a><img alt src="https://g.oaes.cc/oae/nuxt/img/xml.6704117.png" data-v-5751490a> <span class="name" data-v-5751490a>Download XML (6)</span></a></div> <div class="btn_item" data-v-5751490a><img alt src="https://g.oaes.cc/oae/nuxt/img/cite.7c6f3cb.png" data-v-5751490a> <span class="name" data-v-5751490a>Cite This Article (34)</span></div> <div class="btn_item" data-v-5751490a><a data-v-5751490a><img alt src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAAAXNSR0IArs4c6QAAAARzQklUCAgICHwIZIgAAAKSSURBVFiF7ZhNSBRhGMd/z8y629qa28dBcDUowS6RJEKHDgNGfkEQRXTtXqB26NhCp4xiJbsFBR76sINdLInQaygUhBAaqa2SlGFk2dc6TwetXHJn3Jlx89D/Ns8+7//9zfufeedlIQBZyZFYED4Ahl+Dls7J2mi85F1L6nVDEEDiZ3Bzx3gZYXMISIB+WLSNuv72ipd+PD2vkJUcCFFkdi/BAEjcNOwev/F5BiqOV11AOJRdlZrieMlVP0CeImtOpY+B3svZoJzua6u8VhCgxtR0tbA4LJAzGlUyInKwr7XiSb7+eUVmJUdiBpleJxgAEUJg9zZ3jJetK1C0NHYDZM/auqVMw+YtKzkQWhegplT6nIgcz8dcwIrGd1/Kc4y7Gjun6sW2Hy5Fkb8UTj5orbwTCFDTxbGEhCNPEXZ4gVkG+mTbeqC/feeIW697ZJFItx8YAIGYaZB7m1gh9whUrmMwmFVStQQsh0HPEKM3u2TMrAXI28bYmT6PatKh5WZfa+UpL95ZQI2p6SOimV2C6Rilih4WNOfXXWFI1LjtOrutkwvzifuDScn8Kv2OrOFKukrUbgEDRV3uQh2fKVFKFa12BTKoLt6SngSG/5SWZar4Pht5kS1G0crrfwLhpP9AbgoMaNvHN9S9eETk+4IvH0/fptV0tucMW+ffMrjvKHetNs8+ga3Qt9AmAL6GN/vyCWyFLp/oonz2FWOJGl8+gQF9jsYZrdi/dOG8rzpqw71l3lZIeQ6M5v6ZH4UFAlQkdzCqnkPzBiTsFbTWoWNUyT5DrSuQKu/FITJgTYex1eQxMplQmPA6qZM23Fu2cYG06MsMMFfY6XVBTDPr/6S/Dvn1XVPbC4XzeLZ8jqTYK2s/AWtatIa1NHXoAAAAAElFTkSuQmCC" data-v-5751490a> <span class="name" data-v-5751490a>Export Citation (7)</span></a></div></ul></div> <div id="Updates-con2" class="Journal-right-Updates publish-content" data-v-5751490a><h2 data-v-5751490a>See Updates</h2> <img alt src="https://g.oaes.cc/oae/nuxt/img/CROSSMARK_Color_horizontal.a6fa1ee.svg" type="image/svg+xml" width="150" data-v-5751490a></div> <div id="Contents-con" class="Journal-right-Contents publish-content" data-v-5751490a><div id="scrollspy" class="scroll-box" data-v-5751490a><div data-v-5751490a></div></div></div></div> <div class="imgDolg" style="display:none;" data-v-5751490a><div class="img_btn_box" data-v-5751490a><i class="el-icon-error img_btn" data-v-5751490a></i> <i class="el-icon-circle-plus img_btn" data-v-5751490a></i> <i class="el-icon-remove img_btn" data-v-5751490a></i></div> <img alt src="" style="transform:scale(1);" data-v-5751490a></div> <div class="el-dialog__wrapper" style="display:none;" data-v-5751490a><div role="dialog" aria-modal="true" aria-label="Citation" class="el-dialog" style="margin-top:15vh;width:700px;"><div class="el-dialog__header"><span class="el-dialog__title">Citation</span><button type="button" aria-label="Close" class="el-dialog__headerbtn"><i class="el-dialog__close el-icon el-icon-close"></i></button></div><!----><!----></div></div> <div class="el-dialog__wrapper delogCheck" style="display:none;" data-v-5751490a><div role="dialog" aria-modal="true" aria-label="CrossMark" class="el-dialog" style="margin-top:15vh;width:700px;"><div class="el-dialog__header"><span class="el-dialog__title">CrossMark</span><button type="button" aria-label="Close" class="el-dialog__headerbtn"><i class="el-dialog__close el-icon el-icon-close"></i></button></div><!----><!----></div></div> <div class="el-dialog__wrapper delogTable" style="display:none;" data-v-5751490a><div role="dialog" aria-modal="true" aria-label="dialog" class="el-dialog" style="margin-top:15vh;width:800px;"><div class="el-dialog__header"><span class="el-dialog__title"></span><button type="button" aria-label="Close" class="el-dialog__headerbtn"><i class="el-dialog__close el-icon el-icon-close"></i></button></div><!----><!----></div></div> <div data-v-513adf11 data-v-5751490a><div class="el-dialog__wrapper loginDol" style="display:none;" data-v-513adf11><div role="dialog" aria-modal="true" aria-label="Login to Online Publication System" class="el-dialog" style="margin-top:15vh;width:700px;"><div class="el-dialog__header"><span class="el-dialog__title">Login to Online Publication System</span><button type="button" aria-label="Close" class="el-dialog__headerbtn"><i class="el-dialog__close el-icon el-icon-close"></i></button></div><!----><!----></div></div></div></div></main> <div data-v-11ddc367 data-v-0bee1158><!----></div> <div class="PcComment" data-v-0bb094a3 data-v-0bee1158><div class="foot_one clo_bor" data-v-0bb094a3><div class="wrapper foot_box" data-v-0bb094a3><div class="mgb_20 el-row" style="margin-left:-10px;margin-right:-10px;" data-v-0bb094a3><div class="el-col el-col-24 el-col-sm-12 el-col-md-8" style="padding-left:10px;padding-right:10px;" data-v-0bb094a3><div class="grid-content" data-v-0bb094a3><div class="foot_title" data-v-0bb094a3>Intelligence &amp; Robotics</div> <div class="foot-con" data-v-0bb094a3><div data-v-0bb094a3>ISSN 2770-3541 (Online)</div> <div data-v-0bb094a3><a href="/cdn-cgi/l/email-protection#2346474a574c514a424f634a4d57464f4f514c414c570d404c4e" data-v-0bb094a3><span class="__cf_email__" data-cfemail="badfded3ced5c8d3dbd6fad3d4cedfd6d6c8d5d8d5ce94d9d5d7">[email&#160;protected]</span></a></div></div></div></div> <div class="hidden-sm-and-down el-col el-col-24 el-col-sm-12 el-col-md-8" style="padding-left:10px;padding-right:10px;" data-v-0bb094a3><div class="grid-content" data-v-0bb094a3><div class="foot_title" data-v-0bb094a3>Navigation</div> <div data-v-351032c4="" class="foot-con" data-v-0bb094a3><div data-v-0bb094a3><a href="/ir/contact_us" data-v-0bb094a3>Contact Us</a></div><div data-v-0bb094a3><a href="/ir/sitemap" data-v-0bb094a3>Sitemap</a></div></div></div></div> <div class="el-col el-col-24 el-col-sm-12 el-col-md-8" style="padding-left:10px;padding-right:10px;" data-v-0bb094a3><div class="grid-content" data-v-0bb094a3><div class="foot_title" data-v-0bb094a3>Follow Us</div> <div class="oaemedia-link" data-v-0bb094a3><ul data-v-0bb094a3><li data-v-0bb094a3><a href="https://www.linkedin.com/in/irene-liu-799041249/" rel="nofollow" target="_blank" data-v-0bb094a3><span class="bk LinkedIn" data-v-0bb094a3><i class="iconfont icon-linkedin" data-v-0bb094a3></i></span>LinkedIn</a></li><li data-v-0bb094a3><a href="https://twitter.com/OAE_IR" rel="nofollow" target="_blank" data-v-0bb094a3><span class="bk Twitter" data-v-0bb094a3><i class="iconfont icon-tuite1" data-v-0bb094a3></i></span>Twitter</a></li> <li data-v-0bb094a3><a href="javascript:void(0);" class="weixin-sign-and" data-v-0bb094a3><span class="bk wxbk" data-v-0bb094a3><i aria-hidden="true" class="iconfont icon-weixin" data-v-0bb094a3></i></span>WeChat</a></li> <div class="wx_code_and wx_code2" style="display:none;" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20250411/d65c4b141d53429ab98ad2649306cc9c.jpg" alt="" class="code" style="width:100px;" data-v-0bb094a3></div></ul></div></div></div></div> <div class="pad_box" data-v-0bb094a3><div class="foot_nav1" data-v-0bb094a3><div class="foot_title" data-v-0bb094a3>Navigation</div> <div data-v-351032c4 class="foot-con" data-v-0bb094a3><div data-v-0bb094a3><a href="/ir/contact_us" data-v-0bb094a3>Contact Us</a></div><div data-v-0bb094a3><a href="/ir/sitemap" data-v-0bb094a3>Sitemap</a></div></div></div> <div class="foot_box" data-v-0bb094a3><div class="foot_cont" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20230811/49f92f416c9845b58a01de02ecea785f.jpg" alt data-v-0bb094a3> <div class="fot_c_right" data-v-0bb094a3><h4 data-v-0bb094a3>Committee on Publication Ethics</h4> <a href="https://members.publicationethics.org/members/intelligence-robotics" data-v-0bb094a3>https://members.publicationethics.org/members/intelligence-robotics</a></div></div> <div class="foot_cont" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20230911/67d78ebf8c55485db6ae5b5b4bcda421.jpg" alt class="img2" style="padding: 5px;" data-v-0bb094a3> <div class="fot_c_right" data-v-0bb094a3><h4 data-v-0bb094a3>Portico</h4> <p data-v-0bb094a3>All published articles are preserved here permanently:</p> <a href="https://www.portico.org/publishers/oae/" data-v-0bb094a3>https://www.portico.org/publishers/oae/</a></div></div></div></div> <div class="foot_btn hidden-sm-and-down" data-v-0bb094a3><div class="foot_cont" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20230811/49f92f416c9845b58a01de02ecea785f.jpg" alt data-v-0bb094a3> <div class="fot_c_right" data-v-0bb094a3><h4 data-v-0bb094a3>Committee on Publication Ethics</h4> <a href="https://members.publicationethics.org/members/intelligence-robotics" data-v-0bb094a3>https://members.publicationethics.org/members/intelligence-robotics</a></div></div> <div class="foot_cont" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20230911/67d78ebf8c55485db6ae5b5b4bcda421.jpg" alt class="img2" style="padding: 5px;" data-v-0bb094a3> <div class="fot_c_right" data-v-0bb094a3><h4 data-v-0bb094a3>Portico</h4> <p data-v-0bb094a3>All published articles are preserved here permanently:</p> <a href="https://www.portico.org/publishers/oae/" data-v-0bb094a3>https://www.portico.org/publishers/oae/</a></div></div></div></div> <div class="footer_container" data-v-6bfe7148 data-v-0bb094a3><div id="mjx_box" data-v-6bfe7148></div> <div class="footer_content container wrapper" data-v-6bfe7148><div class="el-row" style="margin-left:-10px;margin-right:-10px;" data-v-6bfe7148><div class="el-col el-col-24 el-col-xs-24 el-col-sm-8" style="padding-left:10px;padding-right:10px;" data-v-6bfe7148><div class="left_box" data-v-6bfe7148><a href="/" class="nuxt-link-active" style="display:inline-block;width:auto;border-bottom:none!important;" data-v-6bfe7148><img src="https://g.oaes.cc/oae/nuxt/img/oae_10th_w.d4c21db.png" alt="" class="foot_logo" style="height:70px;margin-bottom:20px;margin-top:10px;" data-v-6bfe7148></a> <div class="small-box" data-v-6bfe7148><div data-v-6bfe7148><a href="/cdn-cgi/l/email-protection#cbbbaab9bfa5aeb9b88ba4aaaebbbea9a7a2b8a3e5a8a4a6" data-v-6bfe7148><span class="__cf_email__" data-cfemail="463627343228233435062927233633242a2f352e6825292b">[email&#160;protected]</span></a></div> <div data-v-6bfe7148><a href="/about/who_we_are" data-v-6bfe7148>Company</a></div> <div data-v-6bfe7148><a href="/about/contact_us" data-v-6bfe7148>Contact Us</a></div></div></div></div> <div class="el-col el-col-24 el-col-xs-24 el-col-sm-8" style="padding-left:10px;padding-right:10px;" data-v-6bfe7148><div class="center_box" data-v-6bfe7148><div class="tit" data-v-6bfe7148>Discover Content</div> <div data-v-6bfe7148><div data-v-6bfe7148><a href="/alljournals" data-v-6bfe7148>Journals A-Z</a></div> <div data-v-6bfe7148><a href="/about/language_editing_services" data-v-6bfe7148>Language Editing</a></div> <div data-v-6bfe7148><a href="/about/layout_and_production" data-v-6bfe7148>Layout &amp; Production</a></div> <div data-v-6bfe7148><a href="/about/graphic_abstracts" data-v-6bfe7148>Graphical Abstracts</a></div> <div data-v-6bfe7148><a href="/about/video_abstracts" data-v-6bfe7148>Video Abstracts</a></div> <div data-v-6bfe7148><a href="/about/expert_lecture" data-v-6bfe7148>Expert Lecture</a></div> <div data-v-6bfe7148><a href="/about/organizer" data-v-6bfe7148>Conference Organizer</a></div> <div data-v-6bfe7148><a href="/about/collaborators" data-v-6bfe7148>Strategic Collaborators</a></div> <div data-v-6bfe7148><a href="https://www.scierxiv.com/" target="_blank" data-v-6bfe7148>ScieRxiv.com</a></div> <div data-v-6bfe7148><a href="https://www.oaescience.com/" target="_blank" data-v-6bfe7148>Think Tank</a></div></div></div></div> <div class="el-col el-col-24 el-col-xs-24 el-col-sm-8" style="padding-left:10px;padding-right:10px;" data-v-6bfe7148><div class="right_box" data-v-6bfe7148><div class="tit" data-v-6bfe7148>Follow OAE</div> <div class="small-box" data-v-6bfe7148><div class="item_cla" data-v-6bfe7148><a href="https://twitter.com/OAE_Publishing" target="_blank" data-v-6bfe7148><span class="bk color1" data-v-6bfe7148><span class="iconfont icon-tuite1" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>Twitter</span></a></div> <div class="item_cla" data-v-6bfe7148><a href="https://www.facebook.com/profile.php?id=100018840961346" target="_blank" data-v-6bfe7148><span class="bk color2" data-v-6bfe7148><span class="iconfont icon-facebook" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>Facebook</span></a></div> <div class="item_cla" data-v-6bfe7148><a href="https://www.linkedin.com/company/oae-publishing-inc/mycompany/" target="_blank" data-v-6bfe7148><span class="bk color3" data-v-6bfe7148><span class="iconfont icon-linkedin" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>LinkedIn</span></a></div> <div class="item_cla" data-v-6bfe7148><a href="https://www.youtube.com/channel/UCjlAxBPaZErsc7qJ3fFYBLg" target="_blank" data-v-6bfe7148><span class="bk color4" data-v-6bfe7148><span class="iconfont icon-youtube" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>YouTube</span></a></div> <div class="item_cla" data-v-6bfe7148><a href="https://space.bilibili.com/1259987867" target="_blank" data-v-6bfe7148><span class="bk color5" data-v-6bfe7148><span class="iconfont icon-bilibili-line" style="color:#000000;" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>BiLiBiLi</span></a></div> <div class="oaemedia-link" data-v-6bfe7148><span data-v-6bfe7148><a href="javascript:void(0);" class="weixin-sign-and" data-v-6bfe7148><span class="bk wxbk" style="margin-right:13px;" data-v-6bfe7148><i aria-hidden="true" class="iconfont icon-weixin" data-v-6bfe7148></i></span>WeChat</a></span> <div class="wx_code_and wx_code2" style="display:none;" data-v-6bfe7148><img src="https://g.oaes.cc/oae/nuxt/img/oaeshare.b5aa6a9.png" alt="" class="code" style="width:100px;" data-v-6bfe7148></div></div> <div class="item_cla" data-v-6bfe7148><a href="https://f.oaes.cc/rss/ir.xml" target="_blank" data-v-6bfe7148><span class="bk color6" data-v-6bfe7148><span class="iconfont icon-xinhao" data-v-6bfe7148></span></span> <span class="title" data-v-6bfe7148>Rss</span></a></div></div></div></div></div></div> <div class="backUp" data-v-6bfe7148><div class="go_top" style="display: flex;align-items: center;" data-v-6bfe7148><svg t="1770348047759" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1828" width="32" height="32" class="icon" data-v-6bfe7148><path d="M732.032 607.232c-12.2112 12.2112-34.2784 12.2112-46.4896 0l-139.264-139.2896V854.272c0 17.0752-14.72 31.7696-31.7952 31.7696a32.256 32.256 0 0 1-31.744-31.744V465.408l-141.9264 141.7984c-12.2112 12.2112-31.744 12.2112-46.464 0-12.2112-12.2112-12.2112-31.7696 0-46.464l220.032-220.032 217.5232 217.5232c12.3392 14.72 12.3392 36.7616 0.128 48.9728zM749.2096 294.272c0 17.1776-14.6944 31.744-31.744 31.744H306.5344a32.256 32.256 0 0 1-31.7696-31.744c0-17.0752 14.6944-31.7696 31.744-31.7696h413.2864c14.6944-2.5088 29.3888 12.2112 29.3888 31.744z" p-id="1829" fill="#0853D7" data-v-6bfe7148></path></svg></div></div> <div class="footer_text wrapper" data-v-6bfe7148><div class="mgb_10" data-v-6bfe7148>
      © 2016-2026 OAE Publishing Inc., except certain content provided by third parties
    </div> <div data-v-6bfe7148><a href="/abouts/privacy" data-v-6bfe7148>Privacy</a> <a href="/abouts/cookies" data-v-6bfe7148>Cookies</a> <a href="/abouts/terms_of_service" data-v-6bfe7148>Terms of Service</a></div></div> <!----></div></div> <div class="backUp" data-v-0bb094a3><div class="go_top" data-v-0bb094a3><i class="iconfont icon-tuite1" data-v-0bb094a3></i> <i class="iconfont icon-iconfonterweima" data-v-0bb094a3></i> <div class="wx_code" style="display:none;" data-v-0bb094a3><img src="https://i.oaes.cc/uploads/20230824/5249ddabb6d642558c9843fba9283219.png" alt class="code" style="width: 100px" data-v-0bb094a3></div> <i class="iconfont icon-ai-top" data-v-0bb094a3></i></div></div></div> <!----></div></div></div><script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co,cp,cq){bZ[0]={language:N,new_title:y,new_abstract:K,new_keywords:b_,is_check:g};bZ[1]={language:"cn",new_title:"基于深度学习的自主机器人场景理解：一项调查",new_abstract:"自主机器人是科学技术领域内备受瞩目的研究课题，对社会经济发展产生了重大影响。自主机器人感知和理解其工作环境的能力是解决更复杂问题的基础。近年来，基于人工智能的方法在自主机器人场景理解领域得到了越来越多的提出，深度学习是目前该领域的重点之一。基于深度学习的场景理解在自主机器人领域取得了显著进展。因此，本文对基于深度学习的自主机器人场景理解最新研究进行了综述。本调查提供了对机器人场景理解演变的详细概述，并总结了深度学习方法在自主机器人场景理解中的应用。此外，分析了自主机器人场景理解中的关键问题，如姿态估计、显著性预测、语义分割和物体检测。然后，总结了针对这些问题的一些代表性基于深度学习的解决方案。最后，讨论了自主机器人场景理解领域面临的未来挑战。",new_keywords:"自主机器人、场景理解、深度学习、物体检测、姿态估计",is_check:d};bZ[2]={language:"de",new_title:"Tiefenlernenbasierte Szenenverständnis für autonome Roboter: eine Übersicht",new_abstract:"Autonome Roboter sind ein heißes Forschungsthema innerhalb der Bereiche Wissenschaft und Technologie, was einen großen Einfluss auf die sozioökonomische Entwicklung hat. Die Fähigkeit des autonomen Roboters, seine Arbeitsumgebung wahrzunehmen und zu verstehen, bildet die Grundlage zur Lösung komplexerer Probleme. In den letzten Jahren wurden vermehrt auf künstlicher Intelligenz basierende Methoden im Bereich der Szenenverständnis für autonome Roboter vorgeschlagen, wobei Deep Learning eines der aktuellen Schlüsselgebiete in diesem Bereich ist. Herausragende Fortschritte wurden im Bereich des Szenenverständnisses für autonome Roboter auf Basis von Deep Learning erzielt. Daher präsentiert dieses Papier einen Überblick über aktuelle Forschung zum Deep-Learning-basierten Szenenverständnis für autonome Roboter. Diese Studie bietet einen detaillierten Überblick über die Entwicklung des robotischen Szenenverständnisses und fasst die Anwendungen von Deep-Learning-Methoden im Szenenverständnis für autonome Roboter zusammen. Darüber hinaus werden die Hauptprobleme im Szenenverständnis für autonome Roboter analysiert, wie Pose-Schätzung, Saliency-Vorhersage, semantische Segmentierung und Objekterkennung. Anschließend werden einige repräsentative Deep-Learning-Lösungen für diese Probleme zusammengefasst. Abschließend werden zukünftige Herausforderungen im Bereich des Szenenverständnisses für autonome Roboter diskutiert.",new_keywords:"Autonome Roboter, Szenenverständnis, Deep Learning, Objekterkennung, Posenschätzung.",is_check:d};bZ[3]={language:"fa",new_title:"Compréhension de la scène basée sur l'apprentissage profond pour les robots autonomes : une enquête",new_abstract:"Les robots autonomes sont un sujet de recherche très prisé dans les domaines de la science et de la technologie, ayant un impact majeur sur le développement socio-économique. La capacité du robot autonome à percevoir et comprendre son environnement de travail est la base pour résoudre des problèmes plus complexes. Ces dernières années, un nombre croissant de méthodes basées sur l'intelligence artificielle ont été proposées dans le domaine de la compréhension de scènes pour les robots autonomes, et l'apprentissage profond est l'un des domaines clés actuels dans ce domaine. Des progrès remarquables ont été réalisés dans le domaine de la compréhension de scènes pour les robots autonomes basée sur l'apprentissage profond. Ainsi, cet article présente une revue des recherches récentes sur la compréhension de scènes basée sur l'apprentissage profond pour les robots autonomes. Cette enquête offre un aperçu détaillé de l'évolution de la compréhension de scènes robotiques et résume les applications des méthodes d'apprentissage profond dans la compréhension de scènes pour les robots autonomes. De plus, les problèmes clés dans la compréhension de scènes des robots autonomes sont analysés, tels que l'estimation de pose, la prédiction de saillance, la segmentation sémantique et la détection d'objets. Ensuite, des solutions représentatives basées sur l'apprentissage profond pour ces problèmes sont résumées. Enfin, les futurs défis dans le domaine de la compréhension de scènes pour les robots autonomes sont discutés.",new_keywords:"Robots autonomes, compréhension de scène, apprentissage en profondeur, détection d'objets, estimation de la pose",is_check:d};bZ[4]={language:"jp",new_title:"自律ロボットのための深層学習ベースのシーン理解：調査",new_abstract:"\u003Cp\u003E自律ロボットは、科学技術の分野でのホットな研究テーマであり、社会経済の発展に大きな影響を与えています。自律ロボットが自身の作業環境を知覚し理解する能力は、さらに複雑な問題を解決する基盤となります。最近、自律ロボットのためのシーン理解の分野で、人工知能に基づいた手法が提案されており、その中でディープラーニングは現在の重要な分野の一つです。ディープラーニングに基づく自律ロボットのシーン理解の分野で顕著な進展が見られています。したがって、この論文では、自律ロボットのためのディープラーニングに基づくシーン理解の最近の研究を紹介します。この調査は、ロボットのシーン理解の進化の詳細な概要を提供し、自律ロボットのシーン理解におけるディープラーニングの応用をまとめています。さらに、ポーズ推定、サリエンシー予測、意味的セグメンテーション、オブジェクト検出など、自律ロボットのシーン理解における重要な課題が分析されています。その後、これらの課題に対するいくつかの代表的なディープラーニングに基づく解決策がまとめられています。最後に、自律ロボットのシーン理解の分野での将来の課題について議論されています。\u003C\u002Fp\u003E",new_keywords:"自律ロボット、シーン理解、ディープラーニング、物体検出、ポーズ推定",is_check:d};bZ[5]={language:"py",new_title:"Глубокое обучение для понимания сцены автономных роботов: обзор",new_abstract:"Автономные роботы являются горячей темой исследований в области науки и технологий, что имеет большое влияние на социально-экономическое развитие. Способность автономного робота воспринимать и понимать рабочую среду является основой для решения более сложных проблем. За последние годы в области понимания сцены для автономных роботов было предложено все больше методов на основе искусственного интеллекта, причем глубокое обучение является одной из ключевых областей в этой сфере. Были достигнуты значительные успехи в области понимания сцены для автономных роботов на основе глубокого обучения. Таким образом, в данной статье приводится обзор недавних исследований по пониманию сцены для автономных роботов на основе глубокого обучения. Этот обзор предоставляет подробное описание эволюции понимания сцен для роботов и резюмирует применение методов глубокого обучения в понимании сцены для автономных роботов. Кроме того, анализируются ключевые проблемы в понимании сцены для автономных роботов, такие как оценка позы, предсказание выдачи, семантическая сегментация и обнаружение объектов. Затем резюмируются некоторые репрезентативные решения на основе глубокого обучения для этих проблем. Наконец, обсуждаются будущие вызовы в области понимания сцены для автономных роботов.",new_keywords:"Автономные роботы, понимание сцены, глубокое обучение, обнаружение объектов, оценка позы",is_check:d};bZ[6]={language:"sk",new_title:"자율로봇을 위한 심층학습 기반 장면 이해: 설문조사",new_abstract:"자율 로봇은 과학 기술 분야에서 핫한 연구 주제이며, 사회 경제 발전에 큰 영향을 미칩니다. 자율 로봇의 작업 환경을 인지하고 이해하는 능력은 더 복잡한 문제를 해결하기 위한 기초입니다. 최근 몇 년간, 자율 로봇의 장면 이해 분야에서는 인공 지능 기반 방법이 제안되어 왔으며, 딥 러닝은 이 분야의 주요 연구 분야 중 하나입니다. 딥 러닝을 기반으로 한 장면 이해 분야에서 자율 로봇을 위한 우수한 성과를 거두었습니다. 따라서 본 논문은 자율 로봇을 위한 딥 러닝 기반 장면 이해에 대한 최근 연구를 검토합니다. 이 조사는 로봇의 장면 이해의 진화에 대한 상세한 개요를 제공하고, 자율 로봇을 위한 장면 이해에서 딥 러닝 방법의 응용을 요약합니다. 또한, 자율 로봇의 장면 이해에서 자세 추정, 주목도 예측, 의미 분할 및 물체 감지와 같은 주요 문제가 분석되었습니다. 그런 다음, 이러한 문제에 대한 대표적인 딥 러닝 기반 솔루션을 요약했습니다. 마지막으로, 자율 로봇의 장면 이해 분야에서의 미래 도전 과제에 대해 논의합니다.",new_keywords:"자율 로봇, 장면 이해, 심층 학습, 물체 감지, 자세 측정",is_check:d};bZ[7]={language:"it",new_title:"Comprensione della scena basata sull'apprendimento profondo per robot autonomi: un sondaggio",new_abstract:"I robot autonomi sono un argomento di ricerca molto caldo nei campi della scienza e della tecnologia, che ha un grande impatto sullo sviluppo socio-economico. La capacità del robot autonomo di percepire e comprendere il proprio ambiente di lavoro è la base per risolvere problemi più complicati. Negli ultimi anni, un numero sempre maggiore di metodi basati sull'intelligenza artificiale sono stati proposti nel campo della comprensione della scena per i robot autonomi, e il deep learning è una delle attuali aree chiave in questo campo. Sono stati ottenuti notevoli progressi nel campo della comprensione della scena per i robot autonomi basata sul deep learning. Pertanto, questo articolo presenta una panoramica delle ricerche recenti sulla comprensione della scena basata sul deep learning per i robot autonomi. Questo studio fornisce una panoramica dettagliata dell'evoluzione della comprensione della scena dei robotica e riassume le applicazioni dei metodi di deep learning nella comprensione della scena per i robot autonomi. Inoltre, vengono analizzati i principali problemi nella comprensione della scena dei robot autonomi, come la stima della posa, la predizione della salienza, la segmentazione semantica e il rilevamento degli oggetti. Successivamente, vengono riassunte alcune soluzioni rappresentative basate sul deep learning per questi problemi. Infine, vengono discusse le sfide future nel campo della comprensione della scena per i robot autonomi.",new_keywords:"Robot autonomi, comprensione della scena, apprendimento profondo, rilevamento di oggetti, stima della posa.",is_check:d};bZ[8]={language:"fs",new_title:"La comprensión de escenas basada en aprendizaje profundo para robots autónomos: una encuesta.",new_abstract:"Los robots autónomos son un tema de investigación candente dentro de los campos de la ciencia y la tecnología, que tiene un gran impacto en el desarrollo socioeconómico. La capacidad del robot autónomo para percibir y comprender su entorno de trabajo es la base para resolver problemas más complicados. En los últimos años, se han propuesto un número creciente de métodos basados en inteligencia artificial en el campo de la comprensión de escenas para robots autónomos, y el aprendizaje profundo es una de las áreas clave actuales en este campo. Se han logrado avances destacados en el campo de la comprensión de escenas para robots autónomos basados en el aprendizaje profundo. Por lo tanto, en este documento se presenta una revisión de la investigación reciente sobre la comprensión de escenas basada en el aprendizaje profundo para robots autónomos. Esta encuesta proporciona una visión general detallada de la evolución de la comprensión de escenas robóticas y resume las aplicaciones de métodos de aprendizaje profundo en la comprensión de escenas para robots autónomos. Además, se analizan los problemas clave en la comprensión de escenas de robots autónomos, como la estimación de postura, la predicción de saliencia, la segmentación semántica y la detección de objetos. Luego, se resumen algunas soluciones representativas basadas en el aprendizaje profundo para estos problemas. Finalmente, se discuten los desafíos futuros en el campo de la comprensión de escenas para robots autónomos.",new_keywords:"Robots autónomos, comprensión de escenas, aprendizaje profundo, detección de objetos, estimación de pose.",is_check:d};bZ[9]={language:"po",new_title:"Compreensão de cena baseada em aprendizagem profunda para robôs autônomos: uma pesquisa",new_abstract:"Robôs autônomos são um assunto de pesquisa quente dentro dos campos da ciência e tecnologia, que tem um grande impacto no desenvolvimento sócio-econômico. A capacidade do robô autônomo de perceber e entender seu ambiente de trabalho é a base para resolver problemas mais complicados. Nos últimos anos, um número crescente de métodos baseados em inteligência artificial tem sido proposto no campo da compreensão de cenários para robôs autônomos, sendo a aprendizagem profunda uma das áreas-chave atuais neste campo. Avanços significativos foram alcançados no campo da compreensão de cenários para robôs autônomos com base em aprendizagem profunda. Assim, este artigo apresenta uma revisão das recentes pesquisas sobre compreensão de cenários baseada em aprendizagem profunda para robôs autônomos. Esta pesquisa fornece uma visão detalhada da evolução da compreensão de cenas robóticas e resume as aplicações de métodos de aprendizagem profunda na compreensão de cenários para robôs autônomos. Além disso, são analisadas as questões-chave na compreensão de cenas de robôs autônomos, como estimativa de pose, predição de saliência, segmentação semântica e detecção de objetos. Em seguida, são resumidas algumas soluções representativas baseadas em aprendizagem profunda para essas questões. Por fim, são discutidos os desafios futuros no campo da compreensão de cenários para robôs autônomos.",new_keywords:"Robôs autônomos, compreensão de cena, aprendizado profundo, detecção de objetos, estimativa de pose.",is_check:d};return {layout:"oaelayouta",data:[{ArtData:{date_published:"2023-08-15 00:00:00",section:"Review",title:y,doi:"10.20517\u002Fir.2023.22",abstract:K,pdfurl:"https:\u002F\u002Ff.oaes.cc\u002Fxmlpdf\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022.pdf",xmlurl:"https:\u002F\u002Ff.oaes.cc\u002Fxmlpdf\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022.xml",elocation_id:s,fpage:374,article_id:b,viewed:545,downloaded:42,video_url:Q,volume:R,year:S,cited:T,corresponding:"Correspondence to: Prof. Jianjun Ni, School of Artificial Intelligence and Automation, Hohai University, No.200, North Jinling Road, Xinbei District, Changzhou 213022, Jiangsu, China. E-mail: \u003Cemail\u003Enjjhhuc@gmail.com\u003C\u002Femail\u003E; ORCID: 0000-0002-7130-8331",editor:[],editor_time:"\u003Cspan\u003E\u003Cb\u003EReceived:\u003C\u002Fb\u003E 25 Apr 2023 | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003EFirst Decision:\u003C\u002Fb\u003E 5 Jul 2023 | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003ERevised:\u003C\u002Fb\u003E 15 Jul 2023 | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003EAccepted:\u003C\u002Fb\u003E 4 Aug 2023 | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003EPublished:\u003C\u002Fb\u003E 15 Aug 2023\u003C\u002Fspan\u003E",cop_link:"https:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F",cop_info:"© The Author(s) 2023. \u003Cb\u003EOpen Access\u003C\u002Fb\u003E This article is licensed under a Creative Commons Attribution 4.0 International License (\u003Ca target=\"_blank\"href=\"https:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F\u003C\u002Fa\u003E), which permits unrestricted use, sharing, adaptation, distribution and reproduction in any medium or format, for any purpose, even commercially, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.",keywords:["Autonomous robots","scene understanding","deep learning","object detection","pose estimation"],issue:R,image:t,tag:" \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E 2023;3(3):374-401.",authors:"Jianjun  Ni\u003Ca href='http:\u002F\u002Forcid.org\u002F0000-0002-7130-8331' target='_blank'\u003E\u003Cimg src='https:\u002F\u002Fi.oaes.cc\u002Fimages\u002Forcid.png' class='author_id' alt='Jianjun  Ni'\u003E\u003C\u002Fa\u003E, ... Pengfei  Shi\u003Ca href='https:\u002F\u002Forcid.org\u002F0000-0002-2966-1676' target='_blank'\u003E\u003Cimg src='https:\u002F\u002Fi.oaes.cc\u002Fimages\u002Forcid.png' class='author_id' alt='Pengfei  Shi'\u003E\u003C\u002Fa\u003E",picurl:s,expicurl:s,picabstract:s,interview_pic:a,interview_url:s,review:a,cop_statement:"© The Author(s) 2023.",seo:{title:y,keywords:a,description:U},video_img:V,lpage:401,author:[{base:"Jianjun Ni\u003Csup\u003E1,2\u003C\u002Fsup\u003E",email:"njjhhuc@gmail.com",orcid:"http:\u002F\u002Forcid.org\u002F0000-0002-7130-8331"},{base:"Yan Chen\u003Csup\u003E1,2\u003C\u002Fsup\u003E",email:a,orcid:"https:\u002F\u002Forcid.org\u002F0000-0003-4969-5723"},{base:"Guangyi Tang\u003Csup\u003E1\u003C\u002Fsup\u003E",email:a,orcid:"https:\u002F\u002Forcid.org\u002F0000-0002-1542-6392"},{base:"Jiamei Shi\u003Csup\u003E2\u003C\u002Fsup\u003E",email:a,orcid:"https:\u002F\u002Forcid.org\u002F0009-0005-5313-842X"},{base:"Weidong Cao\u003Csup\u003E1,2\u003C\u002Fsup\u003E",email:a,orcid:"https:\u002F\u002Forcid.org\u002F0000-0002-0394-9639"},{base:"Pengfei Shi\u003Csup\u003E1,2\u003C\u002Fsup\u003E",email:a,orcid:"https:\u002F\u002Forcid.org\u002F0000-0002-2966-1676"}],specialissue:{id:1243,name:" Scene Understanding for Autonomous Robotics"},specialinfo:a,date_published_stamp:1692028800,year1:S,CitedImage:"https:\u002F\u002Fi.oaes.cc\u002Fimages_2018\u002Fjournals\u002FCrossref.png",article_editor:[],editoruser:"\u003Cspan\u003E\u003Cb\u003EAcademic Editors:\u003C\u002Fb\u003E Simon X. Yang, Hongtian Chen | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003ECopy Editor:\u003C\u002Fb\u003E Yanbin Bai | \u003C\u002Fspan\u003E\u003Cspan\u003E\u003Cb\u003EProduction Editor:\u003C\u002Fb\u003E Yanbin Bai\u003C\u002Fspan\u003E",commentsNums:p,oaestyle:W,amastyle:X,ctstyle:Y,acstyle:Z,copyImage:"https:\u002F\u002Fi.oaes.cc\u002Fimages_2018\u002Fjournals\u002Fccb_4.png",affiliation:[{id:68276,article_id:b,Content:"\u003Clabel\u003E\u003Csup\u003E1\u003C\u002Fsup\u003E\u003C\u002Flabel\u003E\u003Caddr-line\u003ESchool of Artificial Intelligence and Automation, Hohai University, Changzhou 213022, Jiangsu, China.\u003C\u002Faddr-line\u003E"},{id:68277,article_id:b,Content:"\u003Clabel\u003E\u003Csup\u003E2\u003C\u002Fsup\u003E\u003C\u002Flabel\u003E\u003Caddr-line\u003ECollege of Information Science and Engineering, Hohai University, Changzhou 213022, Jiangsu, China.\u003C\u002Faddr-line\u003E"}],related:[{article_id:_,journal_id:h,section_id:m,path:e,journal:f,ar_title:$,date_published:aa,doi:ab,author:[{first_name:A,middle_name:a,last_name:B,ans:c,email:a,bio:a,photoUrl:a},{first_name:ac,middle_name:a,last_name:ad,ans:c,email:ae,bio:a,photoUrl:a},{first_name:af,middle_name:a,last_name:ag,ans:c,email:a,bio:a,photoUrl:a},{first_name:ah,middle_name:a,last_name:B,ans:c,email:a,bio:a,photoUrl:a},{first_name:ai,middle_name:a,last_name:o,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:aj,journal_id:h,section_id:m,path:e,journal:f,ar_title:ak,date_published:al,doi:am,author:[{first_name:an,middle_name:a,last_name:A,ans:c,email:a,bio:a,photoUrl:a},{first_name:ao,middle_name:a,last_name:ap,ans:j,email:a,bio:a,photoUrl:a},{first_name:aq,middle_name:a,last_name:ar,ans:as,email:a,bio:a,photoUrl:a},{first_name:at,middle_name:a,last_name:C,ans:D,email:au,bio:a,photoUrl:a}]},{article_id:av,journal_id:h,section_id:m,path:e,journal:f,ar_title:aw,date_published:ax,doi:ay,author:[{first_name:az,middle_name:a,last_name:aA,ans:c,email:a,bio:a,photoUrl:a},{first_name:aB,middle_name:a,last_name:aC,ans:c,email:a,bio:a,photoUrl:a},{first_name:aD,middle_name:a,last_name:C,ans:c,email:a,bio:a,photoUrl:a},{first_name:aE,middle_name:a,last_name:aF,ans:j,email:a,bio:a,photoUrl:a},{first_name:aG,middle_name:a,last_name:aH,ans:j,email:a,bio:a,photoUrl:a}]},{article_id:aI,journal_id:h,section_id:m,path:e,journal:f,ar_title:L,date_published:aJ,doi:M,author:[{first_name:aK,middle_name:a,last_name:aL,ans:c,email:aM,bio:a,photoUrl:a},{first_name:aN,middle_name:a,last_name:aO,ans:c,email:a,bio:a,photoUrl:a},{first_name:aP,middle_name:a,last_name:aQ,ans:c,email:a,bio:a,photoUrl:a},{first_name:aR,middle_name:a,last_name:aS,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:aT,journal_id:h,section_id:q,path:e,journal:f,ar_title:aU,date_published:aV,doi:aW,author:[{first_name:aX,middle_name:a,last_name:aY,ans:r,email:a,bio:a,photoUrl:a},{first_name:u,middle_name:a,last_name:E,ans:r,email:a,bio:a,photoUrl:a},{first_name:aZ,middle_name:a,last_name:a_,ans:r,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:r,email:F,bio:a,photoUrl:a}]},{article_id:a$,journal_id:h,section_id:m,path:e,journal:f,ar_title:ba,date_published:bb,doi:bc,author:[{first_name:bd,middle_name:a,last_name:w,ans:c,email:a,bio:a,photoUrl:a},{first_name:be,middle_name:a,last_name:bf,ans:c,email:a,bio:a,photoUrl:a},{first_name:bg,middle_name:a,last_name:bh,ans:j,email:a,bio:a,photoUrl:a}]},{article_id:bi,journal_id:h,section_id:m,path:e,journal:f,ar_title:bj,date_published:bk,doi:bl,author:[{first_name:u,middle_name:a,last_name:bm,ans:c,email:bn,bio:a,photoUrl:a},{first_name:bo,middle_name:a,last_name:x,ans:j,email:a,bio:a,photoUrl:a},{first_name:bp,middle_name:a,last_name:o,ans:j,email:a,bio:a,photoUrl:a},{first_name:bq,middle_name:a,last_name:br,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:bs,journal_id:h,section_id:q,path:e,journal:f,ar_title:bt,date_published:bu,doi:bv,author:[{first_name:u,middle_name:a,last_name:E,ans:c,email:a,bio:a,photoUrl:a},{first_name:bw,middle_name:a,last_name:bx,ans:c,email:a,bio:a,photoUrl:a},{first_name:by,middle_name:a,last_name:bz,ans:D,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:c,email:F,bio:a,photoUrl:a},{first_name:bA,middle_name:a,last_name:bB,ans:bC,email:bD,bio:a,photoUrl:a}]},{article_id:bE,journal_id:h,section_id:q,path:e,journal:f,ar_title:bF,date_published:bG,doi:bH,author:[{first_name:bI,middle_name:a,last_name:w,ans:c,email:a,bio:a,photoUrl:a},{first_name:bJ,middle_name:a,last_name:bK,ans:c,email:a,bio:a,photoUrl:a},{first_name:bL,middle_name:a,last_name:x,ans:c,email:bM,bio:a,photoUrl:a},{first_name:bN,middle_name:a,last_name:bO,ans:c,email:a,bio:a,photoUrl:a},{first_name:bP,middle_name:a,last_name:w,ans:j,email:bQ,bio:a,photoUrl:a}]},{article_id:bR,journal_id:h,section_id:q,path:e,journal:f,ar_title:bS,date_published:bT,doi:bU,author:[{first_name:bV,middle_name:a,last_name:x,ans:c,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:j,email:a,bio:a,photoUrl:a}]}],down:"https:\u002F\u002Ff.oaes.cc\u002Fris\u002F6014.ris",xml:{id:4165,article_id:b,xml_down:bW,cite_click:bX,export_click:p},zan:1,cited_type:"cited",subarray:[],issn:"ISSN 2770-3541 (Online)",uuid:a,abstractUuid:a,apiurl:a,api_abstract_url:a,journal_id:bY,journal_path:e},loadingAbs:void 0,loading:z,ArtDataC:{content:"\u003Cdiv id=\"sec11\" class=\"article-Section\"\u003E\u003Ch2 \u003E1. INTRODUCTION\u003C\u002Fh2\u003E\u003Cp class=\"\"\u003EIn recent years, science and technology have developed rapidly, and the applications of autonomous robots become increasingly extensive \u003Csup\u003E[\u003Ca href=\"#b1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b1\"\u003E1\u003C\u002Fa\u003E–\u003Ca href=\"#b3\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b3\"\u003E3\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. With the development of the technologies, the tasks for autonomous robots have become more complicated and challenging. To complete these tasks, one of the main requirements for autonomous robots is the strong capability of the robot to effectively perceive and understand the complicated three-dimensional (3D) environment in which it is positioned.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe ability of an autonomous robot to perceive and understand its own environment, akin to human perception, serves as the foundation for further autonomous interaction with the environment and human users. This problem is also a prominent topic in the field of computer vision, which has made great progress, and lots of research findings have been used for practical applications of autonomous robots. Many research findings in this field are based on two-dimensional (2D) images. However, the real world is a 3D environment, and there remains ample room for future research on the perception and understanding of 3D environments. The environment perception is the basis of scene understanding, which can provide stable and accurate information for scene understanding. On the other hand, scene understanding can provide richer and higher-level information for environment perception. In this paper, we will mainly discuss the scene understanding problems.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThere are lots of research results in this field. Nevertheless, a significant portion of current research is focused on more idealized situations. However, the real world is a complicated scene with a number of issues that affect the accuracy of environmental perception and understanding, such as image interference, clutter occlusion, etc. Consequently, it is crucial to study the essential technologies that enable autonomous robots to perceive and comprehend their environment within complex 3D space, addressing both theoretical underpinnings and practical implementation.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThis paper provides a survey on the deep learning-based scene understanding for autonomous robots. We provide a brief overview of the research methodologies used to study the perception and comprehension of the robotic environment, and then we concentrate on deep learning-based approaches to these issues. Other relevant surveys in the field of deep learning-based scene understanding can be used as supplements to this paper (see e.g., \u003Csup\u003E[\u003Ca href=\"#b4\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b4\"\u003E4\u003C\u002Fa\u003E,\u003Ca href=\"#b5\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b5\"\u003E5\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E and \u003Csup\u003E[\u003Ca href=\"#b6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b6\"\u003E6\u003C\u002Fa\u003E,\u003Ca href=\"#b7\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b7\"\u003E7\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E). The main differences between this paper and other surveys lie in its function as an overview of the state-of-the-art approaches in this field, owing to the continuous emergence of new approaches driven by the rapid development of deep learning-based scene understanding. In addition, this paper provides a selection of the latest related works from our research group.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe main contributions of this paper are summarized as follows: (1) The advancement of scene understanding for autonomous robots is thoroughly analyzed and reviewed; (2) A survey on the applications of deep learning methods in scene understanding for autonomous robots is given out; and (3) Some representative deep learning-based methods in the field of autonomous robot scene understanding are analyzed. At last, some possible future study directions in this field are discussed.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThis paper is organized as follows. Section 2 provides a summary of the development of autonomous robots and their ability to perceive and comprehend their environment. In Section 3, the key issues of the scene understanding for autonomous robots are analyzed. Additionally, select representative deep learning-based methods based on deep learning techniques in the field of scene understanding are outlined and analyzed. The potential study directions of deep learning-based perception and comprehension of the environment for autonomous robots are given out in Section 4. Finally, conclusions are given out in Section 5.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec12\" class=\"article-Section\"\u003E\u003Ch2 \u003E2. BACKGROUND AND SIGNIFICANCE OF THE SCENE UNDERSTANDING\u003C\u002Fh2\u003E\u003Cp class=\"\"\u003EThe global economy has witnessed rapid growth in recent years, paralleled by swift advancements in science and technology. The applications of robots are becoming more and more popular \u003Csup\u003E[\u003Ca href=\"#b8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b8\"\u003E8\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. Autonomous robots are the representative of advanced technologies, which are the integration of the robotics, information technology, communication technology, and artificial intelligence. These robots have been more integrated into human society, not only creating huge economic benefits for society but also effectively improving individual living standards \u003Csup\u003E[\u003Ca href=\"#b9\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b9\"\u003E9\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe autonomous robot industry is an important standard to evaluate the innovation and high-end manufacturing level of a country. The development of the autonomous robot has attracted growing attention from countries all over the world. A number of famous research institutions and companies across the globe have focused on the realm of autonomous robots.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe representative robotics research institutions include the Robotics and Mechatronics Center (RMC) of the German Aerospace Center, the Computer Science and Artificial Intelligence Laboratory (CSAIL) of Massachusetts Institute of Technology, the Humanoid Robotics Institute (HRI) of Waseda University, Shenyang Institute of Automation Chinese Academy of Sciences, the Robotics Institute of Shanghai Jiaotong University, and so on. There are lots of representative robotic enterprises, such as ABB (Switzerland), KUKA Robotics (Germany), Yaskawa Electric Corporation (Japan), iRobot (USA), AB Precision (UK), Saab Seaeye (Sweden), SIASUN (China), etc \u003Csup\u003E[\u003Ca href=\"#b10\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b10\"\u003E10\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EDue to the current technical limitations, the functions of common autonomous robots in daily life are still relatively simple. For example, the serving robot [see \u003Ca href=\"#Figure1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure1\"\u003EFigure 1A\u003C\u002Fa\u003E] and the sweeping robot [see \u003Ca href=\"#Figure1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure1\"\u003EFigure 1B\u003C\u002Fa\u003E] can only complete some simple tasks, such as moving according to the planned trajectory to the designated position. The expansion of the robot application range requires that the functions of robots are no longer limited to mechanized or programmed operations, narrow human-computer interactions, etc. There is an increasing need for autonomous robots to carry out more difficult tasks. Robots are anticipated to be able to do complicated tasks, such as picking up and dropping off goods or even operating tools autonomously by sensing their surroundings. Empowering autonomous robots with ample environmental perception and a comprehensive understanding of their intricate 3D surroundings stands as an essential prerequisite to satisfy the requirements for these more difficult jobs. For example, the logistics robot can make mobility control decisions after it can autonomously perceive and understand the traffic and road environment [see \u003Ca href=\"#Figure1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure1\"\u003EFigure 1C\u003C\u002Fa\u003E]. To operate effectively and securely in the unknown and complex underwater environment, the underwater search robot must be aware of its surroundings [see \u003Ca href=\"#Figure1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure1\"\u003EFigure 1D\u003C\u002Fa\u003E].\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure1\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure1\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-1.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 1. Applications of scene understanding for autonomous robots: (A) Service robots; (B) Sweeping robots; (C) Logistics robots; (D) Underwater search robots.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EWhen an autonomous robot conducts a task in a complicated environment, it must first determine its current position and estimate its displacement pose change through a visual Simultaneous Localization and Mapping (SLAM) system. The robot also needs to assess the shape of the environment and comprehend the range of its surroundings. In addition, it is of utmost practical importance to research room layout estimation in complex cluttered environments. Next, the autonomous robot should perform the saliency detection, namely directing its attention toward the regions of interest, akin to human behavior. This is followed by target detection, a crucial step in identifying manipulable items and their locations within the environment. Notably, the study of functional availability detection of objects in 3D space is fundamentally important for robots to further perform complex operational tasks because autonomous robots need to understand the functional availability and even the usage of each part of the object to be interacted with. This facet is closely related to the 3D structure of the object. The main tasks of the scene understanding for the autonomous robot in a complicated environment are shown in \u003Ca href=\"#Figure2\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure2\"\u003EFigure 2\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure2\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure2\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-2.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 2. The main tasks of the scene understanding for the autonomous robot.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAll of these tasks introduced above are the research topics in the scene understanding of autonomous robots. In a word, scene understanding of autonomous robots is to analyze a scene by considering the geometric and semantic context of its contents and the intrinsic relationships between them. The process mainly involves matching signal information from sensors observing the scene with a model that humans use to understand the scene. On this basis, scene understanding is the semantic extraction and addition of sensor data, which is used to describe the scene for autonomous robots.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the early research of scene understanding, parts-based representations for object description and scene understanding were the mainstream methods. In these methods, the basic information and hidden deeper information of images are reflected by extracting the low-level and middle-level visual features. And these early methods often realize semantic classification through feature modeling. There are many traditional feature representation methods that have been used widely in scene understanding. Scale-Invariant Feature Transform (SIFT) \u003Csup\u003E[\u003Ca href=\"#b11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b11\"\u003E11\u003C\u002Fa\u003E,\u003Ca href=\"#b12\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b12\"\u003E12\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E has rotation, scale, and affine invariant qualities. It has an excellent classification effect even for images with huge scale changes. GIST \u003Csup\u003E[\u003Ca href=\"#b13\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b13\"\u003E13\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E is an image global description feature based on fusing contextual data and a spatial envelope model, which can extract the spatial structural data of pictures using the energy spectrum. LSA (Latent Semantic Analysis) \u003Csup\u003E[\u003Ca href=\"#b14\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b14\"\u003E14\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E is used to address the issue of many words with a single meaning and multiple meanings of a word in text analysis. Other good manually designed features include Speeded Up Robust Features (SURF) \u003Csup\u003E[\u003Ca href=\"#b15\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b15\"\u003E15\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, Histogram of Oriented Gradient (HOG) \u003Csup\u003E[\u003Ca href=\"#b16\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b16\"\u003E16\u003C\u002Fa\u003E,\u003Ca href=\"#b17\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b17\"\u003E17\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, and so on. Based on these features, a number of traditional image scene semantic classification techniques were developed. For example, Vailaya \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b18\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b18\"\u003E18\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E classified scene images using dual features of color moments and texture and derived global features of images in a Bayesian framework. Li \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b19\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b19\"\u003E19\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E presented the target library-based image classification technique by decomposing an image into a number of objects and identifying the semantics of each object to realize the semantic classification of images.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EWith the rapid and substantial growth of hardware computing power, deep learning methods have gained rapid development \u003Csup\u003E[\u003Ca href=\"#b20\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b20\"\u003E20\u003C\u002Fa\u003E–\u003Ca href=\"#b22\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b22\"\u003E22\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. Data-driven methods, especially those based on deep neural networks, have been proven to have outstanding advantages in feature learning and visual data description. The scene understanding of autonomous robots based on deep learning has been developed rapidly. Compared to traditional scene understanding methods, the methods based on deep neural networks can more flexibly use the adaptively extracted features to perform tasks such as object detection, semantic segmentation, and more. As a result, they achieve far better performance.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EEnvironment perception and understanding of autonomous robots in complex 3D scenes is a hot topic both in computer vision and the robotic field. However, there are some differences between normal computer vision and the scene understanding for autonomous robots. Firstly, normal computer vision usually obtains data from static images or videos for analysis and pays more attention to the detection, recognition, and positioning of objects. Scene understanding for autonomous robots usually requires the combination of multiple sensor data and needs to consider the dynamic changes in the environment and the 3D perception and understanding of the environment in order to carry out tasks such as path planning and obstacle avoidance. Furthermore, this process often entails interactions with both the environment and individuals, leading to decision-making based on the interaction output. In contrast, the normal computer vision does not require such interactivity.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EA lot of challenging, realistic issues still need to be resolved, and various methods have been used in this field, such as traditional image processing methods, traditional artificial intelligence methods, and so on. Among these methods, deep learning-based methods have achieved great success in this field for their distinct advantages, such as high accuracy, strong robustness, and low cost. This paper will focus on the deep learning-based methods used in the field of scene understanding for autonomous robots.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec13\" class=\"article-Section\"\u003E\u003Ch2 \u003E3. DEEP LEARNING FOR SCENE UNDERSTANDING\u003C\u002Fh2\u003E\u003Cp class=\"\"\u003EDeep neural networks, which serve as the foundational network for image classification, target recognition, image segmentation, target tracking, and video analysis, are used in the deep learning-based vision system. The network parameters are trained through big data, and the feature extraction and classification are realized end-to-end, avoiding complex feature engineering design. Deep learning-based methods have strong feature representation capabilities that can be used to transform the original image into low-level spatial features, middle-level semantic features, and high-level target features. Then, through feature combination, classification and prediction tasks can be achieved efficiently. In addition, learning-based methods have strong generality and make it simpler to complete multi-task learning and multi-modal learning tasks that incorporate video, text, and speech. This helps to advance the development of scene understanding for autonomous robots.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EAs introduced in Section 2, lots of issues should be solved in the field of scene understanding for autonomous robots. In this section, the detailed applications grounded in various deep learning methods will be introduced. The main applications for scene understanding based on deep learning summarized here stem from the extensive awareness of the authors, which can demonstrate the key issues and the latest advances in the field of scene understanding. The main applications of deep learning in scene understanding include object detection, semantic segmentation, pose estimation, and so on. These applications will be introduced in detail as follows. \u003Ca href=\"#Figure3\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure3\"\u003EFigure 3\u003C\u002Fa\u003E shows these deep learning-based models according to the time they are published. To describe easily without loss of generality, we do not distinguish the applications between normal computer vision and scene understanding for autonomous robots in this paper.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure3\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure3\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-3.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 3. Some deep learning-based models in the field of scene understanding in recent years.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAs we know, the datasets are very important for the scene understanding based on deep learning methods. Lots of works of literature have introduced various datasets in different tasks of scene understanding. So, before introducing the main applications of deep learning in this field, the most used datasets in the field of scene understanding are summarized and shown in \u003Ca href=\"#Table1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table1\"\u003ETable 1\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table1\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 1\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EThe most used datasets of deep learning in the field of scene understanding\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003EKITTI\u003Csup\u003E[\u003Ca href=\"#b23\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b23\"\u003E23\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"left\"\u003EThe KITTI dataset contains 7, 481 training samples and 7, 518 test samples divided into three categories (i.e., Car, Pedestrian, and Cyclist). In addition, it is divided into three difficulty levels based on the scale, occlusion, and truncation levels of the objects in the context of autonomous driving (i.e., Easy, Moderate, and Hard)\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EnuScenes\u003Csup\u003E[\u003Ca href=\"#b24\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b24\"\u003E24\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EThe nuScenes dataset consists of 1000 challenging driving video sequences, each about 20 seconds long, with 30\u003Cinline-formula\u003E\u003Ctex-math id=\"M1\"\u003E$$ k $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E points per frame. It has 700, 150, and 150 annotated sequences for training, evaluation, and test segmentation, respectively\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ELINEMOD\u003Csup\u003E[\u003Ca href=\"#b25\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b25\"\u003E25\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EIt is a dataset widely used for 6D object pose estimation. There are 13 objects in this dataset. For each object, there are about 1100-1300 images with annotations and only one object with annotation per image\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EFAST-YCB\u003Csup\u003E[\u003Ca href=\"#b26\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b26\"\u003E26\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EIt consists of six realistic synthetic sequences, each containing the fast motion of a single object from the YCB model set in the desktop scene. Each sequence is rendered in bright static lighting conditions and provides \u003Cinline-formula\u003E\u003Ctex-math id=\"M2\"\u003E$$ 1280\\times720 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E RGB-D frames with accurate ground truth of 6D object pose and velocity\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EPASCAL VOC 2012\u003Csup\u003E[\u003Ca href=\"#b27\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b27\"\u003E27\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EIt is a benchmark dataset that initially contains 1464 images for training, 1449 for validation, and 1456 for testing. In the original PASCAL VOC 2012 dataset, there are a total of 20 foreground object classes and one background class\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ECityscapes\u003Csup\u003E[\u003Ca href=\"#b28\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b28\"\u003E28\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EThe dataset has 5, 000 images captured from 50 different cities. Each image has \u003Cinline-formula\u003E\u003Ctex-math id=\"M3\"\u003E$$ 2048\\times10244 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E pixels, which have high-quality pixel-level labels of 19 semantic classes\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EDHF1K\u003Csup\u003E[\u003Ca href=\"#b29\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b29\"\u003E29\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003EIt contains the most common and diverse scenarios, with 1000 video samples and no publicly available ground-truth annotations. Only the first 700 annotated maps and videos are available in the DHF1K dataset, and the remaining 300 annotations are reserved for benchmarking\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2 table_bottom_border\" align=\"center\"\u003EVOT-2017\u003Csup\u003E[\u003Ca href=\"#b30\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b30\"\u003E30\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2 table_bottom_border\" align=\"left\"\u003EThe VOT-2017 dataset can be used for target tracking of different tasks and contains 60 short sequences labeled with six different attributes\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec26\" class=\"article-Section\"\u003E\u003Ch3 \u003E3.1. 3D object detection\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EObject detection is an image segmentation based on geometric and statistical features of the object. It combines object segmentation and recognition into one task, with the aim of determining the location and class of object appearances. Currently, 2D object detection has been relatively mature, especially with the emergence of Faster Regions with convolutional neural network Features (Faster RCNN), which has brought it to an unprecedented boom. For example, in the previous work of our research group \u003Csup\u003E[\u003Ca href=\"#b31\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b31\"\u003E31\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, a deep neural network-based SSD framework is proposed to improve the feature representation capability of feature extraction networks. However, in the application scenarios of driverless, robotics, and augmented reality, 2D object detection can only provide the confidence of the position and corresponding category of the object in a 2D image (see \u003Ca href=\"#Figure4\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure4\"\u003EFigure 4\u003C\u002Fa\u003E), while the general 2D object detection cannot provide all the information needed for perceiving the environment.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure4\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure4\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-4.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 4. 2D object detection visualization: (A) in the bedroom; (B) in the kitchen \u003Csup\u003E[\u003Ca href=\"#b31\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b31\"\u003E31\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EIn the real world, objects have 3D shapes, and most applications require information about the length, width, height, and also the deflection angle of the target object. Therefore, research on methods related to 3D target detection is needed. In scene understanding for autonomous robots, object detection is a critical task to understand the position and class of the objects with which they interact. In real 3D complex scenes, the background information is very rich; therefore, object detection techniques can be used to understand the location and category of interactable objects by giving a 3D rectangular location candidate box and categorizing them according to their attribution possibilities.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E3D object detection based on deep learning is a hot research topic in the field of environment perception and understanding. In the deep learning-based model, during the process of making the proposals in regional proposal networks in a bottom-up manner, the resulting proposals somehow deviate from the ground truth and appear densely in local communities. Due to the lack of a corresponding information compensation mechanism, the proposals generated by the general regional proposal networks give up a large amount of boundary information. To deal with this problem, Qian \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed BADet, a 3D object detection model from point clouds, which can efficiently model the local boundary correlations of objects through local neighborhood graphs and significantly facilitate the complete boundaries of each individual proposal.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EBADet consists of three key components, namely, a backbone and region generation network, a region feature aggregation module, and a boundary-aware graph neural network. Its network overview is shown in \u003Ca href=\"#Figure5\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure5\"\u003EFigure 5\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure5\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure5\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-5.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 5. The network overview of the BADet, where RPN denotes the region proposal network \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EIn the backbone and region proposal network (RPN) of BADet, the original point cloud is voxelized into a volume mesh for multi-scale semantic feature abstraction and 3D proposal generation with the help of the backbone and a series of 3D sparse convolutions. Specifically, let \u003Cinline-formula\u003E\u003Ctex-math id=\"M4\"\u003E$$ p $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E be a point in a raw point cloud \u003Cinline-formula\u003E\u003Ctex-math id=\"M5\"\u003E$$ P $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E with 3D coordinates \u003Cinline-formula\u003E\u003Ctex-math id=\"M6\"\u003E$$ (p_x , p_y , p_z ) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and reflectance intensities \u003Cinline-formula\u003E\u003Ctex-math id=\"M7\"\u003E$$ p_r $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, then\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(1)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E1\"\u003E $$ P = \\left\\{ {p^i = (p_x^i , p_y^i , p_z^i , p_r^i ) \\in \\Re ^4 , i = 1, 2, \\cdots , N} \\right\\} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M8\"\u003E$$ N $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E indicates the number of points within \u003Cinline-formula\u003E\u003Ctex-math id=\"M9\"\u003E$$ P $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E. Let \u003Cinline-formula\u003E\u003Ctex-math id=\"M10\"\u003E$$ [v_L , v_W , v_H ] \\in \\Re ^3 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E be the quantization step, then the voxelized coordinates of \u003Cinline-formula\u003E\u003Ctex-math id=\"M11\"\u003E$$ p $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E can be obtained, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(2)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E2\"\u003E $$ V_p = \\left( {\\left\\lfloor {\\frac{{p_x }}{{v_L }}} \\right\\rfloor , \\left\\lfloor {\\frac{{p_y }}{{v_W }}} \\right\\rfloor , \\left\\lfloor {\\frac{{p_z }}{{v_H }}} \\right\\rfloor } \\right) $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M12\"\u003E$$ \\left\\lfloor {\\cdot} \\right\\rfloor $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the floor function. Therefore, the point cloud \u003Cinline-formula\u003E\u003Ctex-math id=\"M13\"\u003E$$ P $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E can be positioned into a feature map with a resolution of \u003Cinline-formula\u003E\u003Ctex-math id=\"M14\"\u003E$$ L \\times W \\times H $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, subject to the quantization step \u003Cinline-formula\u003E\u003Ctex-math id=\"M15\"\u003E$$ [v_L , v_W , v_H ] $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the region feature aggregation module of BADet, multi-level semantic features are leveraged to obtain more informative RoI-wise representations. In the boundary-aware graph neural network, neighboring 3D proposals are used as inputs for graph construction within a given cutoff distance. Specifically, the local neighborhood graph \u003Cinline-formula\u003E\u003Ctex-math id=\"M16\"\u003E$$ G(V, E) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E can be constructed as\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(3)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E3\"\u003E $$ E = \\left\\{ {(i, j)\\left|\\; {\\left\\| {x_i - x_j } \\right\\|_2 &lt; r} \\right.} \\right\\} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M17\"\u003E$$ V $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M18\"\u003E$$ E $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the nodes and edges, respectively; \u003Cinline-formula\u003E\u003Ctex-math id=\"M19\"\u003E$$ r $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the threshold; and \u003Cinline-formula\u003E\u003Ctex-math id=\"M20\"\u003E$$ x_i $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E denotes the 3D coordinates of a node of graph \u003Cinline-formula\u003E\u003Ctex-math id=\"M21\"\u003E$$ G $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, an overall loss \u003Cinline-formula\u003E\u003Ctex-math id=\"M22\"\u003E$$ L $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is used, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(4)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E4\"\u003E $$ L = L_{rpn} + L_{gnn} + L_{offset} + L_{seg} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M23\"\u003E$$ L_{rpn} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M24\"\u003E$$ L_{gnn} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are Focal Loss and Smooth-L1 Loss for the bounding box classification and regression, respectively; \u003Cinline-formula\u003E\u003Ctex-math id=\"M25\"\u003E$$ L_{offset} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the center offset estimation loss, which is used to obtain better boundary-aware voxelwise representations, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M26\"\u003E$$ L_{seg} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the foreground segmentation loss.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ETo evaluate the performance of BADet, some comparison experiments are conducted on the KITTI and nuScenes datasets. The results of BADet are listed in \u003Ca href=\"#Table2\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table2\"\u003ETable 2\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table2\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 2\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EThe results of BADet on KITTI test server and nuScenes dataset \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd align=\"center\" style=\"class:table_top_border\"\u003E\u003C\u002Ftd\u003E\u003Ctd colspan=\"3\" align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003EKITTI test server\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003EnuScenes dataset\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E\u003Cb\u003EEasy\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E\u003Cb\u003EModerate\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E\u003Cb\u003EHard\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"6\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M27\"\u003E$$AP_{3D}$$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M28\"\u003E$$AP_{BEV}$$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E mean the Average Precision (\u003Ci\u003EAP\u003C\u002Fi\u003E) with 40 recall positions on both BEV (Bird's Eye View) and 3D object detection leaderboard; \u003Ci\u003EmAP\u003C\u002Fi\u003E and \u003Ci\u003ENDS\u003C\u002Fi\u003E denote the mean Average Precision and nuScenes detection score, respectively.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M29\"\u003E$$ AP_{3D} (\\%) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E89.28\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E81.61\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E76.58\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M30\"\u003E$$ mAP (\\%) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003E47.65\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M31\"\u003E$$ AP_{BEV} (\\%) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E95.23\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E91.32\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E86.48\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M32\"\u003E$$ NDS (\\%) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E58.84\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EThe results in \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E show that BADet outperforms all its competitors with remarkable margins on KITTI BEV detection leaderboard and ranks 1st in \"Car\" category of moderate difficulty.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E3D object detection methods have developed rapidly with the development of deep learning techniques. In recent years, many scholars have been exploring new results in this field. For example, Shi \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b33\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b33\"\u003E33\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the Part-A2 net to implement the 3D object detection using only LiDAR point cloud data. Li \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b34\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b34\"\u003E34\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the TGNet, a new graph convolution structure, that can effectively learn expressive and compositional local geometric features from point clouds.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EAccording to the type of input data, 3D object detection can be divided into single-modal methods and multi-modal methods. Single-modal 3D object detection refers to the use of data collected by one kind of sensor as input. The advantage of single-modal 3D target detection is that the input data are simple and the processing flow is clear; the disadvantage is that the input data may not be sufficient to describe the target information in 3D space. Multi-modal 3D object detection refers to the use of multiple data collected by multiple types of sensors as inputs. The advantage of multi-modal 3D target detection is that the input data are rich, and the complementarity of different modal data can be utilized to improve the accuracy and robustness of the detection. The disadvantage is the complexity of the input data and the need to deal with inconsistencies between different modal data. In the following, a summary of the deep learning-based 3D object detection models presented in the last five years is illustrated in \u003Ca href=\"#Table3\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table3\"\u003ETable 3\u003C\u002Fa\u003E, where the type of the input data of each method is given out.\u003C\u002Fp\u003E\u003Cdiv id=\"Table3\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 3\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EA summary of the deep learning-based 3D object detection models presented in the last five years\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStructure\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EReference\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EInput data type\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"left\"\u003E\u003Cb\u003EPerformances\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"4\"\u003E\u003Ci\u003EAP\u003C\u002Fi\u003E means the average precision. \u003Ci\u003EMIoU\u003C\u002Fi\u003E: the Mean Intersection over Union.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ESECOND\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EYan \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b35\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b35\"\u003E35\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M33\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 83.13% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EF-pointNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EQi \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b36\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b36\"\u003E36\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EMulti-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M34\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 81.20% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EF-ConvNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EWang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b37\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b37\"\u003E37\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M35\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 85.88% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EFast Point R-CNN\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EChen \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b38\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b38\"\u003E38\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M36\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 84.28% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ESA-SSD\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHe \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b39\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b39\"\u003E39\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M37\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 88.75% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ETANet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELiu \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b40\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b40\"\u003E40\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E3D \u003Cinline-formula\u003E\u003Ctex-math id=\"M38\"\u003E$$ mAP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 62.00% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ETGNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELi \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b34\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b34\"\u003E34\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M39\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 68.17% on Paris-Lille-3D datasets\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ECenterPoint\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EYin \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b41\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b41\"\u003E41\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M40\"\u003E$$ mAP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 58.0% on nuScenes test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EPart-A2\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EShi \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b33\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b33\"\u003E33\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EMulti-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M41\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 85.94% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ERGBNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EWang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b42\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b42\"\u003E42\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EMulti-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M42\"\u003E$$ mAP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 70.2% on ScanNetV2 val set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EBADet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EQian \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b32\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b32\"\u003E32\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESingle-modal\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M43\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 89.28% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EDCLM\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EChen \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2023) \u003Csup\u003E[\u003Ca href=\"#b43\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b43\"\u003E43\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EMulti-modal\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M44\"\u003E$$ mAP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 65.6% on SUN RGB-D dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec27\" class=\"article-Section\"\u003E\u003Ch3 \u003E3.2. Pose estimation\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EPose estimation is a crucial component of autonomous robot technology. The pose estimation task deals with finding the position and orientation of an object with respect to a specific coordinate system. The vision-based pose estimation approaches employ a number of feature extraction techniques to obtain the spatial positional information of the target from the image.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThere are two classical methods for pose estimation, namely, the feature-based techniques and the template matching methods. The traditional feature-based technique primarily extracts features from images and creates a relationship between the 2D pixel points and 3D coordinate points in space. The differences in lighting and background complexity have a significant impact on the feature extraction process. In addition, the feature-based methods struggle to handle sparse target texture features. The template matching method can effectively solve the pose estimation problem for the targets with weak texture features in images. However, the accuracy of the template matching method is determined by the number of samples in the template library. While its accuracy improves with the number of the template libraries, it also causes a decrease in problem-solving efficiency, making it unable to meet real-time requirements.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe development of deep learning has influenced pose estimation, and there are numerous study findings in this field. For example, Hoang \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b44\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b44\"\u003E44\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the Voting and Attention-based model, which enhances the accuracy of object pose estimation by learning higher-level characteristics from the dependencies between the individual components of the object and object instances. The structure of this Voting and Attention-based network is shown in \u003Ca href=\"#Figure6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure6\"\u003EFigure 6\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure6\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure6\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-6.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 6. Network architecture of the Voting and Attention-based model \u003Csup\u003E[\u003Ca href=\"#b44\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b44\"\u003E44\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAs shown in \u003Ca href=\"#Figure6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure6\"\u003EFigure 6\u003C\u002Fa\u003E, there are four main parts in the Voting and Attention-based model, namely the feature extraction module based on PointNet++ architecture, part proposals learning module (\u003Cinline-formula\u003E\u003Ctex-math id=\"M45\"\u003E$$ \\text{M}_{\\text{p}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E), object proposals learning module (\u003Cinline-formula\u003E\u003Ctex-math id=\"M46\"\u003E$$ \\text{M}_{\\text{o}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E), and the voting module in both \u003Cinline-formula\u003E\u003Ctex-math id=\"M47\"\u003E$$ \\text{M}_{\\text{p}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M48\"\u003E$$ \\text{M}_{\\text{o}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E based on VoteNet.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the \u003Cinline-formula\u003E\u003Ctex-math id=\"M49\"\u003E$$ \\text{M}_{\\text{p}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E module of the Voting and Attention-based model, the higher-order interactions between the proposed features can be explicitly modeled, which is formulated as non-local operations:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(5)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E5\"\u003E $$ \tH_{part-part}=f\\left( \\theta\\left( H\\right) \\phi\\left( H\\right) \\right) g\\left( H\\right) . $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M50\"\u003E$$ \\theta(\\cdot), \\phi(\\cdot), and g(\\cdot) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the learnable transformation on the input feature map \u003Cinline-formula\u003E\u003Ctex-math id=\"M51\"\u003E$$ H $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M52\"\u003E$$ f(\\cdot) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the encoding function of the relationship between any two parts.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn addition, the compact generalized non-local network (CGNL) \u003Csup\u003E[\u003Ca href=\"#b45\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b45\"\u003E45\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E is used as the self-attentive module in \u003Cinline-formula\u003E\u003Ctex-math id=\"M53\"\u003E$$ \\text{M}_{\\text{p}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E. Specifically, the CGNL-based self-attentive module takes \u003Cinline-formula\u003E\u003Ctex-math id=\"M54\"\u003E$$ K $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E clusters \u003Cinline-formula\u003E\u003Ctex-math id=\"M55\"\u003E$$ C=(C_1, C_2, ...C_K) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E as input. Then, votes from each cluster are processed by the Multi-Layer Perceptron (MLP) and passed to CGNL. The self-attention mechanism allows features from different clusters to interact with each other and find out who they should pay more attention to.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ESimilarly, in the \u003Cinline-formula\u003E\u003Ctex-math id=\"M56\"\u003E$$ \\text{M}_{\\text{o}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E module of the Voting and Attention-based model, the instance-to-instance correlation is modeled. Firstly, \u003Cinline-formula\u003E\u003Ctex-math id=\"M57\"\u003E$$ K $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E clusters from the high-dimensional features and a set of object centers are generated. Then, CGNL is used to model the rich interdependencies between clusters in feature space. The output is a new feature mapping:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(6)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E6\"\u003E $$ \tH_{obj-obj}=CGNL\\left( max\\left(MLP\\left( v_i\\right) \\right) \\right) , i=1, \\ldots, n $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M58\"\u003E$$ v_i $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the \u003Cinline-formula\u003E\u003Ctex-math id=\"M59\"\u003E$$ i $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E-th vote.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EFinally, the new feature maps \u003Cinline-formula\u003E\u003Ctex-math id=\"M60\"\u003E$$ H_{part-part} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M61\"\u003E$$ H_{obj-obj} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are aggregated to the global information by an MLP layer after a max-pooling and concatenation operations.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the Voting and Attention-based model, a multi-task loss is used for joint learning, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(7)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E7\"\u003E $$ \tL=\\lambda_1L_{part-vote}+\\lambda_2L_{object-vote}+\\lambda_3L_{pose} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M62\"\u003E$$ \\lambda_1 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, \u003Cinline-formula\u003E\u003Ctex-math id=\"M63\"\u003E$$ \\lambda_2 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M64\"\u003E$$ \\lambda_3 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the weights of each task. The losses include voting partial loss \u003Cinline-formula\u003E\u003Ctex-math id=\"M65\"\u003E$$ L_{part-vote} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, object voting loss \u003Cinline-formula\u003E\u003Ctex-math id=\"M66\"\u003E$$ L_{object-vote} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and pose loss \u003Cinline-formula\u003E\u003Ctex-math id=\"M67\"\u003E$$ L_{pose} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe pose estimation results based on the Voting and Attention-based model on nine objects in the Sil\u003Cinline-formula\u003E\u003Ctex-math id=\"M68\"\u003E$$ \\acute{e} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003Eane dataset and two objects in the Fraunhofer IPA dataset are listed in \u003Ca href=\"#Table4\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table4\"\u003ETable 4\u003C\u002Fa\u003E, and some qualitative results are shown in \u003Ca href=\"#Figure7\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure7\"\u003EFigure 7\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table4\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 4\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EThe pose estimation results based on the Voting and Attention-based model on nine objects in the Sil\u003Cinline-formula\u003E\u003Ctex-math id=\"M69\"\u003E$$ \\acute{e} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003Eane dataset and two objects in the Fraunhofer IPA dataset \u003Csup\u003E[\u003Ca href=\"#b44\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b44\"\u003E44\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003EObjects\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd colspan=\"9\" align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003ESiléane dataset\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003EFraunhofer IPA dataset\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border\"\u003E\u003Cb\u003EMean\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EBrick\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EBunny\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EC. stick\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EC.cup\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EGear\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EPepper\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003ETless 20\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003ETless 22\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003ETless 29\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003EGear shaft\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_top_border2\"\u003ERing screw\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M70\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.48\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.61\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.60\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.52\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.64\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.39\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.44\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.37\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.46\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.65\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.67\u003C\u002Ftd\u003E\u003Ctd align=\"center\" style=\"class:table_bottom_border\"\u003E0.53\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure7\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure7\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-7.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 7. Visualization for pose estimation results based on the Voting and Attention-based model: (A) 3D point cloud input; (B) True values of the poses; (C) Results obtained by the method in \u003Csup\u003E[\u003Ca href=\"#b44\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b44\"\u003E44\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. The different color means the visualization of point-wise distance error, ranging from 0 (green) to greater than 0.2 times the diameter of the object (red).\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EThe results in \u003Ca href=\"#Table4\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table4\"\u003ETable 4\u003C\u002Fa\u003E and \u003Ca href=\"#Figure7\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure7\"\u003EFigure 7\u003C\u002Fa\u003E show that the Voting and Attention model is very effective in improving the accuracy of the pose estimation, which can obtain an average precision of 53%.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn addition to the above voting-based model, there are lots of research results in pose estimation based on deep learning methods. For example, Chen \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b46\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b46\"\u003E46\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E presented a probabilistic PnP (EPro-PnP) model for general end-to-end pose estimation, which is based on the method of locating 3D objects from a single RGB image via Perspective-n-Points (PnP). The EPro-PnP model can realize reliable end-to-end training for a PnP-based object pose estimation network by back-propagating the probability density of the pose to learn the 2D-3D association of the object.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ECurrently, there are five main types of methods for pose estimation, including feature-based methods, regression-based methods, projection-based methods, representation learning methods, and graph neural network methods. The feature-based method refers to restoring camera pose by establishing feature correspondence between images and scenes. A regression-based method uses a regressor to predict the camera pose. A projection-based method utilizes projection transformation to estimate the pose of a target from an image or video. A representation learning method utilizes deep neural networks to learn high-resolution representations of objects from images or videos, which can improve the accuracy and interpretability of pose estimation. Graph neural network methods use graph neural networks to learn structured representations of objects from images or videos, which can improve robustness of pose estimation. In the following, a summary of the deep learning-based pose estimation models presented in the last five years is illustrated in \u003Ca href=\"#Table5\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table5\"\u003ETable 5\u003C\u002Fa\u003E, where the type of each method is given out.\u003C\u002Fp\u003E\u003Cdiv id=\"Table5\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 5\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EA summary of the deep learning-based poss estimation models in the last five years\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStructure\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EReference\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EType of the method\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"left\"\u003E\u003Cb\u003EPerformances\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"4\"\u003E\u003Ci\u003EADD\u003C\u002Fi\u003E means average distance metric. \u003Ci\u003EADD-AUC\u003C\u002Fi\u003E means area under the curve.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EV2V-PoseNet\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EMoon \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b47\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b47\"\u003E47\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ERegression-based\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003ETop1 in the HANDS 2017 frame-based dataset.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ECDPN\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELi \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b48\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b48\"\u003E48\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFeature-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003EADD of 89.86% on the LINEMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ENOCS\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EWang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b49\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b49\"\u003E49\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EProjection-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M71\"\u003E$$ mAP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 88.4% for 3D IoU on Occluded LINEMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EDPVL\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EYu \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b50\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b50\"\u003E50\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ERepresentation learning\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003EMean \u003Cinline-formula\u003E\u003Ctex-math id=\"M72\"\u003E$$ ADD $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 91.5% on the LINEMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EG2L-Net\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EChen \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b51\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b51\"\u003E51\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGraph neural network\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003EMean \u003Cinline-formula\u003E\u003Ctex-math id=\"M73\"\u003E$$ ADD $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 98.7% on the LINEMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EPVN3D\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHe \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b52\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b52\"\u003E52\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EProjection-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M74\"\u003E$$ ADD $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 99.4% on the LineMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EFFB6D\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHe \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b53\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b53\"\u003E53\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFeature-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003EMean \u003Cinline-formula\u003E\u003Ctex-math id=\"M75\"\u003E$$ ADD $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 99.7% on the LINEMOD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EROFT\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EPiga \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b26\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b26\"\u003E26\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFeature-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M76\"\u003E$$ ADD-AUC $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 76.59% on the FAST-YCB dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EVoting and Attention\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHoang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b44\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b44\"\u003E44\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFeature-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M77\"\u003E$$ AP $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 53% on the Siléane dataset and Fraunhofer IPA dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EEPro-PnP\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EChen \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b46\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b46\"\u003E46\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EProjection-based\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M78\"\u003E$$ ADD $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 95.80% on the LineMOD Dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec28\" class=\"article-Section\"\u003E\u003Ch3 \u003E3.3. Semantic segmentation\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003ESemantic segmentation is a refined version of image classification. For an image, traditional image classification is to detect and recognize the objects that appear in the image, while semantic segmentation is to classify every pixel point in the image. In the field of autonomous robot environment perception and understanding, semantic segmentation is used to label each pixel in an image with its corresponding semantically meaningful category. Semantic segmentation can help robots recognize and understand surrounding objects and scenes. It is very useful for the semantic segmentation for the robot to find a specific object in the environment. For example, in the field of logistics robotics, semantic segmentation can help the autonomous robots perceive and understand road conditions, traffic signs, pedestrians, and vehicles, which can improve the safety and efficiency of logistics robotics.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe traditional semantic segmentation algorithms are mainly grayscale segmentation, conditional random fields, etc. Grayscale segmentation algorithms recursively segment images into sub-regions until labels can be assigned and then combine adjacent sub-regions with the same labels by merging them. The conditional random field is a type of statistical modeling method for structured prediction.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EWith the continuous development of deep learning techniques, deep learning has been widely applied in semantic segmentation tasks and achieved impressive results. There are a series of classical deep learning-based models for semantic segmentation, such as Full convolution network (FCN) \u003Csup\u003E[\u003Ca href=\"#b54\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b54\"\u003E54\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, SegNet \u003Csup\u003E[\u003Ca href=\"#b55\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b55\"\u003E55\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, DeepLab series \u003Csup\u003E[\u003Ca href=\"#b56\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b56\"\u003E56\u003C\u002Fa\u003E,\u003Ca href=\"#b57\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b57\"\u003E57\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, RefineNet \u003Csup\u003E[\u003Ca href=\"#b58\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b58\"\u003E58\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, DenseASPP \u003Csup\u003E[\u003Ca href=\"#b59\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b59\"\u003E59\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, etc. Recently, some improvements have been proposed based on those classical models. For example, Yin \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E presented a multi-sensor fusion for lane marking semantic segmentation (FusionLane) based on the DeepLabV3+ network. The workflow of the FusionLane model is shown in \u003Ca href=\"#Figure8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure8\"\u003EFigure 8\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure8\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure8\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-8.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 8. The workflow of the FusionLane model \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. CBEV denotes the camera bird's eye view data. LBEV denotes the points cloud bird's eye view data. C-Region denotes the obtained semantic segmentation result on CBEV.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAs shown in \u003Ca href=\"#Figure8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure8\"\u003EFigure 8\u003C\u002Fa\u003E, firstly, the DeepLabV3+ network is used to achieve semantic segmentation on camera BEV (CBEV) data (called as C-Region). Then, the C-Region and LiDAR point cloud BEV (LBEV) data are input into the FusionLane model to realize the lane marking semantic segmentation. Unlike other methods that mainly focus on the analysis of camera images, the semantic segmentation data used in FusionLane is a BEV image converted from the LiDAR point cloud instead of the images captured by the camera to obtain the accurate location information of the segmentation results.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe network contains two data input branches: the camera data and the point cloud data. The data from the two branches need to be preprocessed to meet the network input requirements. For the camera data, the front view is converted into CBEV. In CBEV, one pixel represents an area of \u003Cinline-formula\u003E\u003Ctex-math id=\"M79\"\u003E$$ 5cm\\times 5cm $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E in real space. Then, the CBEV image is semantically segmented using the trained DeepLabV3+ network to obtain the C-Region input data. For the point cloud data, it is projected into the 3D BEV with three channels. The values of the three channels are calculated as follows:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(8)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E8\"\u003E $$ \tF\\left( x, y\\right) =\\frac{\\sum\\nolimits_1^ni}{n}\\times255 $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(9)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E9\"\u003E $$ \tS\\left( x, y\\right) =\\frac{\\sum\\nolimits_1^n\\left( h+2\\right) }{n}\\times255 $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(10)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E10\"\u003E $$ \tT\\left( x, y\\right) =255\\times\\frac{2}{\\pi}\\times arctan\\sqrt{\\frac{\\sum\\nolimits_1^n\\left( h-\\sum\\nolimits_1^n\\frac{h}{n}\\right) ^2}{n}} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M80\"\u003E$$ F\\left( x, y\\right) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, \u003Cinline-formula\u003E\u003Ctex-math id=\"M81\"\u003E$$ S\\left( x, y\\right) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M82\"\u003E$$ T\\left( x, y\\right) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E denote the values of the first channel, the second channel, and the third channel, respectively; \u003Cinline-formula\u003E\u003Ctex-math id=\"M83\"\u003E$$ i \\in [0, 1] $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the reflection intensity value of each point falling within the grid corresponding to the pixel; \u003Cinline-formula\u003E\u003Ctex-math id=\"M84\"\u003E$$ h \\in [ - 2, - 1] $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the height value of each laser spot falling within the grid, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M85\"\u003E$$ arctan $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is used as the normalization function.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the FusionLane model of \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, an encoder-decoder network model is proposed, and the LSTM structure is added to the network to assist the semantic segmentation of the lane marking. At last, the KITTI dataset is used to test the performance of the FusionLane model, which is processed and divided into seven categories. The experimental results are listed in \u003Ca href=\"#Table6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table6\"\u003ETable 6\u003C\u002Fa\u003E, and some segmentation results based on the FusionLane network are shown in \u003Ca href=\"#Figure9\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure9\"\u003EFigure 9\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table6\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 6\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003ESome comparison experiment results of the semantic segmentation on the KITTI dataset \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EMethods\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EBackground\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003ESolid line\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EDotted line\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EArrow\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EProhibited area\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStop line\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EOther point\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EMIoU\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EPA\u003C\u002Fb\u003E (%)\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"10\"\u003E\u003Ci\u003EIoU\u003C\u002Fi\u003E: the evaluation metrics include the Intersection over Union on each category; \u003Ci\u003EMIoU\u003C\u002Fi\u003E: the Mean Intersection over Union; \u003Ci\u003EPA\u003C\u002Fi\u003E: the Pixel Accuracy.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EDeepLabv3+(LBEV)\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.9419\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.2587\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.2648\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.2793\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.1915\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.3586\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.2770\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.3674\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E91.31\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EDeepLabv3+(CBEV)\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.9106\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.6287\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.7012\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.5821\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.6935\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.5294\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E-\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E0.6743\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E85.76\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EFusionLane\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E1.0000\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.7477\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.7838\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.7526\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.7979\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.9053\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.9867\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E0.8535\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E99.92\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure9\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure9\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-9.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 9. The segmentation results based on the FusionLane network for some scenarios \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EThe results in \u003Ca href=\"#Table6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table6\"\u003ETable 6\u003C\u002Fa\u003E show that DeepLabV3+ has a low \u003Cinline-formula\u003E\u003Ctex-math id=\"M86\"\u003E$$ IoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E for all scenarios except \"Background\". However, it can be seen that the FusionLane model achieves the best results in all metrics compared to the traditional DeepLabV3+ model. The results in \u003Ca href=\"#Table6\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table6\"\u003ETable 6\u003C\u002Fa\u003E and \u003Ca href=\"#Figure9\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure9\"\u003EFigure 9\u003C\u002Fa\u003E show that relying on a single kind of sensor, whether camera or LiDAR, cannot give sufficiently accurate semantic segmentation results. Effective fusion of data from different sensors can be considered a viable approach to solving the problem.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn addition to the above DeepLab-based model, there are lots of good semantic segmentation models based on deep learning methods. For example, Hu \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b61\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b61\"\u003E61\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E presented the FANet model, which is based on an improved self-attention mechanism, to capture the rich spatial context at a small computational cost. Sun \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b62\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b62\"\u003E62\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the FuseSeg model, a new RGB and thermal data fusion network, to achieve superior semantic segmentation performance in urban scenes.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EMore and more scholars have researched many results in this field. According to the type of network structure, semantic segmentation can be divided into encoder-decoder structure, attention mechanism, graph neural network, generative adversarial network (GAN), and transfer learning. The semantic segmentation method based on encoder-decoder utilizes the encoder-decoder structure to learn and predict the semantic category of each pixel from an image. The method based on GAN uses a generator and a discriminator to conduct confrontation learning. Attention mechanism is a technique that simulates the process of human visual attention. It can calculate the correlation between different positions or channels, give different weights, and highlight the parts of interest while suppressing irrelevant parts. A graph neural network is a deep neural network that can process graph-structured data, which can update the features of nodes and edges through graph convolution operations. Transfer learning is a machine learning technology that can use the knowledge of one domain (source domain) to help the learning of another domain (target domain), thus reducing the dependence on the labeled data of the target domain. A summary of the deep learning-based 3D semantic segmentation models presented in the last five years is illustrated in \u003Ca href=\"#Table7\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table7\"\u003ETable 7\u003C\u002Fa\u003E, where the type of the network structure of each method is given out.\u003C\u002Fp\u003E\u003Cdiv id=\"Table7\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 7\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EA summary of the deep learning-based models of semantic segmentation in the last five years\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStructure\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EReference\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003ENetwork structure\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"left\"\u003E\u003Cb\u003EPerformances\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EDenseASPP\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EYang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b63\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b63\"\u003E63\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EEncoder-decoder\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M87\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 80.6% on Cityscapes datasets\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EEncNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EZhang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b64\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b64\"\u003E64\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EAttention mechanism\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M88\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 85.9% on PASCAL VOC 2012\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EDANet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFu \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b65\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b65\"\u003E65\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGraph neural network\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M89\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 81.5% on Cityscapes test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EAPCNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHe \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b66\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b66\"\u003E66\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EAttention mechanism\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003EA new record 84.2% on PASCAL VOC 2012 test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ECANet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EZhang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b67\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b67\"\u003E67\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EAttention mechanism\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M90\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 57.1% on PASCAL-5i test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EEfficientFCN\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELiu \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b68\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b68\"\u003E68\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGAN\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M91\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 55.3% on PASCAL Context test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EFuseSeg\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESun \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b62\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b62\"\u003E62\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EEncoder-decoder\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M92\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 54.5% on the dataset released in \u003Csup\u003E[\u003Ca href=\"#b69\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b69\"\u003E69\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EMaskFormer\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ECheng \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b70\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b70\"\u003E70\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ETransformer learning\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M93\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 55.6% on the ADE20K dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EFANet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHu \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b61\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b61\"\u003E61\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EEncoder-decoder\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M94\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 75.5% on Cityscapes test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EFusionLane\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EYin \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b60\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b60\"\u003E60\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EEncoder-decoder\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M95\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 85.35% on KITTI test set\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EBCINet\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EZhou \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2023) \u003Csup\u003E[\u003Ca href=\"#b71\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b71\"\u003E71\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EEncoder-decoder\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M96\"\u003E$$ MIoU $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E score of 52.95% on the NYUv2 dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec29\" class=\"article-Section\"\u003E\u003Ch3 \u003E3.4. Saliency prediction\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EThe human visual system selectively attends to salient parts of a scene and performs a detailed understanding of the most salient regions. The detection of salient regions corresponds to important objects and events in a scene and their mutual relationships. In the field of scene understanding for autonomous robots, the task of the saliency prediction is to mimic the characteristics of human vision to focus on obvious or interested targets by acquiring 3D environment information containing color and depth through sensors. In detail, the saliency prediction needs to identify and segment the most salient objects from the acquired 3D environment information and pay attention to the focal objects.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe traditional saliency prediction problem is commonly known as the task of capturing rare and unique elements from images. Traditionally, salient prediction methods can be classified into three types: (1) Block-based detection models. In this type of method, the linear subspace method is used instead of actual image segmentation, and the significant regions are selected by measuring the feature pair ratio and geometric properties of the region. (2) Region-based detection models. This type of method divides the image into multiple regions, and the saliency of each region is regarded as the sum of the product of its contrast and the weight of all the other regions. (3) Detection model based on external cues of the image. This model utilizes accurate annotations (ground-truth) obtained from the training set, video sequences, similar images, and other sources to make the results more accurate. The performance of the saliency prediction based on similar images will be improved if a large number of data sets are available. In general, the traditional methods use a large amount of saliency a priori information for saliency detection, mainly relying on hand-crafted features. These hand-crafted features have some shortcomings; for example, they may not be able to describe complex image scenes and object structures, cannot adapt to new scenes and objects, and have poor generalization ability. So, the saliency detection based on traditional methods has hit a bottleneck.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ERecently, deep learning-based methods have been used widely in various image tasks (e.g., target detection, semantic segmentation, edge detection, etc.), which provide new ideas for saliency prediction and show surprising effect enhancement in some studies. For example, Lou \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the TranSalNet network model. Its basic workflow is shown in \u003Ca href=\"#Figure10\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure10\"\u003EFigure 10\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure10\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure10\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-10.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 10. The schematic overview of the TranSalNet network \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAs shown in \u003Ca href=\"#Figure10\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure10\"\u003EFigure 10\u003C\u002Fa\u003E, the convolutional neural network (CNN)-based encoding is used to extract features for saliency prediction. The outputs of the CNN encoding are three sets of multi-scale feature maps with \u003Cinline-formula\u003E\u003Ctex-math id=\"M97\"\u003E$$ \\frac{w}{8} \\times \\frac{h}{8} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, \u003Cinline-formula\u003E\u003Ctex-math id=\"M98\"\u003E$$ \\frac{w}{16} \\times \\frac{h}{16} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M99\"\u003E$$ \\frac{w}{32} \\times \\frac{h}{32} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, respectively. Then, these feature maps are input into the transformer encoders to enhance the long-range and contextual information. At last, a CNN decoder is used to fuse the enhanced feature maps from the three transformer encoders. The CNN decoder used in \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E is a full CNN network with seven blocks. The processes from block1 to block6 are as follows:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(11)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E11\"\u003E $$ X_i^f = \\left\\{ \\begin{array}{l} X_i^c , \\begin{array}{*{20}c} {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {\\begin{array}{*{20}c} {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {i = 1} \\hfill \\\\ \\end{array}} \\hfill \\\\ \\end{array} \\\\ {\\rm{ReLU(Upsamle(\\hat X}}_{i - 1}^f {\\rm{)}} \\odot X_i^c {\\rm{), }}\\begin{array}{*{20}c} {} &amp; {} &amp; {i = 2, 3} \\\\ \\end{array} \\\\ {\\rm{Upsamle(\\hat X}}_{i - 1}^f {\\rm{)}}, \\begin{array}{*{20}c} {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {} \\hfill &amp; {i = 4, 5, 6} \\hfill \\\\ \\end{array} \\\\ \\end{array} \\right. $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M100\"\u003E$$ X_i^f $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M101\"\u003E$$ {\\hat X}_{i}^f $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the input and output of the \u003Cinline-formula\u003E\u003Ctex-math id=\"M102\"\u003E$$ i $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E-th block. The output of the block7 \u003Cinline-formula\u003E\u003Ctex-math id=\"M103\"\u003E$$ \\hat y $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the predicted saliency map, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(12)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E12\"\u003E $$ \\hat y = {\\rm{Sigmoid}}({\\rm{Conv}}_{3 \\times 3} (X_6^f )) $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M104\"\u003E$$ \\rm{Sigmoid}(\\cdot) $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the sigmoid activation function; \u003Cinline-formula\u003E\u003Ctex-math id=\"M105\"\u003E$$ {\\rm{Conv}}_{3 \\times 3} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E denotes the \u003Cinline-formula\u003E\u003Ctex-math id=\"M106\"\u003E$$ 3 \\times 3 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E convolution operation; and \u003Cinline-formula\u003E\u003Ctex-math id=\"M107\"\u003E$$ X_6^f $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the output of the block6.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the TranSalNet network model, a linear combination of four losses is used as the loss function, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(13)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E13\"\u003E $$ L = \\omega _1 L_{NSS} + \\omega _2 L_{KLD} + \\omega _3 L_{CC} + \\omega _4 L_{SIM} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M108\"\u003E$$ L_{NSS} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the Normalized Scanpath Saliency loss; \u003Cinline-formula\u003E\u003Ctex-math id=\"M109\"\u003E$$ L_{KLD} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the Kullback–Leibler divergence loss; \u003Cinline-formula\u003E\u003Ctex-math id=\"M110\"\u003E$$ L_{CC} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the Linear Correlation Coefficient loss; and \u003Cinline-formula\u003E\u003Ctex-math id=\"M111\"\u003E$$ L_{SIM} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the Similarity loss. \u003Cinline-formula\u003E\u003Ctex-math id=\"M112\"\u003E$$ \\omega _1 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, \u003Cinline-formula\u003E\u003Ctex-math id=\"M113\"\u003E$$ \\omega _2 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, \u003Cinline-formula\u003E\u003Ctex-math id=\"M114\"\u003E$$ \\omega _3 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M115\"\u003E$$ \\omega _4 $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the weights of each loss.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ESome results of the TranSalNet network model are listed in \u003Ca href=\"#Table8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table8\"\u003ETable 8\u003C\u002Fa\u003E, where TranSalNet_Res and TranSalNet_Dense denote the CNN encoders used in the TranSalNet network, ResNet_50 and DenseNet_161, respectively. Here, two public datasets are as follows: (1) MIT1003 \u003Csup\u003E[\u003Ca href=\"#b73\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b73\"\u003E73\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E: This dataset contains 300 natural images and eye movement data from 39 observers and is the most influential and widely used dataset in the field of image human eye focus detection. (2) CAT2000 \u003Csup\u003E[\u003Ca href=\"#b74\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b74\"\u003E74\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E: This dataset includes 4000 images, 200 in each of 20 categories, covering different types of scenes such as cartoon, art, object, low-resolution image, indoor, outdoor, chaotic, random, and line drawings. Some saliency maps generated by the two models are shown in \u003Ca href=\"#Figure11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure11\"\u003EFigure 11\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table8\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 8\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003ESome results of the TranSalNet network model on MIT1003 and CAT2000 datasets \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" rowspan=\"3\" align=\"center\"\u003E\u003Cb\u003EModel name\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" colspan=\"6\" align=\"center\"\u003E\u003Cb\u003EMIT1003\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" colspan=\"6\" align=\"center\"\u003E\u003Cb\u003ECAT2000\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" colspan=\"3\" align=\"center\"\u003E\u003Cb\u003EPerception metrics\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" colspan=\"3\" align=\"center\"\u003E\u003Cb\u003ENon-perception metrics\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" colspan=\"3\" align=\"center\"\u003E\u003Cb\u003EPerception metrics\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" colspan=\"3\" align=\"center\"\u003E\u003Cb\u003ENon-perception metrics\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ECC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ESIM\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ENSS\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EsAUC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EAUC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKLD\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ECC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ESIM\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003ENSS\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EsAUC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EAUC\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKLD\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ETranSalNet-Res\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.7595\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.6145\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E2.8501\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.7546\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.9093\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.7779\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.8786\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.7492\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E2.4154\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.6054\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.8811\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.5036\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003ETranSalNet-Dense\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.7743\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.6279\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E2.9214\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.7547\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.9116\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.7862\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.8823\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.7512\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E2.4290\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.6099\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.8820\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.4715\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure11\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure11\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-11.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 11. Results of saliency maps generated by TranSalNet_Res and TranSalNet_Dense \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. The images from (A) to (C) are from the MIT1003 dataset, and the images from (D) to (F) are from the CAT2000 dataset.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EThe results in \u003Ca href=\"#Table8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table8\"\u003ETable 8\u003C\u002Fa\u003E and \u003Ca href=\"#Figure11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure11\"\u003EFigure 11\u003C\u002Fa\u003E prove that the TranSalNet architecture presented in \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E is effective in the saliency prediction tasks. In addition, the results in \u003Ca href=\"#Table8\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table8\"\u003ETable 8\u003C\u002Fa\u003E and \u003Ca href=\"#Figure11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure11\"\u003EFigure 11\u003C\u002Fa\u003E show that the performance of the TranSalNet could be further enhanced by replacing ResNet-50 with DenseNet-161.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn addition to the above TranSalNet model, there are other saliency prediction models based on deep learning, which also have obtained good results in this field. For example, Zou \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b75\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b75\"\u003E75\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the STA3D model, where the S3D network is used as an encoder and the prediction network with spatial dimensional upsampling and temporal dimensional compression is used as a decoder, to solve the difficulty of video significance prediction in the continuous frame with a fixed offset.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EAt present, there are five types of methods for saliency prediction, including gradient-based methods, perturbation-based methods, SHAP value-based methods, attention mechanism-based methods, and transfer learning-based methods. The gradient-based method utilizes the gradient information of neural networks to calculate the contribution of each pixel in the input image to the output saliency map. The perturbation-based method evaluates the importance of each pixel by randomly or regularly perturbing the input image. The method based on SHAP values utilizes shapely additive explanations to quantify the impact of each pixel on the output saliency map. The saliency prediction, based on attention mechanisms, utilizes an attention mechanism to simulate the process of human visual attention, thereby improving the accuracy and interpretability of saliency prediction. Transfer learning is used to solve the problem of data shortage and domain differences in saliency prediction, which can improve the generalization ability and adaptability of saliency prediction. A summary of the deep learning-based models in the last five years is illustrated in \u003Ca href=\"#Table9\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table9\"\u003ETable 9\u003C\u002Fa\u003E, where the type of each method is given out.\u003C\u002Fp\u003E\u003Cdiv id=\"Table9\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 9\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EA summary of the deep learning-based saliency prediction models in the last five years\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStructure\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EReference\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EType of methods\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"left\"\u003E\u003Cb\u003EPerformances\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"4\"\u003E\u003Ci\u003EMAE\u003C\u002Fi\u003E means mean absolute error. \u003Cinline-formula\u003E\u003Ctex-math id=\"M116\"\u003E$$AUC_{J}$$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E means the area under the receiver operating characteristic curve. \u003Ci\u003EEAO\u003C\u002Fi\u003E means expected average overlap.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EASNet\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EWang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2018) \u003Csup\u003E[\u003Ca href=\"#b76\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b76\"\u003E76\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M117\"\u003E$$ MAE $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E scores of 0.072 on the PASCAL-S dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ERGB-D-SOD\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EHuang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b77\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b77\"\u003E77\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EPerturbation-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M118\"\u003E$$ AUC $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.874 on the NJU400 dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EAF-RGB-D\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E\u003Ci\u003Eet al\u003C\u002Fi\u003E. (2019) \u003Csup\u003E[\u003Ca href=\"#b78\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b78\"\u003E78\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ESHAP value-based methods\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M119\"\u003E$$ MAE $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E scores of 0.0462 on the STEREO dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ECMP-SOI\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EZhang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b79\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b79\"\u003E79\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M120\"\u003E$$ AUC_{J} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.8839 on the ODI dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EDevsNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EFang \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2020) \u003Csup\u003E[\u003Ca href=\"#b80\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b80\"\u003E80\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M121\"\u003E$$ MAE $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E scores of 0.016 on the UVSD dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EAMDFNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELi \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b81\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b81\"\u003E81\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M122\"\u003E$$ MAE $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E scores of 0.019 on the RGBD135 dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ESSPNet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003ELee \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b82\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b82\"\u003E82\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M123\"\u003E$$ EAO $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.285 on the VOT-2017 dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003ESTA3D\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EZou \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2021) \u003Csup\u003E[\u003Ca href=\"#b75\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b75\"\u003E75\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EGradient-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M124\"\u003E$$ AUC_{J} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.927 on the Hollywood2-actions dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EECANet\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EXue \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b83\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b83\"\u003E83\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003EAttention mechanism-based\u003C\u002Ftd\u003E\u003Ctd align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M125\"\u003E$$ AUC_{J} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.903 on the DHF1K dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003ETranSalNet\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003ELou \u003Ci\u003Eet al\u003C\u002Fi\u003E. (2022) \u003Csup\u003E[\u003Ca href=\"#b72\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b72\"\u003E72\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003ETransformer learning-based\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"left\"\u003E\u003Cinline-formula\u003E\u003Ctex-math id=\"M126\"\u003E$$ AUC $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E of 0.9116 on the MIT1003 dataset\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec210\" class=\"article-Section\"\u003E\u003Ch3 \u003E3.5. Other applications\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EIn addition to the applications mentioned above, there are many other applications of deep learning methods in autonomous robot environment perception and understanding, such as image enhancement \u003Csup\u003E[\u003Ca href=\"#b84\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b84\"\u003E84\u003C\u002Fa\u003E,\u003Ca href=\"#b85\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b85\"\u003E85\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, visual SLAM \u003Csup\u003E[\u003Ca href=\"#b1\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b1\"\u003E1\u003C\u002Fa\u003E,\u003Ca href=\"#b86\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b86\"\u003E86\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, scene classification \u003Csup\u003E[\u003Ca href=\"#b87\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b87\"\u003E87\u003C\u002Fa\u003E,\u003Ca href=\"#b88\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b88\"\u003E88\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, moving object detection \u003Csup\u003E[\u003Ca href=\"#b89\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b89\"\u003E89\u003C\u002Fa\u003E,\u003Ca href=\"#b90\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b90\"\u003E90\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, and layout estimation \u003Csup\u003E[\u003Ca href=\"#b91\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b91\"\u003E91\u003C\u002Fa\u003E,\u003Ca href=\"#b92\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b92\"\u003E92\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. In this section, some recent jobs of our group related to this review will be introduced in detail as follows.\u003C\u002Fp\u003E\u003Cdiv id=\"sec32\" class=\"article-Section\"\u003E\u003Ch4 \u003E3.5.1. Visual SLAM\u003C\u002Fh4\u003E\u003Cp class=\"\"\u003EWhen a robot enters an unknown environment, vision SLAM technology can be used to solve the problem of the robots about where they are. It estimates the current position, pose, and travel trajectory of the robot in the 3D scene by the changes in the visual data acquired during the robot's travel. In order to implement vision SLAM, there are three main methods: feature-based methods, direct methods, and semi-direct methods.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EWith feature-based visual SLAM methods, feature points are found and matched. Then, the poses of robots are calculated, and maps are built from geometric relationships. Scalar Transformation (SIFT) \u003Csup\u003E[\u003Ca href=\"#b11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b11\"\u003E11\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, Accelerated Robust Feature (SURF) \u003Csup\u003E[\u003Ca href=\"#b93\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b93\"\u003E93\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, and Fast Rotational Abbreviation (ORB) \u003Csup\u003E[\u003Ca href=\"#b94\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b94\"\u003E94\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E are the most frequently used feature extraction techniques. The most widely used method for visual SLAM is ORB-SLAM \u003Csup\u003E[\u003Ca href=\"#b95\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b95\"\u003E95\u003C\u002Fa\u003E,\u003Ca href=\"#b96\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b96\"\u003E96\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. To overcome the problem of high computational complexity in the traditional ORB-SLAM, Fu \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b97\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b97\"\u003E97\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed the Fast ORB-SLAM that is light-weight and efficient as it tracks keypoints between adjacent frames without computing descriptors.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EDirect methods do not rely on one-to-one matching of points. These types of methods minimize the photometric error function of the pixels by extracting pixels with significant gradients and optimizing the inter-frame pose. The classical direct methods include Large Scale Direct Monocular SLAM (LSD-SLAM) \u003Csup\u003E[\u003Ca href=\"#b98\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b98\"\u003E98\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, Direct Sparse Range (DSO) \u003Csup\u003E[\u003Ca href=\"#b99\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b99\"\u003E99\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, etc. Recently, Wang \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b100\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b100\"\u003E100\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E introduced a new ceiling-view visual odometry method that introduces plane constraints as additional conditions and achieves better accuracy.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ESemi-direct methods, such as SVO \u003Csup\u003E[\u003Ca href=\"#b101\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b101\"\u003E101\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, employ a similar structure to the feature-based methods, which combine the tracking of the direct method with the motion optimization of feature-based methods. Both feature-based and semi-direct methods rely on highly repeatable low-level geometric feature extractors. Both of them are inappropriate for surfaces with little texture or many repetitive features.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EDirect methods, on the other hand, can be applied to a wider variety of scenes. However, compared to feature-based methods, direct methods are less robust. The performance of the direct visual SLAM system under the influence of various camera imaging perturbations will be reduced obviously. To deal with this problem, our group proposed an improved Direct Sparse Odometry with Loop Closure (LDSO) method \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, which is shown in \u003Ca href=\"#Figure12\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure12\"\u003EFigure 12\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure12\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure12\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-12.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 12. The framework of the improved LDSO method based on the variable radius side window \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EIn the framework of the improved LDSO shown in \u003Ca href=\"#Figure12\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure12\"\u003EFigure 12\u003C\u002Fa\u003E, the region surrounding each pixel is divided into blocks when a new frame is introduced, using the side window approach. Then, a CNN structure is created by this multiple-layer superposition of pixel information fusion \u003Csup\u003E[\u003Ca href=\"#b22\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b22\"\u003E22\u003C\u002Fa\u003E,\u003Ca href=\"#b103\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b103\"\u003E103\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. The middle layer shows the presence of semi-static items. In the later layers, the radius of the side windows of the pixels belonging to the semi-static objects is increased. Points with an adequate gradient intensity and corners are chosen using dynamic grid searches. The robustness of the system is increased by the addition of points in direct SLAM. To accomplish edge protection, the fusion method is used with a side window mechanism. Finally, to lessen the weight of semi-static objects, the radius of the adjustment side windows is modified in accordance with the semantic information based on a pre-trained Yolov5 model \u003Csup\u003E[\u003Ca href=\"#b104\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b104\"\u003E104\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the experiments to test the performance of the improved LDSO in \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, two public datasets are used: the KITTI dataset (outdoor datasets) and the TUM RGB-D dataset (indoor datasets). To test the improved LDSO under different camera sensor noises, Gaussian noise and Salt-and-Pepper noise are added to the two datasets. Some results of visual SLAM based on the improved LDSO are shown in \u003Ca href=\"#Table10\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table10\"\u003ETable 10\u003C\u002Fa\u003E and \u003Ca href=\"#Table11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table11\"\u003ETable 11\u003C\u002Fa\u003E, where RMSE\u003Cinline-formula\u003E\u003Ctex-math id=\"M127\"\u003E$$ _{\\rm{ATE}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E means the root mean squared error of absolute trajectory error. The comparison results on the KITTI dataset with Salt-and-Pepper noise are not given out because the general LDSO is entirely inoperable on the datasets under Salt-and-Pepper noise. The results of the point cloud map constructed on the sequence 'KITTI_07' in the KITTI dataset are shown in \u003Ca href=\"#Figure13\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure13\"\u003EFigure 13\u003C\u002Fa\u003E. The results in \u003Ca href=\"#Table10\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table10\"\u003ETables 10\u003C\u002Fa\u003E and \u003Ca href=\"#Table11\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table11\"\u003E11\u003C\u002Fa\u003E show that the improved LDSO in \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E can work efficiently in both the indoor and the outdoor datasets under different noises, while the general LDSO will fail to track (see \u003Ca href=\"#Figure13\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure13\"\u003EFigure 13\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\u003Cdiv id=\"Table10\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 10\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003ERMSE\u003Cinline-formula\u003E\u003Ctex-math id=\"M128\"\u003E$$ _{\\rm{ATE}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E on the KITTI dataset with Gaussian noise \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" rowspan=\"2\" align=\"center\"\u003E\u003Cb\u003EMethod\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" colspan=\"12\" align=\"center\"\u003E\u003Cb\u003EGaussian noise\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_00\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_01\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_02\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_03\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_04\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_05\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_06\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_07\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_08\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_09\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EKITTI_10\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003EAverage\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"13\"\u003E‘–’ means tracking failure. The average value is calculated based on the number of successes.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ELDSO \u003Csup\u003E[\u003Ca href=\"#b105\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b105\"\u003E105\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E22.543\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E23.052\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E169.247\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E44.010\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E58.729\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E53.481\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E130.993\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E16.277\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E64.792\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EImproved LDSO\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E17.772\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E13.023\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E120.380\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E2.133\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E1.093\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E5.740\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E13.491\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E1.973\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E102.206\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E52.664\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E14.042\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E31.320\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"Table11\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 11\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003ERMSE\u003Cinline-formula\u003E\u003Ctex-math id=\"M129\"\u003E$$ _{\\rm{ATE}} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E on the TUM RGB-D dataset \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" rowspan=\"2\" align=\"center\"\u003E\u003Cb\u003EMethod\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" colspan=\"5\" align=\"center\"\u003E\u003Cb\u003EGaussian noise\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" colspan=\"5\" align=\"center\"\u003E\u003Cb\u003ESalt-and-pepper noise\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_xyz\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr2_xyz\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr2_rpy\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_desk\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_desk2\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_xyz\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr2_xyz\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr2_rpy\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_desk\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E\u003Cb\u003Efr1_desk2\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctfoot\u003E\u003Ctr\u003E\u003Ctd align=\"left\" colspan=\"11\"\u003E‘–’ means tracking failure.\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftfoot\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003ELDSO \u003Csup\u003E[\u003Ca href=\"#b105\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b105\"\u003E105\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.096\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.518\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E0.841\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E–\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EImproved LDSO\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.156\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.01\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.06\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.801\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.756\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.129\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.011\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.058\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.796\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E0.871\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure13\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure13\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-13.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 13. Sample outputs of the sequence 'KITTI_07' in the KITTI dataset \u003Csup\u003E[\u003Ca href=\"#b102\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b102\"\u003E102\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E: (A) and (B) are the outputs on the sequence with Gaussian noise and Salt-and-Pepper noise, respectively.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec33\" class=\"article-Section\"\u003E\u003Ch4 \u003E3.5.2. Scene classification\u003C\u002Fh4\u003E\u003Cp class=\"\"\u003EScene classification is one of the key technologies of scene understanding for autonomous robots, which can provide the basis for decision-making of the robots. The task of the scene classification for an autonomous robot refers to the information of its surroundings obtained by the on-board sensors, and then the state of the current position is recognized.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ELots of researchers have conducted studies on scene classification. For example, Tang \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b106\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b106\"\u003E106\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed an adaptive discriminative region learning network for remote sensing scene classification, which locates discriminative regions effectively for solving the problems of scene classification, such as scale-variation of objects and redundant and noisy areas. Song \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b107\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b107\"\u003E107\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E used an ensemble alignment subspace adaptation method for the cross-scene classification. It can settle the problem of both foreign objects in the same spectrum and different spectra. Zhu \u003Ci\u003Eet al\u003C\u002Fi\u003E. \u003Csup\u003E[\u003Ca href=\"#b108\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b108\"\u003E108\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E proposed a domain adaptation cross-scene classification approach to simultaneously classify the target common categories and detect the target private categories based on the divergence of different classifiers.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe methods for the scene classification can be divided into two main types. One of them is based on the underlying visual features. This type of method has some shortcomings. For example, the accuracy of the scene classification is low when only the low-level visual features are used to represent the contents of the scene. The other type of the scene classification method is based on the deep learning technologies. To deal with the problem of the scene classification of the road scene, our group presented an improved deep network-based model \u003Csup\u003E[\u003Ca href=\"#b109\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b109\"\u003E109\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. The structure of the proposed model is shown in \u003Ca href=\"#Figure14\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure14\"\u003EFigure 14\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure14\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure14\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-14.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 14. The structure of the proposed deep network for the road scene classification \u003Csup\u003E[\u003Ca href=\"#b109\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b109\"\u003E109\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EAs shown in \u003Ca href=\"#Figure14\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure14\"\u003EFigure 14\u003C\u002Fa\u003E, there are four main parts in the proposed scene classification model, namely, (1) The improved Faster RCNN-based local feature extraction module; (2) The improved Inception_V1-based global feature extraction module; (3) The feature fusion module; (4) The classification network.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the improved Faster RCNN-based local feature extraction module, the VGG16 Net is used to get the feature map of the whole image first. Then, a residual attention module is used to further deal with redundant information in images. The operation on the feature map based on the residual attention module is:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(14)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E14\"\u003E $$ F_{output} (i, j) = F_{input} (i, j) \\otimes a_{ij}+F_{input} (i, j) $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M130\"\u003E$$ F_{output} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M131\"\u003E$$ F_{input} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E are the output and the input feature value of the residual attention module, respectively; \u003Cinline-formula\u003E\u003Ctex-math id=\"M132\"\u003E$$ a_{ij} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the attention weight; \u003Cinline-formula\u003E\u003Ctex-math id=\"M133\"\u003E$$ \\otimes $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the dot product operation.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe output of the residual attention module is input into the RPN to generate region proposals. The output Region-of-Interests (ROIs) of the RPN is processed by a ROI pooling network to get a fixed-size proposal feature map, which is finally input into a fully connected layer for the object classification and generating the positions of the objects.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the global feature extraction module, the Inception_V1 is used as the baseline network, which has nine Inception blocks. One Inception block has four branches. To deal with the shortcomings of the general Inception_V1 \u003Csup\u003E[\u003Ca href=\"#b110\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b110\"\u003E110\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, the Inception_V1 is improved in the proposed model in \u003Csup\u003E[\u003Ca href=\"#b109\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b109\"\u003E109\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E, where a mixed activation function is presented by alternately using the ELU and Leaky ReLU functions for the Inception networks. The Leaky ReLU function is denoted by:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(15)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E15\"\u003E $$ y_i = \\left\\{ \\begin{array}{l} x_i, \\text{ if } x_i \\ge 0 \\\\ \\alpha x_i, \\text{ if } \\alpha x_i &lt; 0 \\\\ \\end{array} \\right. $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M134\"\u003E$$ \\alpha $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is a fixed parameter.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe ELU function is denoted by:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(16)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E16\"\u003E $$ y_i = \\left\\{ \\begin{array}{l} x_i, \\; \\text{if}\\; \\; x_i \\ge 0 \\\\ e^{x_i} - 1, \\; \\text{if}\\; \\; x_i&lt;0 \\\\ \\end{array} \\right. $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIn the feature fusion module, the local feature vectors and the global feature vectors are appended to get the fused feature \u003Cinline-formula\u003E\u003Ctex-math id=\"M135\"\u003E$$ F $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E, namely\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(17)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E17\"\u003E $$ F = \\left[ {L, G} \\right] $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M136\"\u003E$$ L{\\rm{ = }}\\left[ {l _{\\rm{1}}, l _2, \\cdots, l _N } \\right] $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E and \u003Cinline-formula\u003E\u003Ctex-math id=\"M137\"\u003E$$ G{\\rm{ = }}\\left[ {g _{\\rm{1}}, g_2, \\cdots, g_N } \\right] $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E denote the local feature vectors and the global feature vectors, respectively; \u003Cinline-formula\u003E\u003Ctex-math id=\"M138\"\u003E$$ N $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the feature dimension.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EAt last, the fused feature vector is input to the classification network for the scene classification. The loss function used in this classification network is as follows:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E\u003Cdiv class=\"disp-formula\"\u003E\u003Clabel\u003E(18)\u003C\u002Flabel\u003E\u003Ctex-math id=\"E18\"\u003E $$ Loss_{cls} = \\frac{1}{S}\\sum\\limits_i {\\left( { - \\sum\\limits_{j = 1}^C {y_{ij} \\log \\left( {p_{ij} } \\right)} } \\right)} $$ \u003C\u002Ftex-math\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\u003Cp class=\"\"\u003Ewhere \u003Cinline-formula\u003E\u003Ctex-math id=\"M139\"\u003E$$ C $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the number of scene classification; \u003Cinline-formula\u003E\u003Ctex-math id=\"M140\"\u003E$$ S $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the number of the samples; \u003Cinline-formula\u003E\u003Ctex-math id=\"M141\"\u003E$$ p_{ij} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the probability that the \u003Cinline-formula\u003E\u003Ctex-math id=\"M142\"\u003E$$ i $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E-th sample belongs to the \u003Cinline-formula\u003E\u003Ctex-math id=\"M143\"\u003E$$ j $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E-th category, and \u003Cinline-formula\u003E\u003Ctex-math id=\"M144\"\u003E$$ y_{ij} $$\u003C\u002Ftex-math\u003E\u003C\u002Finline-formula\u003E is the indicator variable.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003ETo test the performance of the proposed road scene classification model, our group proposed a special dataset based on two public datasets: KITTI \u003Csup\u003E[\u003Ca href=\"#b111\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b111\"\u003E111\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E and Place365 \u003Csup\u003E[\u003Ca href=\"#b112\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b112\"\u003E112\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E. The results of the comparison experiments are listed in \u003Ca href=\"#Table12\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table12\"\u003ETable 12\u003C\u002Fa\u003E, and some scene classification results based on different models are shown in \u003Ca href=\"#Figure15\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure15\"\u003EFigure 15\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cdiv id=\"Table12\" class=\"Figure-block\"\u003E\u003Cdiv class=\"table-note\"\u003E\u003Cspan class=\"\"\u003ETable 12\u003C\u002Fspan\u003E\u003Cp class=\"\"\u003EThe experimental results of scene classification based on different deep networks \u003Csup\u003E[\u003Ca href=\"#b109\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b109\"\u003E109\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table-responsive article-table\"\u003E\u003Ctable class=\"a-table\"\u003E\u003Cthead\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003ENetwork\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003ETotal accuracy\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EStandard deviation\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EOn sunny days\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EOn rainy days\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border\" align=\"center\"\u003E\u003Cb\u003EAt night\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003EAlexNet \u003Csup\u003E[\u003Ca href=\"#b113\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b113\"\u003E113\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E84.20%\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E5.22%\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E90.20%\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E81.70%\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_top_border2\" align=\"center\"\u003E80.70%\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EEfficientNet \u003Csup\u003E[\u003Ca href=\"#b114\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b114\"\u003E114\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E87.07%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E8.31%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E96.30%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E80.00%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E85.30%\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd align=\"center\"\u003EInception_V1 \u003Csup\u003E[\u003Ca href=\"#b110\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b110\"\u003E110\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E90.53%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E2.51%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E93.40%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E88.70%\u003C\u002Ftd\u003E\u003Ctd align=\"center\"\u003E89.50%\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003EOurs\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E94.76%\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E1.62%\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E96.50%\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E93.30%\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003Ctd style=\"class:table_bottom_border\" align=\"center\"\u003E\u003Cb\u003E94.50%\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"table_footer\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"Figure-block\" id=\"Figure15\"\u003E\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\" class=\"article-figure-image\"\u003E\u003Ca href=\"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure15\" class=\"Article-img\" alt=\"\" target=\"_blank\"\u003E\u003Cimg alt=\"Deep learning-based scene understanding for autonomous robots: a survey\" src=\"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-15.jpg\" class=\"\" title=\"\" alt=\"\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"article-figure-note\"\u003E\u003Cp class=\"figure-note\"\u003E\u003C\u002Fp\u003E\u003Cp class=\"figure-note\"\u003EFigure 15. Some scene classification results based on different models \u003Csup\u003E[\u003Ca href=\"#b109\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"b109\"\u003E109\u003C\u002Fa\u003E]\u003C\u002Fsup\u003E.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"\"\u003EIt can be seen that our proposed model can improve the accuracy to 94.76%, which is 4.67% (Relative value) higher than the general Inception_V1 (the second-best model). In addition, our proposed model has good scene classification performance under some challenging tasks, such as the task on a rainy day or at night (see \u003Ca href=\"#Table12\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Table12\"\u003ETable 12\u003C\u002Fa\u003E and \u003Ca href=\"#Figure15\" class=\"Link_style\" data-jats-ref-type=\"bibr\" data-jats-rid=\"Figure15\"\u003EFigure 15\u003C\u002Fa\u003E for details).\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec14\" class=\"article-Section\"\u003E\u003Ch2 \u003E4. FUTURE DIRECTIONS\u003C\u002Fh2\u003E\u003Cp class=\"\"\u003EWith the developments of the artificial intelligence technologies and deep learning methods, great progress has been made in the research of scene understanding for autonomous robots. However, there are still a lot of difficulties in using deep learning to perceive and understand the surroundings for autonomous robots. There are some problems that should be further studied as follows:\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(1) Light-weight models: With the continuous improvement of the computing power of hardware devices, the scene understanding method based on deep learning technology has achieved great success. However, it is difficult to run large-scale models on autonomous robots with limited processing, memory, and power resources. How to design a practical light-weight deep learning model while keeping the desired accuracy is a challenging task. Meanwhile, it also needs to develop efficient compact representation models for 3D data.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(2) Multi-task learning: A valuable but less explored direction for scene understanding is to jointly train models on multiple terminal tasks. For example, semantic contour detection technology could jointly detect target contours and recognize the semantic information of the contours. This multi-task learning method is useful for model learning without decreasing the performance of any single task.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(3) Transfer learning: Common tasks such as object detection, semantic segmentation, and scene classification usually have many annotated examples for training. However, there is a lack of large datasets for tasks such as layout estimation, affordance prediction, and physics-based reasoning. How to optimally fine-tune an existing model to the desired task so that the knowledge is properly transferred from the source domain to the target domain is a good research direction in this field.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(4) Multi-modal fusion: Building a cross-modal adaptive fusion network will allow us to more fully fuse the sparse information in the point cloud space with the dense information in the image space. Based on these multi-modal fusion methods, the accuracy of the scene understanding can be further improved. In this field, how to fuse different modal information efficiently is a good research direction.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(5) The specific datasets: To improve the performance of the deep learning-based models, some specific datasets should be constructed for the applications of the robots in different environments. For example, how to make the autonomous underwater vehicle (AUV) work efficiently is still a challenging task. The main reason is that the underwater environments are complex; for example, the illumination is low, and the reference objects are fewer. To build a specific dataset for special robots is arduous, but it is very meaningful.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003E(6) Application extensions: With the popularization of robot applications and the important role that robots play in various fields, we need to take a step forward in researching the applications of scene understanding for autonomous robots. In addition to the applications mentioned above, such as target detection and pose estimation, we need to focus on more application extensions, such as physics-based reasoning, affordance prediction, full 3D reconstruction, etc.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EThe scene understanding of autonomous robots is the first prerequisite for autonomous robots to complete complex tasks. On this basis, robots can become smarter to further improve social productivity, produce huge social benefits, and improve people's life quality. Therefore, there are many problems that need to be solved efficiently. The deep learning-based methods for the robotic scene understanding are still on the way.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec15\" class=\"article-Section\"\u003E\u003Ch2 \u003E5. CONCLUSIONS\u003C\u002Fh2\u003E\u003Cp class=\"\"\u003EThis study analyzes the most recent advancements in deep learning-based environment perception and understanding methods for autonomous robots. Firstly, this paper provides a summary of recent advances in the ability of autonomous robots to perceive and understand their environments. The typical application techniques for perceiving and understanding the surroundings by autonomous robots are discussed. Then, the research and application of deep learning-based methods in the field of scene understanding for autonomous robots are further discussed in this study, which also presents exemplary techniques for the use of robot environment perception and understanding. Lastly, the main issues and difficulties of deep learning-based autonomous robot scene understanding are examined.\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EIt is obvious that the deep learning method will become one of the most popular research topics in the field of autonomous robot scene understanding, including theoretical and applied research. Deep learning-based technologies will further improve the intelligence and autonomy of robots. With a better perception and understanding of the environment, the robots will be able to solve complex tasks instead of just performing some simple and single commands. At present, many fundamental problems of robot scene understanding based on deep learning have been explored with exciting results, which show the potential of deep learning. But there are still many questions that need to be further studied.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec16\" class=\"article-Section\"\u003E\u003Ch2 \u003EDECLARATIONS\u003C\u002Fh2\u003E\u003Cdiv id=\"sec21\" class=\"article-Section\"\u003E\u003Ch3 \u003EAuthors' contributions\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EFunding acquisition: Ni J\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EProject administration: Ni J, Shi P\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EWriting-original draft: Chen Y, Tang G\u003C\u002Fp\u003E\u003Cp class=\"\"\u003EWriting-review and editing: Shi J, Cao W\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec22\" class=\"article-Section\"\u003E\u003Ch3 \u003EAvailability of data and materials\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003ENot applicable.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec23\" class=\"article-Section\"\u003E\u003Ch3 \u003EFinancial support and sponsorship\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EThis work has been supported by the National Natural Science Foundation of China (61873086) and the Science and Technology Support Program of Changzhou (CE20215022).\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec24\" class=\"article-Section\"\u003E\u003Ch3 \u003EConflicts of interest\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003EThe authors declared that they have no conflicts of interest related to this work.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec25\" class=\"article-Section\"\u003E\u003Ch3 \u003EEthical approval and consent to participate\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003ENot applicable.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec26\" class=\"article-Section\"\u003E\u003Ch3 \u003EConsent for publication\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003ENot applicable.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"sec27\" class=\"article-Section\"\u003E\u003Ch3 \u003ECopyright\u003C\u002Fh3\u003E\u003Cp class=\"\"\u003E© The Author(s) 2023.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E",translate:bZ},ArtlanguageList:bZ,ArtDataF:[{id:1480728,article_id:b,reference_num:d,reference:"Ni&nbsp;J, Wang&nbsp;X, Gong&nbsp;T, Xie&nbsp;Y. An improved adaptive ORB-SLAM method for monocular vision robot under dynamic environments. \u003Ci\u003EInt J Mach Learn Cyber\u003C\u002Fi\u003E 2022;13:3821-36.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs13042-022-01627-2",pubmed:a,pmc:a},{id:1480729,article_id:b,reference_num:g,reference:"Li&nbsp;J, Xu&nbsp;Z, Zhu&nbsp;D, et al. Bio-inspired intelligence with applications to robotics: a survey. \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E 2021;1:58-83.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2021.08",pubmed:a,pmc:a},{id:1480730,article_id:b,reference_num:i,reference:"Ni&nbsp;J, Tang&nbsp;M, Chen&nbsp;Y, Cao&nbsp;W. An improved cooperative control method for hybrid unmanned aerial-ground system in multitasks. \u003Ci\u003EInt J Aerosp Eng\u003C\u002Fi\u003E 2020; doi: 10.1155\u002F2020\u002F9429108.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1155\u002F2020\u002F9429108",pubmed:a,pmc:a},{id:1480731,article_id:b,reference_num:k,reference:"Zhao&nbsp;ZQ, Zheng&nbsp;P, Xu&nbsp;ST, Wu&nbsp;X. Object Detection with Deep Learning: A Review. \u003Ci\u003EIEEE Trans Neural Netw Learn Syst\u003C\u002Fi\u003E 2019;30:3212-32.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTNNLS.2018.2876865",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F30703038",pmc:a},{id:1480732,article_id:b,reference_num:l,reference:"Garcia-Garcia&nbsp;A, Orts-Escolano&nbsp;S, Oprea&nbsp;S, Villena-Martinez&nbsp;V, Martinez-Gonzalez&nbsp;P, Garcia-rodriguez&nbsp;J. A survey on deep learning techniques for image and video semantic segmentation. \u003Ci\u003EAppl Soft Comput\u003C\u002Fi\u003E 2018;70:41-65.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.asoc.2018.05.018",pubmed:a,pmc:a},{id:1480733,article_id:b,reference_num:n,reference:"Wang&nbsp;L, Huang&nbsp;Y. A survey of 3D point cloud and deep learning-based approaches for scene understanding in autonomous driving. \u003Ci\u003EIEEE Intell Transport Syst Mag\u003C\u002Fi\u003E 2022;14:135-54.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FMITS.2021.3109041",pubmed:a,pmc:a},{id:1480734,article_id:b,reference_num:G,reference:"Naseer&nbsp;M, Khan&nbsp;S, Porikli&nbsp;F. Indoor scene understanding in 2.5\u002F3D for autonomous agents: a survey. \u003Ci\u003EIEEE Access\u003C\u002Fi\u003E 2019;7:1859-87.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FACCESS.2018.2886133",pubmed:a,pmc:a},{id:1480735,article_id:b,reference_num:H,reference:"Zhu&nbsp;M, Ferstera&nbsp;A, Dinulescu&nbsp;S, et al. A peristaltic soft, wearable robot for compression therapy and massage. \u003Ci\u003EIEEE Robot Autom\u003C\u002Fi\u003E 2023;8:4665-72.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLRA.2023.3287773",pubmed:a,pmc:a},{id:1480736,article_id:b,reference_num:I,reference:"Sun&nbsp;P, Shan&nbsp;R, Wang&nbsp;S. An intelligent rehabilitation robot with passive and active direct switching training: improving intelligence and security of human-robot interaction systems. \u003Ci\u003EIEEE Robot Automat Mag\u003C\u002Fi\u003E 2023;30:72-83.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FMRA.2022.3228490",pubmed:a,pmc:a},{id:1480737,article_id:b,reference_num:J,reference:"Wang&nbsp;TM, Tao&nbsp;Y, Liu&nbsp;H. Current researches and future development trend of intelligent robot: a review. \u003Ci\u003EInt J Autom Comput\u003C\u002Fi\u003E 2018;15:525-46.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs11633-018-1115-1",pubmed:a,pmc:a},{id:1480738,article_id:b,reference_num:b$,reference:"Lowe&nbsp;DG. Distinctive image features from scale-invariant keypoints. \u003Ci\u003EInt J comput vis\u003C\u002Fi\u003E 2004;60:91-110.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1023\u002FB:VISI.0000029664.99615.94",pubmed:a,pmc:a},{id:1480739,article_id:b,reference_num:ca,reference:"Zhou&nbsp;H, Yuan&nbsp;Y, Shi&nbsp;C. Object tracking using SIFT features and mean shift. \u003Ci\u003EComput Vis Image Underst\u003C\u002Fi\u003E 2009;113:345-52.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.cviu.2008.08.006",pubmed:a,pmc:a},{id:1480740,article_id:b,reference_num:cb,reference:"Oliva&nbsp;A, Torralba&nbsp;A. Modeling the shape of the scene: a holistic representation of the spatial envelope. \u003Ci\u003EInt J comput vis\u003C\u002Fi\u003E 2001;42:145-75.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1023\u002FA:1011139631724",pubmed:a,pmc:a},{id:1480741,article_id:b,reference_num:"14",reference:"Hofmann&nbsp;T. Unsupervised learning by probabilistic latent semantic analysis. \u003Ci\u003EMach Learn\u003C\u002Fi\u003E 2001;42:177-96.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1023\u002FA:1007617005950",pubmed:a,pmc:a},{id:1480742,article_id:b,reference_num:"15",reference:cc,refdoi:cd,pubmed:ce,pmc:a},{id:1480743,article_id:b,reference_num:"16",reference:"Liu&nbsp;B, Wu&nbsp;H, Su&nbsp;W, Zhang&nbsp;W, Sun&nbsp;J. Rotation-invariant object detection using Sector-ring HOG and boosted random ferns. \u003Ci\u003EVis Comput\u003C\u002Fi\u003E 2018;34:707-19.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs00371-017-1408-3",pubmed:a,pmc:a},{id:1480744,article_id:b,reference_num:"17",reference:"Wang X, Han TX, Yan S. An HOG-LBP human detector with partial occlusion handling. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 32-39.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCV.2009.5459207",pubmed:a,pmc:a},{id:1480745,article_id:b,reference_num:"18",reference:"Vailaya&nbsp;A, Figueiredo&nbsp;MA, Jain&nbsp;AK, Zhang&nbsp;HJ. Image classification for content-based indexing. \u003Ci\u003EXX\u003C\u002Fi\u003E 2001;10:117-30.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002F83.892448",pubmed:a,pmc:a},{id:1480746,article_id:b,reference_num:"19",reference:"Li LJ, Su H, Xing EP, Fei-Fei L. Object bank: a high-level image representation for scene classification &amp; semantic feature sparsification. In: Proceedings of the 23rd International Conference on Neural Information Processing Systems; 2010. p. 1378–86. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Fproceedings.neurips.cc\u002Fpaper\u002F2010\u002Fhash\u002F140f6969d5213fd0ece03148e62e461e-Abstract.html\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Fproceedings.neurips.cc\u002Fpaper\u002F2010\u002Fhash\u002F140f6969d5213fd0ece03148e62e461e-Abstract.html\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480747,article_id:b,reference_num:"20",reference:"Zhang&nbsp;L, Li&nbsp;W, Yu&nbsp;L, Sun&nbsp;L, Dong&nbsp;X, Ning&nbsp;X. GmFace: an explicit function for face image representation. \u003Ci\u003EDisplays\u003C\u002Fi\u003E 2021;68:102022.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.displa.2021.102022",pubmed:a,pmc:a},{id:1480748,article_id:b,reference_num:"21",reference:"Ning&nbsp;X, Gong&nbsp;K, Li&nbsp;W, Zhang&nbsp;L, Bai&nbsp;X, et al. Feature refinement and filter network for person re-identification. \u003Ci\u003EIEEE Trans Circuits Syst Video Technol\u003C\u002Fi\u003E 2021;31:3391-402.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTCSVT.2020.3043026",pubmed:a,pmc:a},{id:1480749,article_id:b,reference_num:"22",reference:"Ni&nbsp;J, Chen&nbsp;Y, Chen&nbsp;Y, Zhu&nbsp;J, Ali&nbsp;D, Cao&nbsp;W. A survey on theories and applications for self-driving cars based on deep learning methods. \u003Ci\u003EAppl Sci\u003C\u002Fi\u003E 2020;10:2749.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Fapp10082749",pubmed:a,pmc:a},{id:1480750,article_id:b,reference_num:"23",reference:"Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition. Providence; 2012 Jun 16-21; RI, USA. IEEE; 2012. p. 3354-61.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2012.6248074",pubmed:a,pmc:a},{id:1480751,article_id:b,reference_num:"24",reference:"Caesar H, Bankiti V, Lang AH, et al. nuScenes: a multimodal dataset for autonomous driving. In: 2020 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11618-28.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR42600.2020.01164",pubmed:a,pmc:a},{id:1480752,article_id:b,reference_num:"25",reference:"Hinterstoisser S, Lepetit V, Ilic S, et al. Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes. In: Lee KM, Matsushita Y, Rehg JM, Hu Z, editors. Computer Vision - ACCV; 2013. p. 548-62.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-642-37331-2-42",pubmed:a,pmc:a},{id:1480753,article_id:b,reference_num:"26",reference:"Piga&nbsp;NA, Onyshchuk&nbsp;Y, Pasquale&nbsp;G, Pattacini&nbsp;U, Natale&nbsp;L. ROFT: Real-time tptical flow-aided 6D object pose and velocity tracking. \u003Ci\u003EIEEE Robot Autom Lett\u003C\u002Fi\u003E 2022;7:159-66.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLRA.2021.3119379",pubmed:a,pmc:a},{id:1480754,article_id:b,reference_num:"27",reference:"Everingham&nbsp;M, Gool&nbsp;LV, Williams&nbsp;CKI, Winn&nbsp;JM, Zisserman&nbsp;A. The pascal visual object classes (VOC) challenge. \u003Ci\u003EInt J Comput Vis\u003C\u002Fi\u003E 2010;88:303-38.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs11263-009-0275-4",pubmed:a,pmc:a},{id:1480755,article_id:b,reference_num:"28",reference:"Cordts M, Omran M, Ramos S, et al. The cityscapes dataset for semantic urban scene understanding. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016 Jun 27-30; Las Vegas, NV, USA. IEEE; 2016. p. 3213-23.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2016.350",pubmed:a,pmc:a},{id:1480756,article_id:b,reference_num:"29",reference:"Wang W, Shen J, Guo F, Cheng MM, Borji A. Revisiting video saliency: a large-scale benchmark and a new model. In: 2018 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 4894-903.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00514",pubmed:a,pmc:a},{id:1480757,article_id:b,reference_num:"30",reference:"Kristan M, Leonardis A, Matas J, et al. The visual object tracking VOT2017 challenge results. In: 2017 IEEE International Conference on Computer Vision Workshops (ICCVW); 2017 Oct 22-29; Venice, Italy. IEEE; 2017. p. 1949-72.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCVW.2017.230",pubmed:a,pmc:a},{id:1480758,article_id:b,reference_num:"31",reference:"Ni&nbsp;J, Shen&nbsp;K, Chen&nbsp;Y, Yang&nbsp;SX. An improved SSD-like deep network-based object detection method for indoor scenes. \u003Ci\u003EIEEE Trans Instrum Meas\u003C\u002Fi\u003E 2023;72:1-15.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTIM.2023.3244819",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F37323850",pmc:a},{id:1480759,article_id:b,reference_num:"32",reference:"Qian&nbsp;R, Lai&nbsp;X, Li&nbsp;X. BADet: boundary-aware 3D object detection from point clouds. \u003Ci\u003EPattern Recognit\u003C\u002Fi\u003E 2022;125:108524.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.patcog.2022.108524",pubmed:a,pmc:a},{id:1480760,article_id:b,reference_num:"33",reference:"Shi&nbsp;S, Wang&nbsp;Z, Shi&nbsp;J, Wang&nbsp;X, Li&nbsp;H. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. \u003Ci\u003EIEEE Trans Pattern Anal Mach Intell\u003C\u002Fi\u003E 2021;43:2647-64.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTPAMI.2020.2977026",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F32142423",pmc:a},{id:1480761,article_id:b,reference_num:"34",reference:"Li&nbsp;Y, Ma&nbsp;L, Zhong&nbsp;Z, Cao&nbsp;D, Li&nbsp;J. TGNet: geometric graph CNN on 3-D point cloud segmentation. \u003Ci\u003EIEEE Trans Geosci Remote Sens\u003C\u002Fi\u003E 2020;58:3588-600.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTGRS.2019.2958517",pubmed:a,pmc:a},{id:1480762,article_id:b,reference_num:"35",reference:"Yan&nbsp;Y, Mao&nbsp;Y, Li&nbsp;B. SECOND: sparsely embedded convolutional detection. \u003Ci\u003ESensors\u003C\u002Fi\u003E 2018;18:3337.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Fs18103337",pubmed:a,pmc:a},{id:1480763,article_id:b,reference_num:"36",reference:"Qi CR, Liu W, Wu C, Su H, Guibas LJ. Frustum pointnets for 3D object detection from RGB-D data. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018 Jun 18-23; Salt Lake City, UT, USA; IEEE; 2018. p. 918-27.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00102",pubmed:a,pmc:a},{id:1480764,article_id:b,reference_num:"37",reference:"Wang Z, Jia K. Frustum convNet: sliding frustums to aggregate local point-wise features for amodal 3D object detection. In: 2019 IEEE\u002FRSJ International Conference on Intelligent Robots and Systems (IROS); 2019 Nov 03-08; Macau, China. IEEE; 2019. p. 1742-49.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FIROS40897.2019.8968513",pubmed:a,pmc:a},{id:1480765,article_id:b,reference_num:"38",reference:"Chen Y, Liu S, Shen X, Jia J. Fast point R-CNN. In: 2019 IEEE\u002FCVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea. IEEE; 2019. p. 9774-83.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCV.2019.00987",pubmed:a,pmc:a},{id:1480766,article_id:b,reference_num:"39",reference:"He C, Zeng H, Huang J, Hua XS, Zhang L. Structure aware single-stage 3D object detection from point cloud. In: 2020 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 11870-9.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR42600.2020.01189",pubmed:a,pmc:a},{id:1480767,article_id:b,reference_num:bY,reference:"Liu Z, Zhao X, Huang T, Hu R, Zhou Y, Bai X. TANet: robust 3D object detection from point clouds with triple attention. In: 34th AAAI Conference on Artificial Intelligence, AAAI; 2020 Feb 7-12; New York, NY, United states. California: AAAI; 2020. p. 11677-84.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v34i07.6837",pubmed:a,pmc:a},{id:1480768,article_id:b,reference_num:"41",reference:"Yin T, Zhou X, Krahenbuhl P. Center-based 3D Object Detection and Tracking. In: 2021 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition, CVPR 2021. Virtual, Online, United states; 2021. pp. 11779 – 11788.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR46437.2021.01161",pubmed:a,pmc:a},{id:1480769,article_id:b,reference_num:"42",reference:"Wang H, Shi S, Yang Z, et al. RBGNet: ray-based Grouping for 3D Object Detection. In: 2022 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2022 Jun 18-24; New Orleans, LA, USA. IEEE; 2022. p. 1100-09.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR52688.2022.00118",pubmed:a,pmc:a},{id:1480770,article_id:b,reference_num:"43",reference:"Chen&nbsp;Y, Ni&nbsp;J, Tang&nbsp;G, Cao&nbsp;W, Yang&nbsp;SX. An improved dense-to-sparse cross-modal fusion network for 3D object detection in RGB-D images. \u003Ci\u003EMultimed Tools Appl\u003C\u002Fi\u003E 2023; doi: 10.1007\u002Fs11042-023-15845-5.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs11042-023-15845-5",pubmed:a,pmc:a},{id:1480771,article_id:b,reference_num:"44",reference:"Hoang&nbsp;DC, Stork&nbsp;JA, Stoyanov&nbsp;T. Voting and attention-based pose relation learning for object pose estimation from 3D point clouds. \u003Ci\u003EIEEE Robot Autom Lett\u003C\u002Fi\u003E 2022;7:8980-7.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLRA.2022.3189158",pubmed:a,pmc:a},{id:1480772,article_id:b,reference_num:"45",reference:"Yue K, Sun M, Yuan Y, Zhou F, Ding E, Xu F. Compact generalized non-local network. arXiv. [Preprint.] November 1, 2018. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1810.13125\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1810.13125\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480773,article_id:b,reference_num:"46",reference:"Chen H, Wang P, Wang F, Tian W, Xiong L, Li H. Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation. arXiv. [Preprint.] August 11, 2022 Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2203.13254\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F2203.13254\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480774,article_id:b,reference_num:"47",reference:"Moon G, Chang JY, Lee KM. V2V-poseNet: voxel-to-voxel prediction network for accurate 3D hand and human pose estimation from a single depth map. In: 2018 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 5079-88.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00533",pubmed:a,pmc:a},{id:1480775,article_id:b,reference_num:"48",reference:"Li Z, Wang G, Ji X. CDPN: coordinates-based disentangled pose network for real-time RGB-based 6-DoF object pose estimation. In: 2019 IEEE\u002FCVF International Conference on Computer Vision (ICCV); 2019 Oct 27 - Nov 02; Seoul, Korea (South). IEEE; 2020. p. 7677-86.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCV.2019.00777",pubmed:a,pmc:a},{id:1480776,article_id:b,reference_num:"49",reference:"Wang H, Sridhar S, Huang J, Valentin J, Song S, Guibas LJ. Normalized object coordinate space for category-level 6D object pose and size estimation. In: 2019 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 2637-46.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2019.00275",pubmed:a,pmc:a},{id:1480777,article_id:b,reference_num:"50",reference:"Yu X, Zhuang Z, Koniusz P, Li H. 6DoF object pose estimation via differentiable proxy voting loss. arXiv. [Preprint.] Febuary 11, 2020. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2002.03923\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F2002.03923\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480778,article_id:b,reference_num:"51",reference:"Chen W, Jia X, Chang HJ, Duan J, Leonardis A. G2L-net: global to local network for real-time 6D pose estimation with embedding vector features. In: 2020 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020 Jun 13-19; Seattle, WA, USA. IEEE; 2020. p. 4232-41.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR42600.2020.00429",pubmed:a,pmc:a},{id:1480779,article_id:b,reference_num:"52",reference:"He Y, Sun W, Huang H, Liu J, Fan H, Sun J. PVN3D: a deep point-wise 3D keypoints voting network for 6DoF pose estimation. In: 2020 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2020 Jun 13-19 Seattle, WA, USA. IEEE; 2020. pp. 11629-38.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR42600.2020.01165",pubmed:a,pmc:a},{id:1480780,article_id:b,reference_num:"53",reference:"He Y, Huang H, Fan H, Chen Q, Sun J. FFB6D: a full flow bidirectional fusion network for 6D pose estimation. In: 2021 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2021 Jun 20-25; Nashville, TN, USA. IEEE; 2021. p. 3002-12.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR46437.2021.00302",pubmed:a,pmc:a},{id:1480781,article_id:b,reference_num:"54",reference:"Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. arXiv. [Preprint.] November 14, 2014. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1411.4038\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1411.4038\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480782,article_id:b,reference_num:"55",reference:"Badrinarayanan&nbsp;V, Kendall&nbsp;A, Cipolla&nbsp;R. SegNet: a deep convolutional encoder-decoder architecture for image segmentation. \u003Ci\u003EIEEE Trans Pattern Anal Mach Intell\u003C\u002Fi\u003E 2017;39:2481-95.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTPAMI.2016.2644615",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F28060704",pmc:a},{id:1480783,article_id:b,reference_num:"56",reference:"Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. Semantic image segmentation with deep convolutional nets and fully connected CRFs. arXiv. [Preprint.] December 22, 2014. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1412.7062\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1412.7062\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480784,article_id:b,reference_num:"57",reference:"Chen&nbsp;LC, Papandreou&nbsp;G, Kokkinos&nbsp;I, Murphy&nbsp;K, Yuille&nbsp;AL. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. \u003Ci\u003EIEEE Trans Pattern Anal Mach Intell\u003C\u002Fi\u003E 2017;40:834-48.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTPAMI.2017.2699184",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F28463186",pmc:a},{id:1480785,article_id:b,reference_num:"58",reference:"Lin G, Milan A, Shen C, Reid I. Refinenet: multi-path refinement networks for high-resolution semantic segmentation. In: 2017IEEE conference on computer vision and pattern recognition (CVPR); 2017. p. 1925-34.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2017.549",pubmed:a,pmc:a},{id:1480786,article_id:b,reference_num:"59",reference:"Zeng L, Zhang S, Wang P, Li Z, Hu Y, Xie T. Defect detection algorithm for magnetic particle inspection of aviation ferromagnetic parts based on improved DeepLabv3+. \u003Ci\u003EMeas Sci Technol\u003C\u002Fi\u003E 2023;34: 065401. Measurement Science and Technology 2023;34: 065401.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1088\u002F1361-6501\u002Facb9ae",pubmed:a,pmc:a},{id:1480787,article_id:b,reference_num:"60",reference:"Yin&nbsp;R, Cheng&nbsp;Y, Wu&nbsp;H, Song&nbsp;Y, Yu&nbsp;B, Niu&nbsp;R. Fusionlane: multi-sensor fusion for lane marking semantic segmentation using deep neural networks. \u003Ci\u003EIEEE Trans Intell Transport Syst\u003C\u002Fi\u003E 2022;23:1543-53.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTITS.2020.3030767",pubmed:a,pmc:a},{id:1480788,article_id:b,reference_num:"61",reference:"Hu&nbsp;P, Perazzi&nbsp;F, Heilbron&nbsp;FC, et al. Real-time semantic segmentation with fast attention. \u003Ci\u003EIEEE Robot Autom Lett\u003C\u002Fi\u003E 2021;6:263-70.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLRA.2020.3039744",pubmed:a,pmc:a},{id:1480789,article_id:b,reference_num:"62",reference:"Sun&nbsp;Y, Zuo&nbsp;W, Yun&nbsp;P, Wang&nbsp;H, Liu&nbsp;M. FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion. \u003Ci\u003EIEEE Trans Automat Sci Eng\u003C\u002Fi\u003E 2021;18:1000-11.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTASE.2020.2993143",pubmed:a,pmc:a},{id:1480790,article_id:b,reference_num:"63",reference:"Yang M, Yu K, Zhang C, Li Z, Yang K. DenseASPP for semantic segmentation in street scenes. In: 2018 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 3684-92.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00388",pubmed:a,pmc:a},{id:1480791,article_id:b,reference_num:"64",reference:"Zhang H, Dana K, Shi J, et al. Context encoding for semantic segmentation. In: 2018 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 7151-60.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00747",pubmed:a,pmc:a},{id:1480792,article_id:b,reference_num:"65",reference:"Fu J, Liu J, Tian H, et al. Dual attention network for scene segmentation. In: 2019 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 3141-9.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2019.00326",pubmed:a,pmc:a},{id:1480793,article_id:b,reference_num:"66",reference:"He J, Deng Z, Zhou L, Wang Y, Qiao Y. Adaptive pyramid context network for semantic segmentation. In: 2019 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 7511-20.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2019.00770",pubmed:a,pmc:a},{id:1480794,article_id:b,reference_num:"67",reference:"Zhang C, Lin G, Liu F, Yao R, Shen C. CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In: 2019 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15-20; Long Beach, CA, USA. IEEE; 2020. p. 5212-21.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2019.00536",pubmed:a,pmc:a},{id:1480795,article_id:b,reference_num:"68",reference:"Liu J, He J, Zhang J, Ren JS, Li H. EfficientFCN: holistically-guided decoding for semantic segmentation. In: Vedaldi A, Bischof H, Brox T, Frahm JM, editors. Computer Vision – ECCV 2020. Cham: Springer; 2020. p. 1-17.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-030-58574-7-1",pubmed:a,pmc:a},{id:1480796,article_id:b,reference_num:"69",reference:"Ha Q, Watanabe K, Karasawa T, Ushiku Y, Harada T. MFNet: towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes. In: 2017 IEEE\u002FRSJ international conference on intelligent robots and systems (IROS); 2017 Sep 24-28; Vancouver, BC, Canada. IEEE; 2017. p. 5108-15.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FIROS.2017.8206396",pubmed:a,pmc:a},{id:1480797,article_id:b,reference_num:"70",reference:"Cheng&nbsp;B, Schwing&nbsp;AG, Kirillov&nbsp;A. Per-pixel classification is not all you need for semantic segmentation. \u003Ci\u003ESignal Process Image Commun\u003C\u002Fi\u003E 2021;88:17864-75.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.image.2020.115950",pubmed:a,pmc:a},{id:1480798,article_id:b,reference_num:"71",reference:"Zhou&nbsp;W, Yue&nbsp;Y, Fang&nbsp;M, Qian&nbsp;X, Yang&nbsp;R, Yu&nbsp;L. BCINet: bilateral cross-modal interaction network for indoor scene understanding in RGB-D images. \u003Ci\u003EInf Fusion\u003C\u002Fi\u003E 2023;94:32-42.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.inffus.2023.01.016",pubmed:a,pmc:a},{id:1480799,article_id:b,reference_num:"72",reference:"Lou&nbsp;J, Lin&nbsp;H, Marshall&nbsp;D, Saupe&nbsp;D, Liu&nbsp;H. TranSalNet: towards perceptually relevant visual saliency prediction. \u003Ci\u003ENeurocomputing\u003C\u002Fi\u003E 2022;494:455-67.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.neucom.2022.04.080",pubmed:a,pmc:a},{id:1480800,article_id:b,reference_num:"73",reference:"Judd T, Ehinger K, Durand F, Torralba A. Learning to predict where humans look. In: 2009 IEEE 12th International Conference on Computer Vision; 2009 Sep 29 - Oct 02; Kyoto, Japan. IEEE; 2010. p. 2106-13.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCV.2009.5459462",pubmed:a,pmc:a},{id:1480801,article_id:b,reference_num:"74",reference:"Ishikura&nbsp;K, Kurita&nbsp;N, Chandler&nbsp;DM, Ohashi&nbsp;G. Saliency detection based on multiscale extrema of local perceptual color differences. \u003Ci\u003EIEEE Trans Image Process\u003C\u002Fi\u003E 2018;27:703-17.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTIP.2017.2767288",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F29185988",pmc:a},{id:1480802,article_id:b,reference_num:"75",reference:"Zou&nbsp;W, Zhuo&nbsp;S, Tang&nbsp;Y, Tian&nbsp;S, Li&nbsp;X, Xu&nbsp;C. STA3D: spatiotemporally attentive 3D network for video saliency prediction. \u003Ci\u003EPattern Recognit Lett\u003C\u002Fi\u003E 2021;147:78-84.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.patrec.2021.04.010",pubmed:a,pmc:a},{id:1480803,article_id:b,reference_num:"76",reference:"Wang W, Shen J, Dong X, Borji A. Salient object detection driven by fixation prediction. In: 2018 IEEE\u002FCVF Conference on Computer Vision and Pattern Recognition; 2018 Jun 18-23; Salt Lake City, UT, USA. IEEE; 2018. p. 1711-20.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FCVPR.2018.00184",pubmed:a,pmc:a},{id:1480804,article_id:b,reference_num:"77",reference:"Huang&nbsp;R, Xing&nbsp;Y, Wang&nbsp;Z. RGB-D salient object detection by a CNN with multiple layers fusion. \u003Ci\u003EIEEE Signal Process Lett\u003C\u002Fi\u003E 2019;26:552-6.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLSP.2019.2898508",pubmed:a,pmc:a},{id:1480805,article_id:b,reference_num:"78",reference:"Wang&nbsp;N, Gong&nbsp;X. Adaptive fusion for RGB-D salient object detection. \u003Ci\u003EIEEE Access\u003C\u002Fi\u003E 2019;7:55277-84.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FACCESS.2019.2913107",pubmed:a,pmc:a},{id:1480806,article_id:b,reference_num:"79",reference:"Zhang&nbsp;J, Yu&nbsp;M, Jiang&nbsp;G, Qi&nbsp;Y. CMP-based saliency model for stereoscopic omnidirectional images. \u003Ci\u003EDigit Signal Process\u003C\u002Fi\u003E 2020;101:102708.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.dsp.2020.102708",pubmed:a,pmc:a},{id:1480807,article_id:b,reference_num:"80",reference:"Fang&nbsp;Y, Zhang&nbsp;C, Min&nbsp;X, et al. DevsNet: deep video saliency network using short-term and long-term cues. \u003Ci\u003EPattern Recognit\u003C\u002Fi\u003E 2020;103:107294.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.patcog.2020.107294",pubmed:a,pmc:a},{id:1480808,article_id:b,reference_num:"81",reference:"Li&nbsp;F, Zheng&nbsp;J, fang&nbsp;Zhang Y, Liu&nbsp;N, Jia&nbsp;W. AMDFNet: adaptive multi-level deformable fusion network for RGB-D saliency detection. \u003Ci\u003ENeurocomputing\u003C\u002Fi\u003E 2021;465:141-56.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.neucom.2021.08.116",pubmed:a,pmc:a},{id:1480809,article_id:b,reference_num:"82",reference:"Lee&nbsp;H, Kim&nbsp;S. SSPNet: learning spatiotemporal saliency prediction networks for visual tracking. \u003Ci\u003EInf Sci\u003C\u002Fi\u003E 2021;575:399-416.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.ins.2021.06.042",pubmed:a,pmc:a},{id:1480810,article_id:b,reference_num:"83",reference:"Xue&nbsp;H, Sun&nbsp;M, Liang&nbsp;Y. ECANet: explicit cyclic attention-based network for video saliency prediction. \u003Ci\u003ENeurocomputing\u003C\u002Fi\u003E 2022;468:233-44.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.neucom.2021.10.024",pubmed:a,pmc:a},{id:1480811,article_id:b,reference_num:"84",reference:"Zhang&nbsp;N, Nex&nbsp;F, Kerle&nbsp;N, Vosselman&nbsp;G. LISU: low-light indoor scene understanding with joint learning of reflectance restoration. \u003Ci\u003ESPRS J Photogramm Remote Sens\u003C\u002Fi\u003E 2022;183:470-81.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.isprsjprs.2021.11.010",pubmed:a,pmc:a},{id:1480812,article_id:b,reference_num:"85",reference:"Tang&nbsp;G, Ni&nbsp;J, Chen&nbsp;Y, Cao&nbsp;W, Yang&nbsp;SX. An improved cycleGAN based model for low-light image enhancement. \u003Ci\u003EIEEE Sensors J\u003C\u002Fi\u003E 2023; doi: 10.1109\u002FJSEN.2023.3296167.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FJSEN.2023.3296167",pubmed:a,pmc:a},{id:1480813,article_id:b,reference_num:"86",reference:"He&nbsp;J, Li&nbsp;M, Wang&nbsp;Y, Wang&nbsp;H. OVD-SLAM: an online visual SLAM for dynamic environments. \u003Ci\u003EIEEE Sensors J\u003C\u002Fi\u003E 2023;23:13210-9.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FJSEN.2023.3270534",pubmed:a,pmc:a},{id:1480814,article_id:b,reference_num:"87",reference:"Lu&nbsp;X, Sun&nbsp;H, Zheng&nbsp;X. A feature aggregation convolutional neural network for remote sensing scene classification. \u003Ci\u003EIEEE Trans Geosci Remote Sens\u003C\u002Fi\u003E 2019;57:7894-906.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTGRS.2019.2917161",pubmed:a,pmc:a},{id:1480815,article_id:b,reference_num:"88",reference:"Ma&nbsp;D, Tang&nbsp;P, Zhao&nbsp;L. SiftingGAN: generating and sifting labeled samples to improve the remote sensing image scene classification baseline \u003Ci\u003Ein vitro\u003C\u002Fi\u003E. \u003Ci\u003EIEEE Geosci Remote Sens Lett\u003C\u002Fi\u003E 2019;16:1046-50.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLGRS.2018.2890413",pubmed:a,pmc:a},{id:1480816,article_id:b,reference_num:"89",reference:"Zhang&nbsp;X, Qiao&nbsp;Y, Yang&nbsp;Y, Wang&nbsp;S. SMod: scene-specific-prior-based moving object detection for airport apron surveillance systems. \u003Ci\u003EIEEE Intell Transport Syst Mag\u003C\u002Fi\u003E 2023;15:58-69.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FMITS.2021.3122926",pubmed:a,pmc:a},{id:1480817,article_id:b,reference_num:"90",reference:"Tang&nbsp;G, Ni&nbsp;J, Shi&nbsp;P, Li&nbsp;Y, Zhu&nbsp;J. An improved ViBe-based approach for moving object detection. \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E 2022;2:130-44.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2022.07",pubmed:a,pmc:a},{id:1480818,article_id:b,reference_num:"91",reference:"Lee CY, Badrinarayanan V, Malisiewicz T, Rabinovich A. Roomnet: end-to-end room layout estimation. arXiv. [Preprint.] August 7, 2017. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06241\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06241\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480819,article_id:b,reference_num:"92",reference:"Hsiao CW, Sun C, Sun M, Chen HT. Flat2layout: Flat representation for estimating layout of general room types. arXiv. [Preprint.] May 29, 2019. Available from: \u003Cext-link ext-link-type=\"uri\" xlink:href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1905.12571\" xmlns:xlink=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1905.12571\u003C\u002Fext-link\u003E [Last accessed on 8 Aug 2023].",refdoi:a,pubmed:a,pmc:a},{id:1480820,article_id:b,reference_num:"93",reference:cc,refdoi:cd,pubmed:ce,pmc:a},{id:1480821,article_id:b,reference_num:"94",reference:"Rublee E, Rabaud V, Konolige K, Bradski G. ORB: an efficient alternative to SIFT or SURF. In: 2011 International conference on computer vision; 2011 Nov 06-13; Barcelona, Spain. IEEE; 2012. p. 2564-71.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICCV.2011.6126544",pubmed:a,pmc:a},{id:1480822,article_id:b,reference_num:"95",reference:"Wang&nbsp;K, Ma&nbsp;S, Ren&nbsp;F, Lu&nbsp;J. SBAS: salient bundle adjustment for visual SLAM. \u003Ci\u003EIEEE Trans Instrum Meas\u003C\u002Fi\u003E 2021;70:1-9.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTIM.2021.3105243",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F33776080",pmc:a},{id:1480823,article_id:b,reference_num:"96",reference:"Ni&nbsp;J, Gong&nbsp;T, Gu&nbsp;Y, Zhu&nbsp;J, Fan&nbsp;X. An improved deep residual network-based semantic simultaneous localization and mapping method for monocular vision robot. \u003Ci\u003EComput Intell Neurosci\u003C\u002Fi\u003E 2020;2020:7490840.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1155\u002F2020\u002F7490840",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F32104171",pmc:a},{id:1480824,article_id:b,reference_num:"97",reference:"Fu&nbsp;Q, Yu&nbsp;H, Wang&nbsp;X, et al. Fast ORB-SLAM without keypoint descriptors. \u003Ci\u003EIEEE Trans Image Process\u003C\u002Fi\u003E 2022;31:1433-46.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTIP.2021.3136710",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F34951846",pmc:a},{id:1480825,article_id:b,reference_num:"98",reference:"Engel J, Schöps T, Cremers D. LSD-SLAM: large-scale direct monocular SLAM. In: Fleet D, Pajdla T, Schiele B, Tuytelaars, editors. Computer Vision – ECCV 2014. Cham: Springer; 2014. p. 834-49.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-319-10605-2-54",pubmed:a,pmc:a},{id:1480826,article_id:b,reference_num:"99",reference:"Engel&nbsp;J, Koltun&nbsp;V, Cremers&nbsp;D. Direct sparse odometry. \u003Ci\u003EIEEE Trans Image Process\u003C\u002Fi\u003E 2018;40:611-25.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTPAMI.2017.2658577",pubmed:a,pmc:a},{id:1480827,article_id:b,reference_num:"100",reference:"Wang&nbsp;Y, Zhang&nbsp;S, Wang&nbsp;J. Ceiling-view semi-direct monocular visual odometry with planar constraint. \u003Ci\u003ERemote Sens\u003C\u002Fi\u003E 2022;14:5447.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Frs14215447",pubmed:a,pmc:a},{id:1480828,article_id:b,reference_num:"101",reference:"Forster&nbsp;C, Zhang&nbsp;Z, Gassner&nbsp;M, Werlberger&nbsp;M, Scaramuzza&nbsp;D. SVO: semidirect visual odometry for monocular and multicamera systems. \u003Ci\u003EIEEE Trans Robot\u003C\u002Fi\u003E 2017;33:249-65.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTRO.2016.2623335",pubmed:a,pmc:a},{id:1480829,article_id:b,reference_num:"102",reference:"Chen&nbsp;Y, Ni&nbsp;J, Mutabazi&nbsp;E, Cao&nbsp;W, Yang&nbsp;SX. A variable radius side window direct SLAM method based on semantic information. \u003Ci\u003EComput Intell Neurosci\u003C\u002Fi\u003E 2022;2022:4075910.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1155\u002F2022\u002F4075910",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F36045974",pmc:a},{id:1480830,article_id:b,reference_num:"103",reference:"Liu&nbsp;L. Image classification in htp test based on convolutional neural network model. \u003Ci\u003EComput Intell Neurosci\u003C\u002Fi\u003E 2021;2021:6370509.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1155\u002F2021\u002F6370509",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F34659394",pmc:a},{id:1480831,article_id:b,reference_num:"104",reference:"Zheng&nbsp;D, Li&nbsp;L, Zheng&nbsp;S, et al. A defect detection method for rail surface and fasteners based on deep convolutional neural network. \u003Ci\u003EComput Intell Neurosci\u003C\u002Fi\u003E 2021;2021:2565500.",refdoi:a,pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F34381497",pmc:a},{id:1480832,article_id:b,reference_num:"105",reference:"Gao X, Wang R, Demmel N, Cremers D. LDSO: direct sparse odometry with loop closure. In: 2018 IEEE\u002FRSJ International Conference on Intelligent Robots and Systems (IROS); 2018 Oct 01-05; Madrid, Spain. IEEE; 2019. p. 2198-204.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FIROS.2018.8593376",pubmed:a,pmc:a},{id:1480833,article_id:b,reference_num:"106",reference:"Tang&nbsp;C, Zheng&nbsp;X, Tang&nbsp;C. Adaptive discriminative regions learning network for remote sensing scene classification. \u003Ci\u003ESensors\u003C\u002Fi\u003E 2023;23:1-5.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Fs23020773",pubmed:a,pmc:a},{id:1480834,article_id:b,reference_num:"107",reference:"Song&nbsp;Y, Feng&nbsp;W, Dauphin&nbsp;G, Long&nbsp;Y, Quan&nbsp;Y, Xing&nbsp;M. Ensemble alignment subspace adaptation method for cross-scene classification. \u003Ci\u003EIEEE Geosci Remote Sensing Lett\u003C\u002Fi\u003E 2023;20:1-5.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FLGRS.2023.3256348",pubmed:a,pmc:a},{id:1480835,article_id:b,reference_num:"108",reference:"Zhu&nbsp;S, Wu&nbsp;C, Du&nbsp;B, Zhang&nbsp;L. Adversarial divergence training for universal cross-scene classification. \u003Ci\u003EIEEE Trans Geosci Remote Sens\u003C\u002Fi\u003E 2023;61:1-12.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTGRS.2023.3274781",pubmed:a,pmc:a},{id:1480836,article_id:b,reference_num:"109",reference:"Ni&nbsp;J, Shen&nbsp;K, Chen&nbsp;Y, Cao&nbsp;W, Yang&nbsp;SX. An improved deep network-based scene classification method for self-driving cars. \u003Ci\u003EIEEE Trans Instrum Meas\u003C\u002Fi\u003E 2022;71:1-14.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FTIM.2022.3146923",pubmed:a,pmc:a},{id:1480837,article_id:b,reference_num:"110",reference:"Mohapatra RK, Shaswat K, Kedia S. Offline handwritten signature verification using CNN inspired by inception V1 architecture. In: 2019 Fifth International Conference on Image Information Processing (ICIIP); 2019 Nov 15-17; Shimla, India. IEEE; 2020. p. 263-7.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FICIIP47207.2019.8985925",pubmed:a,pmc:a},{id:1480838,article_id:b,reference_num:"111",reference:"McCall&nbsp;R, McGee&nbsp;F, Mirnig&nbsp;A, et al. A taxonomy of autonomous vehicle handover situations. \u003Ci\u003ETransp Res Part A Policy Pract\u003C\u002Fi\u003E 2019;124:507-22.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.tra.2018.05.005",pubmed:a,pmc:a},{id:1480839,article_id:b,reference_num:"112",reference:"Wang&nbsp;L, Guo&nbsp;S, Huang&nbsp;W, Xiong&nbsp;Y, Qiao&nbsp;Y. Knowledge guided disambiguation for large-scale scene classification with multi-resolution CNNs. \u003Ci\u003EIEEE Trans Image Process\u003C\u002Fi\u003E 2017;26:2055-68.",refdoi:a,pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F28252402",pmc:a},{id:1480840,article_id:b,reference_num:"113",reference:"Hosny&nbsp;KM, Kassem&nbsp;MA, Fouad&nbsp;MM. Classification of skin lesions into seven classes using transfer learning with AlexNet. \u003Ci\u003EJ Digit Imaging\u003C\u002Fi\u003E 2020;33:1325-34.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs10278-020-00371-9",pubmed:"http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F32607904",pmc:a},{id:1480841,article_id:b,reference_num:"114",reference:"Alhichri&nbsp;H, Alsuwayed&nbsp;A, Bazi&nbsp;Y, Ammour&nbsp;N, Alajlan&nbsp;NA. Classification of remote sensing images using EfficientNet-B3 CNN model with attention. \u003Ci\u003EIEEE Access\u003C\u002Fi\u003E 2021;9:14078-94.",refdoi:"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FACCESS.2021.3051085",pubmed:a,pmc:a}],ArtDataP:[{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure1",post_id:"Figure1",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-1.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure2",post_id:"Figure2",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-2.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure3",post_id:"Figure3",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-3.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure4",post_id:"Figure4",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-4.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure5",post_id:"Figure5",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-5.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure6",post_id:"Figure6",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-6.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure7",post_id:"Figure7",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-7.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure8",post_id:"Figure8",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-8.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure9",post_id:"Figure9",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-9.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure10",post_id:"Figure10",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-10.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure11",post_id:"Figure11",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-11.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure12",post_id:"Figure12",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-12.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure13",post_id:"Figure13",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-13.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure14",post_id:"Figure14",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-14.jpg"},{href:"\u002Farticles\u002Fir.2023.22\u002Fimage\u002FFigure15",post_id:"Figure15",image:"https:\u002F\u002Fimage.oaes.cc\u002F1fac2add-6180-4dc1-83e1-e46b3dc9e6c5\u002Fir3022-15.jpg"}],ArtDataT:[{ar_title:L,doi:M,authors:"Cyrille  Berger\u003Ca href='https:\u002F\u002Forcid.org\u002F0000-0003-3011-1505' target='_blank'\u003E\u003Cimg src='https:\u002F\u002Fi.oaes.cc\u002Fimages\u002Forcid.png' class='author_id' alt='Cyrille  Berger'\u003E\u003C\u002Fa\u003E, ... Mariusz  Wzorek\u003Ca href='https:\u002F\u002Forcid.org\u002F0000-0003-2147-2114' target='_blank'\u003E\u003Cimg src='https:\u002F\u002Fi.oaes.cc\u002Fimages\u002Forcid.png' class='author_id' alt='Mariusz  Wzorek'\u003E\u003C\u002Fa\u003E",article_id:"6671",image_list:t}],ArtNum:{view:2944,click:p,down:1438,read:p,like:9,comment:t,xml_down:bX,cite_click:34,export_click:7,cite_count:T,share_count:bW,tran_click:21,mp3_click:p,sharenum:p,videodown:p,id:b},bottomAuthors:[],articleShow:z,ArtBase:{seo:{title:y,keywords:a,description:U},picabstract:a,interview_pic:a,interview_url:a,review:a,video_url:Q,video_img:V,oaestyle:W,amastyle:X,ctstyle:Y,acstyle:Z,related:[{article_id:_,journal_id:h,section_id:m,path:e,journal:f,ar_title:$,date_published:aa,doi:ab,author:[{first_name:A,middle_name:a,last_name:B,ans:c,email:a,bio:a,photoUrl:a},{first_name:ac,middle_name:a,last_name:ad,ans:c,email:ae,bio:a,photoUrl:a},{first_name:af,middle_name:a,last_name:ag,ans:c,email:a,bio:a,photoUrl:a},{first_name:ah,middle_name:a,last_name:B,ans:c,email:a,bio:a,photoUrl:a},{first_name:ai,middle_name:a,last_name:o,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:aj,journal_id:h,section_id:m,path:e,journal:f,ar_title:ak,date_published:al,doi:am,author:[{first_name:an,middle_name:a,last_name:A,ans:c,email:a,bio:a,photoUrl:a},{first_name:ao,middle_name:a,last_name:ap,ans:j,email:a,bio:a,photoUrl:a},{first_name:aq,middle_name:a,last_name:ar,ans:as,email:a,bio:a,photoUrl:a},{first_name:at,middle_name:a,last_name:C,ans:D,email:au,bio:a,photoUrl:a}]},{article_id:av,journal_id:h,section_id:m,path:e,journal:f,ar_title:aw,date_published:ax,doi:ay,author:[{first_name:az,middle_name:a,last_name:aA,ans:c,email:a,bio:a,photoUrl:a},{first_name:aB,middle_name:a,last_name:aC,ans:c,email:a,bio:a,photoUrl:a},{first_name:aD,middle_name:a,last_name:C,ans:c,email:a,bio:a,photoUrl:a},{first_name:aE,middle_name:a,last_name:aF,ans:j,email:a,bio:a,photoUrl:a},{first_name:aG,middle_name:a,last_name:aH,ans:j,email:a,bio:a,photoUrl:a}]},{article_id:aI,journal_id:h,section_id:m,path:e,journal:f,ar_title:L,date_published:aJ,doi:M,author:[{first_name:aK,middle_name:a,last_name:aL,ans:c,email:aM,bio:a,photoUrl:a},{first_name:aN,middle_name:a,last_name:aO,ans:c,email:a,bio:a,photoUrl:a},{first_name:aP,middle_name:a,last_name:aQ,ans:c,email:a,bio:a,photoUrl:a},{first_name:aR,middle_name:a,last_name:aS,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:aT,journal_id:h,section_id:q,path:e,journal:f,ar_title:aU,date_published:aV,doi:aW,author:[{first_name:aX,middle_name:a,last_name:aY,ans:r,email:a,bio:a,photoUrl:a},{first_name:u,middle_name:a,last_name:E,ans:r,email:a,bio:a,photoUrl:a},{first_name:aZ,middle_name:a,last_name:a_,ans:r,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:r,email:F,bio:a,photoUrl:a}]},{article_id:a$,journal_id:h,section_id:m,path:e,journal:f,ar_title:ba,date_published:bb,doi:bc,author:[{first_name:bd,middle_name:a,last_name:w,ans:c,email:a,bio:a,photoUrl:a},{first_name:be,middle_name:a,last_name:bf,ans:c,email:a,bio:a,photoUrl:a},{first_name:bg,middle_name:a,last_name:bh,ans:j,email:a,bio:a,photoUrl:a}]},{article_id:bi,journal_id:h,section_id:m,path:e,journal:f,ar_title:bj,date_published:bk,doi:bl,author:[{first_name:u,middle_name:a,last_name:bm,ans:c,email:bn,bio:a,photoUrl:a},{first_name:bo,middle_name:a,last_name:x,ans:j,email:a,bio:a,photoUrl:a},{first_name:bp,middle_name:a,last_name:o,ans:j,email:a,bio:a,photoUrl:a},{first_name:bq,middle_name:a,last_name:br,ans:c,email:a,bio:a,photoUrl:a}]},{article_id:bs,journal_id:h,section_id:q,path:e,journal:f,ar_title:bt,date_published:bu,doi:bv,author:[{first_name:u,middle_name:a,last_name:E,ans:c,email:a,bio:a,photoUrl:a},{first_name:bw,middle_name:a,last_name:bx,ans:c,email:a,bio:a,photoUrl:a},{first_name:by,middle_name:a,last_name:bz,ans:D,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:c,email:F,bio:a,photoUrl:a},{first_name:bA,middle_name:a,last_name:bB,ans:bC,email:bD,bio:a,photoUrl:a}]},{article_id:bE,journal_id:h,section_id:q,path:e,journal:f,ar_title:bF,date_published:bG,doi:bH,author:[{first_name:bI,middle_name:a,last_name:w,ans:c,email:a,bio:a,photoUrl:a},{first_name:bJ,middle_name:a,last_name:bK,ans:c,email:a,bio:a,photoUrl:a},{first_name:bL,middle_name:a,last_name:x,ans:c,email:bM,bio:a,photoUrl:a},{first_name:bN,middle_name:a,last_name:bO,ans:c,email:a,bio:a,photoUrl:a},{first_name:bP,middle_name:a,last_name:w,ans:j,email:bQ,bio:a,photoUrl:a}]},{article_id:bR,journal_id:h,section_id:q,path:e,journal:f,ar_title:bS,date_published:bT,doi:bU,author:[{first_name:bV,middle_name:a,last_name:x,ans:c,email:a,bio:a,photoUrl:a},{first_name:v,middle_name:a,last_name:o,ans:j,email:a,bio:a,photoUrl:a}]}],editor:[]},keyText:{abt:"Abstract",kwd:"Keywords",ga:"Graphical Abstract",b1:"Open in new tab",b2:"Download high-res image"},ArtLanguageData:{language:N,new_title:y,new_abstract:K,new_keywords:b_,is_check:g},language:N}],fetch:{"data-v-0bee1158:0":{qKname:e,component:z,screenwidth:a}},error:s,state:{token:a,index:{data:{data:{footer:{},info:{},middle:{},nav:{},top:{}}},oaeNav:[{name:cf,sort:d,children:[{name:"Company",sort:d,url:"\u002Fabout\u002Fwho_we_are"},{name:"Latest News",sort:g,url:"\u002Fnews"},{name:O,sort:i,url:"\u002Fabout\u002Fcontact_us"},{name:"History",sort:k,url:"\u002Fabout\u002Fhistory"},{name:"Careers",sort:l,url:"\u002Fabout\u002Fjoin_us"},{name:"Policies",sort:n,children:[{name:cg,sort:d,url:"\u002Fabout\u002Feditorial_policies"},{name:"Open Access Policy",sort:g,url:"\u002Fabout\u002Fopen_access_policy"},{name:"Research and Publication Ethics",sort:i,url:"\u002Fabout\u002Fresearch_and_publication_ethics"},{name:"Peer-review Policies",sort:k,url:"\u002Fabout\u002Fpeer_review_policies"},{name:"Publication Fees",sort:l,url:"\u002Fabout\u002Fpublication_fees"},{name:"Advertising Policy",sort:n,url:"\u002Fabout\u002Fadvertising_policy"}]}]},{name:ch,sort:g,children:[{name:"All Journals",sort:d,url:"\u002Falljournals"},{name:"Active Journals",sort:g,url:"\u002Factivejournals"},{name:"Archived Journals",sort:i,url:"\u002Factivedjournals"}]},{name:"Services",sort:i,children:[{name:"Language Editing",sort:d,url:"\u002Fabout\u002Flanguage_editing_services"},{name:"Layout & Production",sort:g,url:"\u002Fabout\u002Flayout_and_production"},{name:"Graphic Abstracts",sort:i,url:"\u002Fabout\u002Fgraphic_abstracts"},{name:"Video Abstracts",sort:k,url:"\u002Fabout\u002Fvideo_abstracts"},{name:"Think Tank",url:"https:\u002F\u002Fwww.oaescience.com\u002F"},{name:"Scierxiv",url:"https:\u002F\u002Fwww.scierxiv.com\u002F"},{name:"Submission System",url:"https:\u002F\u002Fwww.oaecenter.com\u002Flogin"}]},{name:"Collaborations",sort:k,children:[{name:"Strategic Collaborators",url:"\u002Fabout\u002Fcollaborators"},{name:"Journal Membership",url:"\u002Fpartners"},{name:"Conference Parterships",url:"\u002Fabout\u002Fparterships"}]},{name:"Insights",sort:l,children:[{name:"Latest Articles",url:"\u002Farticles"},{name:"Academic Talks",url:"\u002Facademic"},{name:"Interactive Webinars",url:"\u002Fwebinars"}]},{name:"Academic Support",sort:n,children:[{name:"Author Hub",sort:d,url:"\u002Fabout\u002Fauthor"},{name:"Editor Hub",sort:g,url:"\u002Fabout\u002Feditor"},{name:"Reviewer Hub",sort:i,url:"\u002Fabout\u002Freviewer"},{name:"Conference Organizer",sort:k,url:"\u002Fabout\u002Forganizer"},{name:"Expert Lecture",sort:l,url:"\u002Fabout\u002Fexpert_lecture"}]}],oaeQkNav:[{name:ch,sort:g,children:[{name:"Biology & Life Science",sort:d,children:[{name:"Extracellular Vesicles and Circulating Nucleic Acids",sort:d,url:"\u002Fevcna"},{name:"Microbiome Research Reports",sort:g,url:"\u002Fmrr"},{name:"One Health & Implementation Research",sort:i,url:"\u002Fohir"}]},{name:"Chemistry & Materials Science",sort:g,children:[{name:"Chemical Synthesis",sort:d,url:"\u002Fcs"},{name:"Energy Materials",sort:g,url:"\u002Fenergymater"},{name:"Journal of Materials Informatics",sort:i,url:"\u002Fjmi"},{name:"Microstructures",sort:k,url:"\u002Fmicrostructures"},{name:"Minerals and Mineral Materials",sort:l,url:"\u002Fminerals"},{name:"Soft Science",sort:n,url:"\u002Fss"}]},{name:"Computer Science & Engineering",sort:i,children:[{name:"Complex Engineering Systems",sort:d,url:"\u002Fcomengsys"},{name:"Disaster Prevention and Resilience",sort:g,url:"\u002Fdpr"},{name:"Green Manufacturing Open",sort:i,url:"\u002Fgmo"},{name:f,sort:k,url:ci},{name:"Journal of Smart Environments and Green Computing",sort:l,url:"\u002Fjsegc"},{name:"Journal of Surveillance, Security and Safety",sort:n,url:"\u002Fjsss"}]},{name:"Medicine & Public Health",sort:k,children:[{name:"Ageing and Neurodegenerative Diseases",sort:d,url:"\u002Fand"},{name:"Artificial Intelligence Surgery",sort:g,url:"\u002Fais"},{name:"Cancer Drug Resistance",sort:i,url:"\u002Fcdr"},{name:"Connected Health And Telemedicine",sort:k,url:"\u002Fchatmed"},{name:"Hepatoma Research",sort:l,url:"\u002Fhr"},{name:"Journal of Cancer Metastasis and Treatment",sort:n,url:"\u002Fjcmt"},{name:"Journal of Translational Genetics and Genomics",sort:G,url:"\u002Fjtgg"},{name:"Metabolism and Target Organ Damage",sort:H,url:"\u002Fmtod"},{name:"Mini-invasive Surgery",sort:I,url:"\u002Fmis"},{name:"Plastic and Aesthetic Research",sort:J,url:"\u002Fpar"},{name:"Rare Disease and Orphan Drugs Journal",sort:b$,url:"\u002Frdodj"},{name:"The Journal of Cardiovascular Aging",sort:ca,url:"\u002Fjca"},{name:"Vessel Plus",sort:cb,url:"\u002Fvp"}]},{name:"Environmental Science",sort:l,children:[{name:"Carbon Footprints",sort:d,url:"\u002Fcf"},{name:"Journal of Environmental Exposure Assessment",sort:g,url:"\u002Fjeea"},{name:"Water Emerging Contaminants & Nanoplastics",sort:i,url:"\u002Fwecn"}]}]}],comList:{},jourtabs:{},trdId:a,timeLong:p,oldPath:a,qkactiveIndex:cj,oaeactiveIndex:cj,scopusCite:"data:image\u002Fjpeg;base64,\u002F9j\u002F4QnqRXhpZgAATU0AKgAAAAgADAEAAAMAAAABAZAAAAEBAAMAAAABAZAAAAECAAMAAAADAAAAngEGAAMAAAABAAIAAAESAAMAAAABAAEAAAEVAAMAAAABAAMAAAEaAAUAAAABAAAApAEbAAUAAAABAAAArAEoAAMAAAABAAIAAAExAAIAAAAfAAAAtAEyAAIAAAAUAAAA04dpAAQAAAABAAAA6AAAASAACAAIAAgACvyAAAAnEAAK\u002FIAAACcQQWRvYmUgUGhvdG9zaG9wIDIyLjQgKFdpbmRvd3MpADIwMjQ6MDY6MTIgMTQ6MDI6NDYAAAAEkAAABwAAAAQwMjMxoAEAAwAAAAEAAQAAoAIABAAAAAEAAABgoAMABAAAAAEAAABgAAAAAAAAAAYBAwADAAAAAQAGAAABGgAFAAAAAQAAAW4BGwAFAAAAAQAAAXYBKAADAAAAAQACAAACAQAEAAAAAQAAAX4CAgAEAAAAAQAACGQAAAAAAAAASAAAAAEAAABIAAAAAf\u002FY\u002F+0ADEFkb2JlX0NNAAH\u002F7gAOQWRvYmUAZIAAAAAB\u002F9sAhAAMCAgICQgMCQkMEQsKCxEVDwwMDxUYExMVExMYEQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMAQ0LCw0ODRAODhAUDg4OFBQODg4OFBEMDAwMDBERDAwMDAwMEQwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAz\u002FwAARCABgAGADASIAAhEBAxEB\u002F90ABAAG\u002F8QBPwAAAQUBAQEBAQEAAAAAAAAAAwABAgQFBgcICQoLAQABBQEBAQEBAQAAAAAAAAABAAIDBAUGBwgJCgsQAAEEAQMCBAIFBwYIBQMMMwEAAhEDBCESMQVBUWETInGBMgYUkaGxQiMkFVLBYjM0coLRQwclklPw4fFjczUWorKDJkSTVGRFwqN0NhfSVeJl8rOEw9N14\u002FNGJ5SkhbSVxNTk9KW1xdXl9VZmdoaWprbG1ub2N0dXZ3eHl6e3x9fn9xEAAgIBAgQEAwQFBgcHBgU1AQACEQMhMRIEQVFhcSITBTKBkRShsUIjwVLR8DMkYuFygpJDUxVjczTxJQYWorKDByY1wtJEk1SjF2RFVTZ0ZeLys4TD03Xj80aUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9ic3R1dnd4eXp7fH\u002F9oADAMBAAIRAxEAPwCwkkkuYepUkkkkpSSSSSlJJJJKUkkkkpSSSSSn\u002F9Cwkkr3RumnqWc2gkilo33OHO0fmt\u002FlWOXMwhKchGIuUjQennOMImUjUYiygxMHMzXbcWl1saFw0aP61joYtFv1U6wRJFTT4F5\u002FgxdZbbhdMw9z9tGNSAGgD\u002FNa1o+k5yxnfXPFDoZjWuZ2cS0H\u002FN3LRPJ8tiAGbKeM9v4epzhznNZbODEOAdT\u002FAN96YuHk9B6vitL7McvYOXVHfHxa33\u002F9FUF2bPrZ0p9L37nV2NaS2p7SC4x9FpbuauNc99jnWWGXvJc8+bjucqvNYsMOE4Z8fFdjfhbXK5c8+IZsfAY1W44lk7Wue8MY0veeGtBJP9lqs9N6df1LKGPT7QPdbYRIY3x\u002FrO\u002FMau3wOnYXTKC2hoYAJstd9J0fnWPTuW5Oeb1XwQH6X\u002Feo5rnIYPTXHkP6I\u002F7p5Gj6t9ZuaHegKgf9K4NP+aN7kV31T6wBIFTvIPP8WLbyfrZ0qkltRfkEd6x7f89+1qAz654ZdD8a1rfEbT+G5WPY5GOhyknvf\u002Fexa\u002Fv8\u002FL1DCAOxH\u002FfSedy+l9QwtcmhzG\u002Fvj3M\u002Fz2T\u002FANJVV03XPrDhZnS3UYjybLnBr2FpaQwHc+Z\u002FejauZVPmceOE6xT441d7\u002FT0tvlsmWcLyw9uV1W3+FUn\u002F0bC6L6mOZ9oy2n6ZYwj4Avlc6rGBnXdPy2ZVOpbo5h0Dmn6TCud5bIMeaEztE6\u002FX0vR8zjOXDOA3kNPMep6j63499vT67KgXMos32tGvthzd\u002FwDYlcfzwvQ+n9SxOo0C3HfP79Z0c0\u002FuvaqOd9V+mZW59TTi2nXdX9Gf5VR9i0Oa5Q5z72KQlxDa9\u002F7snP5TnBgj7OaJjwk61tf70XikiQBJ4Cv9T6Lm9MO64CygmG3s+j8LG\u002F4NUANzmtPDnAH5mFmThKEuGQMZDoXUhOM4iUSJRPUPc\u002FVzp4wumsLh+myALbT31HsZ\u002FYYsT61dVfdkHp1Toopj1o\u002FPefdsd\u002FIr\u002FwCrXXAAAAaAaALza+x1uRda4y6yx7ifi4rT56Xs4IYYaCWh\u002Fuw3\u002FwAZy+QHvZ8maepjqP709v8AFYJJJLKdZSSSSSn\u002F0rCSSkyu2wkVsdYWjc4NBdAH5x2rmKvZ6hZj31vFlbiyxv0XtJBH9pq2+nfWvNoc1maPtNPBeABYB46e2xYYIPGqRIAk6BSYs2TEbhIx\u002FwCifMLMuHHlFTiJf9If3ZPo9dmPmYwewi2i9sjuHNPiCuD6piN6f1K3Hb\u002FN1Pa9k\u002FuGLG\u002F5v0V2H1dx7sfpFFdwLXkF+w8tDnF7W\u002F5rlzH1oe1\u002FWrgNQxjGH4xu\u002FwC\u002FrR5\u002F1cviySHDO4\u002F86PFKLm\u002FD\u002FRzOXHE8WMCX\u002FMlwxk9w1wc0OaZBEg+RXnGXS7HzL6Hc12OHync3\u002Foldj9WeojM6c2px\u002FT4oFbx3IH81Z\u002FaaqX1n6Jbe\u002FwDaGIwvfEX1N+k4D6NjP3nNTuch94wQy49eH1UP3ZfN\u002FireSn935ieLJ6eL02f3o\u002FJ\u002FjPLJJT945Hgksh2FJJgQdAU6Sn\u002F\u002F07C6L6oX4dLsgW2tZkWloY12hLWj81x+l73LnUiAdDqucwZTiyRyACXD0Pi9JnxDLjljJMeLqPB9AyOjdKynepdjVveeXgQT\u002FaZtUaOhdIx7RbVisFjdWuMug+W8uXC1ZOVT\u002FM3WV+TXuA+6VK3MzLhttyLbB4Oe4j8qu\u002Ff8N8RwDj7+n\u002FpcLS+4Zq4RzB4Nq9X\u002FAEeJ7Lq\u002FX8Xp9ZYwi7KI9lQMgH960j6DVxNllltj7bXb7LCXPce5KiABxokqvM81PORxemI+WIbXLcrDBEiPqkfmkU+Fm5GBktycd0PboQfoub3Y\u002FwDkrs+mfWDA6g0N3Ci\u002F86mwgH\u002Frbvo2LhUiAedUeX5vJg0Hqgd4n9iOZ5THn1PpmNpj9v7z6Dk9J6ZmO35GPXY4\u002FnxBP9psOQW\u002FVzojTIxGH4lx\u002FwCqcuJqysukRTfbWPBr3AfdKk\u002FPz7BFmVc4eBsd\u002FerJ5\u002FAdZcuDL\u002FBP\u002FO4WsOQzjSPMER\u002Fwh\u002FzeJ6j6y4+BX0h1bPTpfW5r6WNhpJB9zWtH7zFyCUaydT4nUpKpzGYZp8QiIacNDwbfL4ThhwGZnqZWfF\u002F\u002F1LCSSS5h6lSSSSSlJJJJKUkkkkpSSSSSlJJJJKf\u002F2f\u002FtEX5QaG90b3Nob3AgMy4wADhCSU0EBAAAAAAABxwCAAACAAAAOEJJTQQlAAAAAAAQ6PFc8y\u002FBGKGie2etxWTVujhCSU0EOgAAAAAA1wAAABAAAAABAAAAAAALcHJpbnRPdXRwdXQAAAAFAAAAAFBzdFNib29sAQAAAABJbnRlZW51bQAAAABJbnRlAAAAAEltZyAAAAAPcHJpbnRTaXh0ZWVuQml0Ym9vbAAAAAALcHJpbnRlck5hbWVURVhUAAAAAQAAAAAAD3ByaW50UHJvb2ZTZXR1cE9iamMAAAAFaCFoN4u+f24AAAAAAApwcm9vZlNldHVwAAAAAQAAAABCbHRuZW51bQAAAAxidWlsdGluUHJvb2YAAAAJcHJvb2ZDTVlLADhCSU0EOwAAAAACLQAAABAAAAABAAAAAAAScHJpbnRPdXRwdXRPcHRpb25zAAAAFwAAAABDcHRuYm9vbAAAAAAAQ2xicmJvb2wAAAAAAFJnc01ib29sAAAAAABDcm5DYm9vbAAAAAAAQ250Q2Jvb2wAAAAAAExibHNib29sAAAAAABOZ3R2Ym9vbAAAAAAARW1sRGJvb2wAAAAAAEludHJib29sAAAAAABCY2tnT2JqYwAAAAEAAAAAAABSR0JDAAAAAwAAAABSZCAgZG91YkBv4AAAAAAAAAAAAEdybiBkb3ViQG\u002FgAAAAAAAAAAAAQmwgIGRvdWJAb+AAAAAAAAAAAABCcmRUVW50RiNSbHQAAAAAAAAAAAAAAABCbGQgVW50RiNSbHQAAAAAAAAAAAAAAABSc2x0VW50RiNQeGxAUgAAAAAAAAAAAAp2ZWN0b3JEYXRhYm9vbAEAAAAAUGdQc2VudW0AAAAAUGdQcwAAAABQZ1BDAAAAAExlZnRVbnRGI1JsdAAAAAAAAAAAAAAAAFRvcCBVbnRGI1JsdAAAAAAAAAAAAAAAAFNjbCBVbnRGI1ByY0BZAAAAAAAAAAAAEGNyb3BXaGVuUHJpbnRpbmdib29sAAAAAA5jcm9wUmVjdEJvdHRvbWxvbmcAAAAAAAAADGNyb3BSZWN0TGVmdGxvbmcAAAAAAAAADWNyb3BSZWN0UmlnaHRsb25nAAAAAAAAAAtjcm9wUmVjdFRvcGxvbmcAAAAAADhCSU0D7QAAAAAAEABIAAAAAQACAEgAAAABAAI4QklNBCYAAAAAAA4AAAAAAAAAAAAAP4AAADhCSU0D8gAAAAAACgAA\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FAAA4QklNBA0AAAAAAAQAAAB4OEJJTQQZAAAAAAAEAAAAHjhCSU0D8wAAAAAACQAAAAAAAAAAAQA4QklNJxAAAAAAAAoAAQAAAAAAAAACOEJJTQP1AAAAAABIAC9mZgABAGxmZgAGAAAAAAABAC9mZgABAKGZmgAGAAAAAAABADIAAAABAFoAAAAGAAAAAAABADUAAAABAC0AAAAGAAAAAAABOEJJTQP4AAAAAABwAAD\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FA+gAAAAA\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FwPoAAAAAP\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F8D6AAAAAD\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FA+gAADhCSU0ECAAAAAAAEAAAAAEAAAJAAAACQAAAAAA4QklNBB4AAAAAAAQAAAAAOEJJTQQaAAAAAANTAAAABgAAAAAAAAAAAAAAYAAAAGAAAAAPADIAMAAyADQAMAA2ADEAMgAtADEANAAwADEANQA1AAAAAQAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAABgAAAAYAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAABAAAAABAAAAAAAAbnVsbAAAAAIAAAAGYm91bmRzT2JqYwAAAAEAAAAAAABSY3QxAAAABAAAAABUb3AgbG9uZwAAAAAAAAAATGVmdGxvbmcAAAAAAAAAAEJ0b21sb25nAAAAYAAAAABSZ2h0bG9uZwAAAGAAAAAGc2xpY2VzVmxMcwAAAAFPYmpjAAAAAQAAAAAABXNsaWNlAAAAEgAAAAdzbGljZUlEbG9uZwAAAAAAAAAHZ3JvdXBJRGxvbmcAAAAAAAAABm9yaWdpbmVudW0AAAAMRVNsaWNlT3JpZ2luAAAADWF1dG9HZW5lcmF0ZWQAAAAAVHlwZWVudW0AAAAKRVNsaWNlVHlwZQAAAABJbWcgAAAABmJvdW5kc09iamMAAAABAAAAAAAAUmN0MQAAAAQAAAAAVG9wIGxvbmcAAAAAAAAAAExlZnRsb25nAAAAAAAAAABCdG9tbG9uZwAAAGAAAAAAUmdodGxvbmcAAABgAAAAA3VybFRFWFQAAAABAAAAAAAAbnVsbFRFWFQAAAABAAAAAAAATXNnZVRFWFQAAAABAAAAAAAGYWx0VGFnVEVYVAAAAAEAAAAAAA5jZWxsVGV4dElzSFRNTGJvb2wBAAAACGNlbGxUZXh0VEVYVAAAAAEAAAAAAAlob3J6QWxpZ25lbnVtAAAAD0VTbGljZUhvcnpBbGlnbgAAAAdkZWZhdWx0AAAACXZlcnRBbGlnbmVudW0AAAAPRVNsaWNlVmVydEFsaWduAAAAB2RlZmF1bHQAAAALYmdDb2xvclR5cGVlbnVtAAAAEUVTbGljZUJHQ29sb3JUeXBlAAAAAE5vbmUAAAAJdG9wT3V0c2V0bG9uZwAAAAAAAAAKbGVmdE91dHNldGxvbmcAAAAAAAAADGJvdHRvbU91dHNldGxvbmcAAAAAAAAAC3JpZ2h0T3V0c2V0bG9uZwAAAAAAOEJJTQQoAAAAAAAMAAAAAj\u002FwAAAAAAAAOEJJTQQUAAAAAAAEAAAAAzhCSU0EDAAAAAAIgAAAAAEAAABgAAAAYAAAASAAAGwAAAAIZAAYAAH\u002F2P\u002FtAAxBZG9iZV9DTQAB\u002F+4ADkFkb2JlAGSAAAAAAf\u002FbAIQADAgICAkIDAkJDBELCgsRFQ8MDA8VGBMTFRMTGBEMDAwMDAwRDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAENCwsNDg0QDg4QFA4ODhQUDg4ODhQRDAwMDAwREQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwM\u002F8AAEQgAYABgAwEiAAIRAQMRAf\u002FdAAQABv\u002FEAT8AAAEFAQEBAQEBAAAAAAAAAAMAAQIEBQYHCAkKCwEAAQUBAQEBAQEAAAAAAAAAAQACAwQFBgcICQoLEAABBAEDAgQCBQcGCAUDDDMBAAIRAwQhEjEFQVFhEyJxgTIGFJGhsUIjJBVSwWIzNHKC0UMHJZJT8OHxY3M1FqKygyZEk1RkRcKjdDYX0lXiZfKzhMPTdePzRieUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9jdHV2d3h5ent8fX5\u002FcRAAICAQIEBAMEBQYHBwYFNQEAAhEDITESBEFRYXEiEwUygZEUobFCI8FS0fAzJGLhcoKSQ1MVY3M08SUGFqKygwcmNcLSRJNUoxdkRVU2dGXi8rOEw9N14\u002FNGlKSFtJXE1OT0pbXF1eX1VmZ2hpamtsbW5vYnN0dXZ3eHl6e3x\u002F\u002FaAAwDAQACEQMRAD8AsJJJLmHqVJJJJKUkkkkpSSSSSlJJJJKUkkkkp\u002F\u002FQsJJK90bpp6lnNoJIpaN9zhztH5rf5VjlzMISnIRiLlI0Hp5zjCJlI1GIsoMTBzM123FpdbGhcNGj+tY6GLRb9VOsESRU0+Bef4MXWW24XTMPc\u002FbRjUgBoA\u002FzWtaPpOcsZ31zxQ6GY1rmdnEtB\u002Fzdy0TyfLYgBmynjPb+Hqc4c5zWWzgxDgHU\u002FwDfemLh5PQer4rS+zHL2Dl1R3x8Wt9\u002F\u002FRVBdmz62dKfS9+51djWktqe0guMfRaW7mrjXPfY51lhl7yXPPm47nKrzWLDDhOGfHxXY34W1yuXPPiGbHwGNVuOJZO1rnvDGNL3nhrQST\u002FZarPTenX9Syhj0+0D3W2ESGN8f6zvzGrt8Dp2F0ygtoaGACbLXfSdH51j07luTnm9V8EB+l\u002F3qOa5yGD01x5D+iP+6eRo+rfWbmh3oCoH\u002FSuDT\u002Fmje5Fd9U+sASBU7yDz\u002FFi28n62dKpJbUX5BHese3\u002FPftagM+ueGXQ\u002FGta3xG0\u002FhuVj2ORjocpJ73\u002F3sWv7\u002FPy9QwgDsR\u002F30nncvpfUMLXJocxv749zP89k\u002FwDSVVdN1z6w4WZ0t1GI8my5wa9haWkMB3Pmf3o2rmVT5nHjhOsU+ONXe\u002F09Lb5bJlnC8sPbldVt\u002FhVJ\u002F9Gwui+pjmfaMtp+mWMI+AL5XOqxgZ13T8tmVTqW6OYdA5p+kwrneWyDHmhM7ROv19L0fM4zlwzgN5DTzHqeo+t+Pfb0+uyoFzKLN9rRr7Yc3f8A2JXH88L0Pp\u002FUsTqNAtx3z+\u002FWdHNP7r2qjnfVfpmVufU04tp13V\u002FRn+VUfYtDmuUOc+9ikJcQ2vf+7Jz+U5wYI+zmiY8JOtbX+9F4pIkASeAr\u002FU+i5vTDuuAsoJht7Po\u002FCxv+DVADc5rTw5wB+ZhZk4ShLhkDGQ6F1ITjOIlEiUT1D3P1c6eMLprC4fpsgC2099R7Gf2GLE+tXVX3ZB6dU6KKY9aPz3n3bHfyK\u002F8Aq11wAAAGgGgC82vsdbkXWuMusse4n4uK0+el7OCGGGglof7sN\u002F8AGcvkB72fJmnqY6j+9Pb\u002FABWCSSSynWUkkkkp\u002F9KwkkpMrtsJFbHWFo3ODQXQB+cdq5ir2eoWY99bxZW4ssb9F7SQR\u002Faatvp31rzaHNZmj7TTwXgAWAeOntsWGCDxqkSAJOgUmLNkxG4SMf8AonzCzLhx5RU4iX\u002FSH92T6PXZj5mMHsItovbI7hzT4grg+qYjen9Stx2\u002FzdT2vZP7hixv+b9Fdh9Xce7H6RRXcC15BfsPLQ5xe1v+a5cx9aHtf1q4DUMYxh+Mbv8Av60ef9XL4skhwzuP\u002FOjxSi5vw\u002F0czlxxPFjAl\u002FzJcMZPcNcHNDmmQRIPkV5xl0ux8y+h3Ndjh8p3N\u002F6JXY\u002FVnqIzOnNqcf0+KBW8dyB\u002FNWf2mql9Z+iW3v8A2hiML3xF9TfpOA+jYz95zU7nIfeMEMuPXh9VD92Xzf4q3kp\u002Fd+Yniyeni9Nn96Pyf4zyySU\u002FeOR4JLIdhSSYEHQFOkp\u002F\u002F9Owui+qF+HS7IFtrWZFpaGNdoS1o\u002FNcfpe9y51IgHQ6rnMGU4skcgAlw9D4vSZ8Qy45YyTHi6jwfQMjo3Ssp3qXY1b3nl4EE\u002F2mbVGjoXSMe0W1YrBY3VrjLoPlvLlwtWTlU\u002FzN1lfk17gPulStzMy4bbci2weDnuI\u002FKrv3\u002FDfEcA4+\u002Fp\u002F6XC0vuGauEcweDavV\u002FwBHiey6v1\u002FF6fWWMIuyiPZUDIB\u002FetI+g1cTZZZbY+212+ywlz3HuSogAcaJKrzPNTzkcXpiPliG1y3KwwRIj6pH5pFPhZuRgZLcnHdD26EH6Lm92P8A5K7Ppn1gwOoNDdwov\u002FOpsIB\u002F6276Ni4VIgHnVHl+byYNB6oHeJ\u002FYjmeUx59T6ZjaY\u002Fb+8+g5PSemZjt+Rj12OP58QT\u002FabDkFv1c6I0yMRh+Jcf8AqnLiasrLpEU321jwa9wH3SpPz8+wRZlXOHgbHf3qyefwHWXLgy\u002FwT\u002FzuFrDkM40jzBEf8If83ieo+suPgV9IdWz06X1ua+ljYaSQfc1rR+8xcglGsnU+J1KSqcxmGafEIiGnDQ8G3y+E4YcBmZ6mVnxf\u002F9SwkkkuYepUkkkkpSSSSSlJJJJKUkkkkpSSSSSn\u002F9k4QklNBCEAAAAAAFcAAAABAQAAAA8AQQBkAG8AYgBlACAAUABoAG8AdABvAHMAaABvAHAAAAAUAEEAZABvAGIAZQAgAFAAaABvAHQAbwBzAGgAbwBwACAAMgAwADIAMQAAAAEAOEJJTQQGAAAAAAAHAAgAAAABAQD\u002F4Q7maHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLwA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI\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\u002FeHBhY2tldCBlbmQ9InciPz7\u002F4gxYSUNDX1BST0ZJTEUAAQEAAAxITGlubwIQAABtbnRyUkdCIFhZWiAHzgACAAkABgAxAABhY3NwTVNGVAAAAABJRUMgc1JHQgAAAAAAAAAAAAAAAAAA9tYAAQAAAADTLUhQICAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABFjcHJ0AAABUAAAADNkZXNjAAABhAAAAGx3dHB0AAAB8AAAABRia3B0AAACBAAAABRyWFlaAAACGAAAABRnWFlaAAACLAAAABRiWFlaAAACQAAAABRkbW5kAAACVAAAAHBkbWRkAAACxAAAAIh2dWVkAAADTAAAAIZ2aWV3AAAD1AAAACRsdW1pAAAD+AAAABRtZWFzAAAEDAAAACR0ZWNoAAAEMAAAAAxyVFJDAAAEPAAACAxnVFJDAAAEPAAACAxiVFJDAAAEPAAACAx0ZXh0AAAAAENvcHlyaWdodCAoYykgMTk5OCBIZXdsZXR0LVBhY2thcmQgQ29tcGFueQAAZGVzYwAAAAAAAAASc1JHQiBJRUM2MTk2Ni0yLjEAAAAAAAAAAAAAABJzUkdCIElFQzYxOTY2LTIuMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWFlaIAAAAAAAAPNRAAEAAAABFsxYWVogAAAAAAAAAAAAAAAAAAAAAFhZWiAAAAAAAABvogAAOPUAAAOQWFlaIAAAAAAAAGKZAAC3hQAAGNpYWVogAAAAAAAAJKAAAA+EAAC2z2Rlc2MAAAAAAAAAFklFQyBodHRwOi8vd3d3LmllYy5jaAAAAAAAAAAAAAAAFklFQyBodHRwOi8vd3d3LmllYy5jaAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkZXNjAAAAAAAAAC5JRUMgNjE5NjYtMi4xIERlZmF1bHQgUkdCIGNvbG91ciBzcGFjZSAtIHNSR0IAAAAAAAAAAAAAAC5JRUMgNjE5NjYtMi4xIERlZmF1bHQgUkdCIGNvbG91ciBzcGFjZSAtIHNSR0IAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZGVzYwAAAAAAAAAsUmVmZXJlbmNlIFZpZXdpbmcgQ29uZGl0aW9uIGluIElFQzYxOTY2LTIuMQAAAAAAAAAAAAAALFJlZmVyZW5jZSBWaWV3aW5nIENvbmRpdGlvbiBpbiBJRUM2MTk2Ni0yLjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHZpZXcAAAAAABOk\u002FgAUXy4AEM8UAAPtzAAEEwsAA1yeAAAAAVhZWiAAAAAAAEwJVgBQAAAAVx\u002FnbWVhcwAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAo8AAAACc2lnIAAAAABDUlQgY3VydgAAAAAAAAQAAAAABQAKAA8AFAAZAB4AIwAoAC0AMgA3ADsAQABFAEoATwBUAFkAXgBjAGgAbQByAHcAfACBAIYAiwCQAJUAmgCfAKQAqQCuALIAtwC8AMEAxgDLANAA1QDbAOAA5QDrAPAA9gD7AQEBBwENARMBGQEfASUBKwEyATgBPgFFAUwBUgFZAWABZwFuAXUBfAGDAYsBkgGaAaEBqQGxAbkBwQHJAdEB2QHhAekB8gH6AgMCDAIUAh0CJgIvAjgCQQJLAlQCXQJnAnECegKEAo4CmAKiAqwCtgLBAssC1QLgAusC9QMAAwsDFgMhAy0DOANDA08DWgNmA3IDfgOKA5YDogOuA7oDxwPTA+AD7AP5BAYEEwQgBC0EOwRIBFUEYwRxBH4EjASaBKgEtgTEBNME4QTwBP4FDQUcBSsFOgVJBVgFZwV3BYYFlgWmBbUFxQXVBeUF9gYGBhYGJwY3BkgGWQZqBnsGjAadBq8GwAbRBuMG9QcHBxkHKwc9B08HYQd0B4YHmQesB78H0gflB\u002FgICwgfCDIIRghaCG4IggiWCKoIvgjSCOcI+wkQCSUJOglPCWQJeQmPCaQJugnPCeUJ+woRCicKPQpUCmoKgQqYCq4KxQrcCvMLCwsiCzkLUQtpC4ALmAuwC8gL4Qv5DBIMKgxDDFwMdQyODKcMwAzZDPMNDQ0mDUANWg10DY4NqQ3DDd4N+A4TDi4OSQ5kDn8Omw62DtIO7g8JDyUPQQ9eD3oPlg+zD88P7BAJECYQQxBhEH4QmxC5ENcQ9RETETERTxFtEYwRqhHJEegSBxImEkUSZBKEEqMSwxLjEwMTIxNDE2MTgxOkE8UT5RQGFCcUSRRqFIsUrRTOFPAVEhU0FVYVeBWbFb0V4BYDFiYWSRZsFo8WshbWFvoXHRdBF2UXiReuF9IX9xgbGEAYZRiKGK8Y1Rj6GSAZRRlrGZEZtxndGgQaKhpRGncanhrFGuwbFBs7G2MbihuyG9ocAhwqHFIcexyjHMwc9R0eHUcdcB2ZHcMd7B4WHkAeah6UHr4e6R8THz4faR+UH78f6iAVIEEgbCCYIMQg8CEcIUghdSGhIc4h+yInIlUigiKvIt0jCiM4I2YjlCPCI\u002FAkHyRNJHwkqyTaJQklOCVoJZclxyX3JicmVyaHJrcm6CcYJ0kneierJ9woDSg\u002FKHEooijUKQYpOClrKZ0p0CoCKjUqaCqbKs8rAis2K2krnSvRLAUsOSxuLKIs1y0MLUEtdi2rLeEuFi5MLoIuty7uLyQvWi+RL8cv\u002FjA1MGwwpDDbMRIxSjGCMbox8jIqMmMymzLUMw0zRjN\u002FM7gz8TQrNGU0njTYNRM1TTWHNcI1\u002FTY3NnI2rjbpNyQ3YDecN9c4FDhQOIw4yDkFOUI5fzm8Ofk6Njp0OrI67zstO2s7qjvoPCc8ZTykPOM9Ij1hPaE94D4gPmA+oD7gPyE\u002FYT+iP+JAI0BkQKZA50EpQWpBrEHuQjBCckK1QvdDOkN9Q8BEA0RHRIpEzkUSRVVFmkXeRiJGZ0arRvBHNUd7R8BIBUhLSJFI10kdSWNJqUnwSjdKfUrESwxLU0uaS+JMKkxyTLpNAk1KTZNN3E4lTm5Ot08AT0lPk0\u002FdUCdQcVC7UQZRUFGbUeZSMVJ8UsdTE1NfU6pT9lRCVI9U21UoVXVVwlYPVlxWqVb3V0RXklfgWC9YfVjLWRpZaVm4WgdaVlqmWvVbRVuVW+VcNVyGXNZdJ114XcleGl5sXr1fD19hX7NgBWBXYKpg\u002FGFPYaJh9WJJYpxi8GNDY5dj62RAZJRk6WU9ZZJl52Y9ZpJm6Gc9Z5Nn6Wg\u002FaJZo7GlDaZpp8WpIap9q92tPa6dr\u002F2xXbK9tCG1gbbluEm5rbsRvHm94b9FwK3CGcOBxOnGVcfByS3KmcwFzXXO4dBR0cHTMdSh1hXXhdj52m3b4d1Z3s3gReG54zHkqeYl553pGeqV7BHtje8J8IXyBfOF9QX2hfgF+Yn7CfyN\u002FhH\u002FlgEeAqIEKgWuBzYIwgpKC9INXg7qEHYSAhOOFR4Wrhg6GcobXhzuHn4gEiGmIzokziZmJ\u002FopkisqLMIuWi\u002FyMY4zKjTGNmI3\u002FjmaOzo82j56QBpBukNaRP5GokhGSepLjk02TtpQglIqU9JVflcmWNJaflwqXdZfgmEyYuJkkmZCZ\u002FJpomtWbQpuvnByciZz3nWSd0p5Anq6fHZ+Ln\u002FqgaaDYoUehtqImopajBqN2o+akVqTHpTilqaYapoum\u002Fadup+CoUqjEqTepqaocqo+rAqt1q+msXKzQrUStuK4trqGvFq+LsACwdbDqsWCx1rJLssKzOLOutCW0nLUTtYq2AbZ5tvC3aLfguFm40blKucK6O7q1uy67p7whvJu9Fb2Pvgq+hL7\u002Fv3q\u002F9cBwwOzBZ8Hjwl\u002FC28NYw9TEUcTOxUvFyMZGxsPHQce\u002FyD3IvMk6ybnKOMq3yzbLtsw1zLXNNc21zjbOts83z7jQOdC60TzRvtI\u002F0sHTRNPG1EnUy9VO1dHWVdbY11zX4Nhk2OjZbNnx2nba+9uA3AXcit0Q3ZbeHN6i3ynfr+A24L3hROHM4lPi2+Nj4+vkc+T85YTmDeaW5x\u002Fnqegy6LzpRunQ6lvq5etw6\u002Fvshu0R7ZzuKO6070DvzPBY8OXxcvH\u002F8ozzGfOn9DT0wvVQ9d72bfb794r4Gfio+Tj5x\u002FpX+uf7d\u002FwH\u002FJj9Kf26\u002Fkv+3P9t\u002F\u002F\u002F\u002F7gAOQWRvYmUAZEAAAAAB\u002F9sAhAABAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAgICAgICAgICAgIDAwMDAwMDAwMDAQEBAQEBAQEBAQECAgECAgMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwP\u002FwAARCABgAGADAREAAhEBAxEB\u002F90ABAAM\u002F8QBogAAAAYCAwEAAAAAAAAAAAAABwgGBQQJAwoCAQALAQAABgMBAQEAAAAAAAAAAAAGBQQDBwIIAQkACgsQAAIBAwQBAwMCAwMDAgYJdQECAwQRBRIGIQcTIgAIMRRBMiMVCVFCFmEkMxdScYEYYpElQ6Gx8CY0cgoZwdE1J+FTNoLxkqJEVHNFRjdHYyhVVlcassLS4vJkg3SThGWjs8PT4yk4ZvN1Kjk6SElKWFlaZ2hpanZ3eHl6hYaHiImKlJWWl5iZmqSlpqeoqaq0tba3uLm6xMXGx8jJytTV1tfY2drk5ebn6Onq9PX29\u002Fj5+hEAAgEDAgQEAwUEBAQGBgVtAQIDEQQhEgUxBgAiE0FRBzJhFHEIQoEjkRVSoWIWMwmxJMHRQ3LwF+GCNCWSUxhjRPGisiY1GVQ2RWQnCnODk0Z0wtLi8lVldVY3hIWjs8PT4\u002FMpGpSktMTU5PSVpbXF1eX1KEdXZjh2hpamtsbW5vZnd4eXp7fH1+f3SFhoeIiYqLjI2Oj4OUlZaXmJmam5ydnp+So6SlpqeoqaqrrK2ur6\u002F9oADAMBAAIRAxEAPwAYvfADr6Eeve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r\u002F0Bi98AOvoR697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuv\u002FRGL3wA6+hHr3v3XuhT6q6O7j7yyMuM6i633NvhqadaavytDTRUG18TO5KrHld2ZibH7fpJAVOqMVDzrpP7fHsY8n+3nPPuBcPb8mcsXV\u002FpbS0igJAhxh7iQpEp86ay2Diop0EOcfcHkb29tUu+duarTbw6lkjdi9xIB5x20QeZhwo2gIaju6OVR\u002FypPmFU0i1FRQ9T46dl1Ggquwa+apQ2voeWh2lVUev\u002FgsjLf8APucIvuee9MkayNFs8bEfC12+ofI6Ld1\u002FYxHUFT\u002FfI9j4pjFHcb1LHX41s0Cn50e5Vv2qD8ugX7M+CXy06moJsxubqDI5zB00c09VmOuclR79goqanXXNU12MxQi3JBAieouKFlABJIsfYH5q+7v7xcnwPebjydJc2CgkyWbrdBQOJZI\u002F1lUDJYx0oDU46HnKf3ifZfnS5jsdp53it9wcgLFeo1mXY4CpJJWAknFPGBrSnHopRBBIZXjYcNHLG8UsbD6pLFKqSxSKeGVgGU8EA+4WIIJDKQw4gggg+hByCPMHI8+powcggj1BBB+YIwR6EEg+XXvfuvdS8ZjsnnMpR4LA4nL7gzuQdI6DB7fxVfm8xWPI\u002FjT7fGYunq62RWk9OrRoB+pHtRZ2l5uV5Dt222U1zuEnwRQo8sjf6VEDMfThxx0zd3Vpt9nPuO5XsFtt0QJeWaRIolpk1kkZUBpmla\u002FLo6uyf5b\u002FAMyt8UdPko+rsdsyhqRdD2Ju7E7eySrceqbB45c\u002Fl6Xg30zRRP8A4X49zzsP3XPe3foVuf6rRWMDDH1dxHE\u002F+2iTxZV\u002F2yg\u002FLqBN\u002FwDvTexfL88tq3N8t\u002FcJx+itpJk\u002FKV\u002FBjb7VZh8+l\u002FX\u002FAMqL5g0dO01LSdS5aVRcUlH2Bkaedz\u002FqUkyO0aSm1f8ABnUf4+xFN9zz3ohQuibPKafCl24b7Brt0X9rDoN2\u002FwB8n2QnlEcs29QofxNZowH2hLlm\u002FYD0VPtj4x\u002FIPoxfuO1OqNzbcxHkWH+9NEtJubZ\u002Fle5RJdybcqMlQY8uAdP3n217WHPuH+cfaX3K5AUzc28n3VtZVp46aZ7eppSs0JdErXAk0HBxjqY+TPdr209wj4XJ\u002FOVpdXtK\u002FTvqguaDiRBOqO9PPwvEp0BnuPOpC6\u002F\u002F0hi98AOvoR6NH8OfjfP8pO8MT17V1FVQbIwmPk3h2blKCYU9fFtOiqoaSDB4yo1K9Pld2ZWeOjSZLyU1KKidQXiX3MPsZ7Xv7sc\u002FWewXDMmwW8ZuL11NG8BWCiJD\u002FHPIVjqDVE1yAHRQxD75e6cftB7fXvM0MSScwXEotrCNxqQ3DqWMsg4GO2jBkKnDyeFGTpc9bRO692dMfFHp4ZbMvhOtOqev8dQ4nG4\u002FGUDrTUyyzJSYvCYPD46GatymWydbMFjhhjkqKiZ2drku\u002FvrRu27cl+0\u002FJxvL1oNs5T2+JUVUU0UVosccaAs7ux4AFmJZ2PxN1yJ2bZue\u002FeTng2Vitxu3OW5SvI7yONTUBaSWWVyEjjjUVZmKoigKKdq9VpZL+c31ZDlJIcP0f2rk8KspWHK1eR2ZiK2eC9hP\u002FBps1UyQFl58ckyuPobHj3ixc\u002Ffg5NS6ZLPkjdZbIMQHZ7eNyK8fD8RqV4gFwfWnWVtr9xLnB7RHvvcDZ4b8jMapdSqD6eKIlB+1VI9KjoZcT\u002FNc+K+a2ZuTOHLbn2puzDYOtrcXsbeW2MjS5PcmYio5ZKLC4PI4T+N4PISVleqQ6lqk0B\u002FI4RAzKNrP73\u002FtFf7Fue5fW3Vpu0FuzpaXMDrJNIFOmKNovFiYs9Er4goGDGi1IAt79zb3hsN92rbxZWl5s09wqSXdrOjRwRFgHllSXwpUCpVqGM1I0rqYgHWlr8rl9wZLK7i3DUNV7h3JlspuPcFU7+RqjOZ+vqMtl5vJc6lfIVkmn\u002Fabe+V95fXm6Xt7um4ymTcbqaSaVjxaSVzI5PHizHzP29dXLazstttbPa9siCbZawxwQqBTTFCixxin+kUV+dehw+Nnx13x8oO0KHrbZbriqSCnTM733pVUjVmL2NtYTmBslNT64lyOcyk6NT4ug1p91UBnkZKeGeRZB9qvbDfvdjmy35a2b9K1VfEurplLR20INCxGA8rnthiqC7VLFY1d1j33V90OXvaPlG45q35TNMzGK0tVbTJd3FKhA1DoijBD3E1D4aUVQ0skanaU6L+OvSvxT2LVYnYeJocHSw0b5LeW\u002Fdw1NLLuTcJoKfy1ea3fuiqWnJpaWGFpBGDDQUaBvFFEl\u002FfW7kH225H9o+X3seXrOOCBI9dxdTMpml0CrSXE7U7RQtp7YoxXQiCvXIP3D90OffePmGG95jvZLiVnCWtnCrCCHW1FitrdS3cxIXV3TSmmt3anRVeyP5r\u002FAMV9lVlXjNq1W8e26+kkeE1OwsCn925Jom0SLBuncdZgsTXxKwIEtI1TE9rqxFj7iHmn73\u002FtHy\u002FPNabZNe7xcoxWtpEPBqONJ5miRl\u002FpR+Ip8iepj5V+5r7w7\u002FBBd7xDY7LbuAdN5MfHAORW3gWaRD\u002FRkEbCuQDjoI8P\u002FOX6hqKxI890v21hseXtJXUdRsrOSxR\u002F8dDQQ7io5pD\u002FALSrE\u002F6\u002FsGWv34OR2mVb\u002FkveIrcnLIbeUgepTxUJ+wEn7ehpe\u002FcU52jgZtu582We5phGF1ECfTWYGA+0gDoOvnJ\u002FME6U7o+MWU6+6Y3Rla3dPYmawOI3PgcttnPbfyuD2TR10WZ3A+RkyNDDQKMn\u002FD4qBVgnmMy1D2uisfYZ9\u002FfvI8h86+1d5y5yPvE0m87lPFHNE0MsMkNsrCWUuzoEo5RYqI7Fg7UwCQKPu+fdp5+5D92rTmbnvaIY9n2y3mkgmjnhmjlumQxQhAjl\u002FwBPW0xLoukotckDqjz3z\u002F66Cdf\u002F0xi98AOvoR6uY\u002FkyV2Jj7A+QWMl8Yz1TtLrvIURNvK+Fo8tuymr1jv6ikOQrYC9uAZFv9R7zn+41PaLv\u002FuPavT697SzdM58NHnWSg9A7x1PzXrBP79tveNy37aXaV\u002FdyXt6jeglaO2ZK\u002FMoj0\u002F0p+fRpf5unXe9t4\u002FH7am5trUeRy+F6y7Ai3ZvjDYyCarqE29Pt\u002FNYQbneigDy1NLtWryKTVBVHaClllnsEiciXvvk8s77vvttte5bTA81nte4C4uY0BYiIxSR+PpAJYQM4LUHYjvIaKjERB9ynmfYNi9y942neJ4oL\u002FdttNtaSyEKDMJopfA1GgVrhUKpUgPIqR\u002FE6g64COkqJJE6SxyKrxyRuskciMLq8boSrowNwQSCPfLnjkHHXU1lZGZHUhwaEHBB9CPI9cve+q9Yppo6eGWombRDBFJNK5+iRxKXdj\u002FwVVJ91Zgqsx4AV\u002FZ1eNGldI0FXYgAepOB1tV\u002Fy6ugKXov437Wq6+jjj372tT0PZO+qtkT7qOozlDDNtzbjTBRL9ptXbskFMsZJUVJqJAA0re+wH3avbqDkD2w2eSaADf8AdkS9umI7qyoDDCeJCwQlV01IEhlYU1nrjr96D3Jm9w\u002FdPeIbacnlzZmextFqdJWFyJ56cNVxOHkLcfD8JMhB1WJ\u002FNT+UmW312JWfGnaeTqKTr7r40MvZa0NUUh3vvespaTK0m28oISDU7f2ZQzQSy0zN4qnJVH7qN9onvFT73Xu9fbxzHL7X7LdPHsO36DeFWIFzcsFkWJqDuitlKkrq0tOx1qTChGW33PPaCy5e5Yg91t5tFfmXc9Ysda1NpaKzRtPHX4Zrpw6rIBqSBOxh47dVKf0H4AAA\u002FoALAD\u002FAD3hZ1mj1737r3Xvfuvde9+691\u002F\u002FUGL3wA6+hHoYOhO8N5fHPtXbfbGyFjrK7DefHZ3btTUGloN47RyUlOc7teuqAkopWqxSxT0lSUf7Wup4ZSrIHRhz7b+4G9e2HN+2837EqvNFVJoWOlbi3cgywM2dOrSGjejBJVRypAIII9x\u002Fb\u002FYvdHk7dOTOYCUt56PDMq6ntrmMN4Vwi1GoLqZJEqPEid0BDFWG150D8j+p\u002Fkvsum3d1tn4K0tCqbg2nkmp6Xd20sgVUVOJ3PgfNLPRTRO+lZRrpalSHhkkjYMewvt37m8n+6OxQ71yvuKyAr+rA9FuLd+DRzxVJUg4DCsbjuRmUg9ca\u002Fcn2t5z9qN+l2XmrbWjGqsNwmpra5T8MkE1AHBAqVxIhqsiKwI6K53b\u002FACvPjP2t\u002FE8ttPE13S28shNUVpzfXhjhwE9fUyK8s2U2HWmTbNTC\u002FqutJHQSXYkSA\u002FWJ+ffuo+1nOP1V5tdi+yb3IS3i2mIix83tW\u002FRK8cRiI1zq6l7kD73Xuxyd9JZbzex79sUShPCvamYIooBHdrSdSMUMjTLQUKkdUbfJv4Yd0fFaePI70pKLdPXddV\u002FZ4zs\u002FacNW2BjqHZft6HdmNqPLXbMyFQJAsRnkmoqhwyxVLONHvAH3X9iOePaMi83mOO85adwqXsAbwwTQBZ0NWt3JIpqLRsahJGIIHQf2m9+OQ\u002FeGNrXYZ5LPmeNNUlhcFfGKj4ntpFol0i0qwQLKgoXiCnV0VOnpUrq\u002FE46ZQ0OSzuCxdRGw4eDJZmhoKiNh+VeGoZT\u002FAIH3DcEAubqytW+GW4iQ\u002FY0iqR+YJHUySTNb217dIaPFbzSA+hSJ3B\u002FIqD1vGU1PBR08FJSxJBTUsMVPTwRjTHDBAixQxRqOFSONQAPwB778RxxwxxwxIFiRQABwAAoAPkB18+kssk8sk0rlpXYsxPEkmpJ+ZOetJffe4q\u002FeHYPY27spM9Rkt0dib6ztdUSWMks2Q3TlZk1EcftQFI1H0CoB+PfB3mbc5t75n5n3m5ctc3e5XUrk8Szzuf5YA8gAAMdd9uXdrt9j5Z5X2S0QLaWe12kKAcAEt4x\u002FM1J+ZJ6S\u002Fsl6N+ve\u002Fde697917r3v3Xuv\u002F9UYvfADr6Eeve\u002Fde6nYfK5fbmaody7ZzOY2zuXFyJLjdxbcyldg85QSRtqjNNlMbPTVaor8+MsY2\u002FtKRx7Vbff7htN9Bum0bhPabpEapNDI0UqH+i6EMPs4eo6T31nY7pYXG1btYQXe1TAh4J40licHjqjkDLX50DDyI6tA+Pf81nujrzI4zDd6Qp3BsHyQ01duCix1Fi+0cHRiyNXwfY\u002FY4HefgBLyU80NJVyhfRO7+hsuPbP73\u002FOmwXdrt\u002FuCi7vsBIDTKix3sS\u002FxDTpiuAMVV1SRuPi1wcRvcz7nHIfM1rd3\u002Ft452PmOhZIXd5LCVuOg69c1rXgrq0ka17owuRsE4DPde939b4\u002FPYebDb7617I2ytRTtPSrWYfcW3M5SNHNTV2ProQTHPBK0VRTTxrJG4aORFdWUdHbC+5e545agvrRob\u002FlrcrWo1LqjlhlWhV0ceYJV43UMpqjqGBA5p7lt3M3IHNNzt19HPt3Ne1XelqNplhniaoZHQ8QQGR0YhhRlYgg9ak3yh6lofjZ8jt99b4+Wdtr7J3XtjdW1ZKuWSoqYNjZN8Vu3E0lRUyFpauTBU5moDKxLyij1N6yffGz3a5Ng9r\u002FdLfuWbMOdqsruCe31VYi2k8OdF1EkuYlLRFjlilaZ67S+0XOlx7q+1vLvNVyijd7+znt7gKAFN3GJLaVgowomOmbSML4tBgDrcOoa2lyVFR5GhmSooshS09bR1EZvHPS1UKT08yH8pLFIGH+B99roJ4rqCG5gcNBIgZSOBVhUEfIg164eXFvNaXE9rcRlLiN2RlPEMpIYH5ggjrSy7b2bX9ddudsbCykZirtpdlb0xUiEEBqWTO1mSxFRGSBrhrcJkKaeNvoUlHvhbzxsVxyxzrzfy\u002FdRFJbTc7lM+a+KzIw\u002FoujKwPoeu8\u002FJe+2\u002FNHJXJnMdo+q3vdqtZB8mEKxyKfmkqOhHkVPSA9hfoSde9+691737r3Xvfuvdf\u002F\u002FWGL3wA6+hHp4wu3Ny7mmr6fa+2tx7pqcTi6rOZWm21g8nn6nF4Si0\u002FeZjIU+KpauakxtMXAaV1C3Nhc8e1+3bTu+8SXEWz7TdXcsMTSyCCJ5THEvxSOEVtKLUVY0HSG\u002F3TatpS2l3fdrWzimmWKNp5Y4Vklf4YkMjKGdvJQSaZNB0wwVEFShkp5o5kDFGaJ1cK6mzxvpJ0SIeGU2Kngi\u002FsuV1cVVgf9Xn6fYejGSOSJtMiFWpXIpj1HqD5HgevVFRBSwyVFTKkEEQvJLIbKoJAA\u002FJZmYgKoBZmIABJA9+Z1jUu5oo63HFJNIsUSFpDwA\u002F1ftPADJx1ti\u002Fy8Ng7u63+InU+3t70ddis7V02f3P\u002FAcnE9PX7exm7dy5fceIw1ZTSHyUlXT43JRPNCwV4JZGjZQyke+xv3bOW955V9meT9q36J4r9kmm8JwQ0MdxPJNHGVNCrBHUspAKMxUgEEdcZvvOcybJzV72c57ny\u002FPHNtyPDB40ZDJNJbQRwSyqwwytJGwVhUOoDAkEdUZfzPMtj8\u002F8z+yKemEc8OE2f11tbJEEMklfFgJs1VQtb8x0m4oUYHkEEe+f\u002FwB7G\u002Fttx97uZLeHItrO0hc+RfwfEI+0LKoPz66FfdJs7nbfYjlWWUlXuL69uE9QhmESn82hYjq5P+Wn8iKTur494XaOWr0k7F6WpcZsLdVLLIv3mQwdFSeHZG7Vj0gtS53BUghke7WyFHUqTwL5w\u002Fdd9yrfnv222\u002FaLq4B5k2RI7S4UnuaJV0209Kk6ZYl0kk1MsUlfKuC33rfbGbkL3Mv96srYjlffnkvLdgO1JWat3bV\u002FihmYsox+jLERxNCy\u002FwAzb4S7s35mV+R3TOAqdybiiw9JiO1NkYiJp87uDH4aMxYPeW26IHVlM3hqA\u002FZ1tFH+\u002FV0UcDQq0lOUliz71PsNuvMt0fcvknb2uN2SAJfWsa1lnWMUjuIlHdJLGg8N4wSXjWPQupWrLP3TPf7ZuXLE+1vPW5Ja7W07SbfdyGkULymstrO3+hxSv+rFKe2OVpBIQkmpKEEmjkaaNWIlp5XgqYJEeGppKiMlZKaspZljqaOpiYWeKVEkQ8EA++cx7XkjYUkQkMDgqQaEEHIIIIIIBB66PMjKEYjsYAqQQVZTwZWFVZT5MpIPkesnv3VOo8VXSzSNFDUQSyoXDpHKjspj0+RSFJs0esah9VBF\u002FqPbayxs2lXBbPA+nH9nn6dOvDMih3iYIfMgjjw\u002FbQ09aHqR7c6a6\u002F\u002FXGL3wA6+hHq5f+UJvbp\u002FZuT7got3b42zt3s\u002FeuR2tj9s4TcFRFh6vJ7P2\u002FQVlQWwmTr5IaHKTVGfzdQJaOFzUxeBHZCrqRnR9y3feS9lm50t933+2tuab2WBYYpmEZe3iRjWF3IRy8sj6olPiDwwxBUimCn32tg533215HuNl5fu7rlKwiuHnlhUyrHczOo\u002FVjQF4wsMSaZWGhtbKCCpHVsfYfw3+K\u002Fb2Ubcm+ekthZ3NVf70+fo6B8HlMgZLuJ63KbaqcTV5F31X1yySEi3Nre8w+ZfZT2l50uW3HmDkXb7i8ky0qoYnfzq8kDRtIc8WLH59YZ8se+fvDyTZjauXuf8AcrewTAhZxLGlMUWOdZFQD0UKPl007E+C\u002FwASOtdyY7d+z+jNn0G5MRL9xisrkTmNyTYyqX9FZQR7mymYp6SthPMc0aLLGwurAgH2j5d9gfZ3lTcoN32LkGyi3GI1R38SYow4MonkkVWHFWADKcgg9LOYvvDe9PNW1XWyb57hX0m1TjTJGnhwCRfNXMEcTMh\u002FEhJVhggjoOvlx8+urPjdgq\u002FCYDI4nsPuetopV27sPE18dbR4iqkBjgzXYGQoJXXbuBo3PkMBZa+usI6eP1NNEG\u002FeP7w\u002FKHtfYXFlZXcO486vGfBtI2DiNuAkumQ\u002FpRqSDoJEsuFQAFpEFHsr92\u002FnD3U3C2v9ytZts5EjkHj3kiFWkUZMVmjj9aZhjWAYYq65WwEfV33DuHP7v3Dn937sy0+e3VuvM1+4dyZupSOKbKZrKTtUVlV4IQsFLAGIjggjAjp4ESJAFQe+Se7bruO+7puW97xeNcbreTvNNI1KvJIxZjQABRU0VVAVVAVQAB1122zbNt2TbNt2TZrJbbZ7OBIYIlJIjijFFWpyx83c9zuWdssel70p3T2D8fOxsR2h1nkoaLcGNhkx2RxuQWaXb+7duVM0M+Q2ruakgeOWoxVZJTpJHLGRPRVKJPCQ6kMJfb\u002Fn7mL205mtOaeWbhUvUBSRHqYp4SQWhmUUJRiAwIIZHCuhDKD0G+fuQuWfcvle95R5stWk22Vg6OlBNbTqCEuIGIIWRQSrKeyVC0cgKmo2X\u002FjR8\u002FuifkZQUuP\u002FAI1S9bdliJP4l1tvXJ0VDkXnsivNtXMTGmxm8sY8rkJJSEVKgfvU8J499Ufa37xPt97m2sMC7gm2cz6RrsrmRFevCsEh0pcISDpKUkoKvEnXKH3X+7b7ie11zNc\u002FQPuvKlTovrWN3QDNBcRDVJayUGRJ2E\u002F2csgz0LnZnxQ+NfdGQ\u002FjvZHTew91ZmcB5NwSYlKDOVYKroeozuGkx+UrBoA0mSZ7L9OPYy5p9ovbHnidr7mbkrb7u9cZmMeiVh5apYikjfKrH5dAvlP3l91eQ7b938rc9bjZ2K4EIkLxL8lhlDxrnjRRnj0GFH\u002FLo+FFDUR1MXx92hNJE4dUyFbubLUxYfTXR5TPVlJKo\u002FoyEf4ewnbfdo9irSVZofbmzLg\u002FieeQfmrzMpHyII6Fs\u002FwB6H38uInif3LvVVhQlEgjb8mjhVh9oIPRdf5lXXnRG2viTlds4SLrPrncG0s9tzdPXW1sRT7e25k8zlaDI01JlsJgsTQR01bV1WZ25W1MbiONg7aGlIA1rGn3ouWvb7bPZu92myTads3GzuIZ7KBBDA0kiuqyRxRoFdmkhdwdKmp0l8ColD7qfM\u002FuJuvvTZ7tfvu26bZe209ve3EjTTxxRujNHLNI5ZFWKdYyNTCg1BBU6Trk++X\u002FXUbr\u002F0Bi98AOvoR6xTQQ1MZiqIYp4iVYxzRrImpTdW0uCNSn6H6j3V0SQaXUFfnnq8ckkTa4nKv6g0P8ALpbbX7I7M2MoTZPZvZGz4lN0p9tb73TiKOO97+OgpMolDGOfxGPYg2nmvm3YEWPYebN0soxwEF1PGor\u002FAEVcL\u002FLog3flXlPmEluYOUtqvnPFp7S3kY\u002Fa7Rlz+bdOO5e4O5N6UrUO8e4u190ULjS9Dm+xN2VlFKv+omozlUpp0P5DqwPtTufPHPO9wtbb1zvvF3bEUKS3lw6EfNTJpP5jpLtXI\u002FI2wzC42PkbZrO4HB4rK3Vx9jeGWH5EdBxDBBTqywQxwq7mRxGiprka2qR9IGuRrcsbk+wqiIgIRQB0KXkklIMjliBQVNaD0HoPl1l936p1737r3WGangqUEdRDFOgYOEljWRVdf0uoYEK6\u002Fgjke6OiSAB0BHzFerxySRMWicq1KYNMen2fLpfbX7R7V2NEINk9q9nbRp14Sk27v7dWMoox\u002FqYqCDKiihX\u002FAAWMD2JNp5w5x2CNYdi5w3WyhAwsN3PGo88Kr6f5dBzd+UOTuYXMm\u002F8AJ203sp4tPZ28jn7XMes\u002Fm3T1mu9e+NyU7Um4e8+5MzSuLPTVvZe7\u002FC4\u002FIdKfLQBwf6G49r7z3F9xdyQxbj7g75PEeKve3BU\u002FaBIAfzx0gsPbv262qUTbZ7ebFBMODLYW1R9haM9BS0SyVL1sxkqq6TUJK+smmra+TWbv5K2qeaqk1nk6nNz9fYOYa5WnkYvO3FmJZj9rMSTx9ehkHKxLAgCW44IoCoKcKIoCinlQdZPe+qdf\u002F9EYvfADr6Eeve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r\u002F0hi98AOvoR697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuve\u002Fde697917r3v3Xuv\u002FZ",crossref:"data:image\u002Fpng;base64,iVBORw0KGgoAAAANSUhEUgAAAYQAAACCCAMAAABxTU9IAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA3FpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw\u002FeHBhY2tldCBiZWdpbj0i77u\u002FIiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDYuMC1jMDAyIDc5LjE2NDQ4OCwgMjAyMC8wNy8xMC0yMjowNjo1MyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1ZGY5NjA0ZC1hZTk1LWYxNGMtYjk0Zi01NTMwNzcxZWZkNGMiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6MjkyRDk2MUQwQkRGMTFFRTk5OTlFOEQwM0UzNUM3MkQiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6MjkyRDk2MUMwQkRGMTFFRTk5OTlFOEQwM0UzNUM3MkQiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENDIChXaW5kb3dzKSI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOjA1YjA4ODVjLWFiZDYtN2Q0Ny1iNDQyLTEyM2M0ZDMxMzI3YSIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo1ZGY5NjA0ZC1hZTk1LWYxNGMtYjk0Zi01NTMwNzcxZWZkNGMiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw\u002FeHBhY2tldCBlbmQ9InIiPz7MCU3SAAABgFBMVEV0enpkbGx9hISIjo7\u002F1WpYc3iHydj4+Pj+\u002Fv6ipqbLzc33iEtZYWFrc3O8vr6ytrbwRkqKkJD\u002F8c709PT\u002F4JH+vzj28O389PTo6enC5ez\u002F6rXT1dVvd3fh4uLn6OjvM0CFi4uTmJiNk5NLVFR5gYH\u002FxiScoqKfpKQwrcXHyspnw9Sws7PuIjLq9vn\u002F6bD5+Pb96Ol8ytr\u002FxBTX18jAwsL\u002FzSjvKzpIUlLuJUFETk7\u002FzTpWV1Wk2eRHUVH29fLMzs7\u002F01T18\u002FD8\u002FPurqqK\u002FvLLWz8Dw8PBWX1\u002F\u002F\u002Ffj\u002F+vD\u002F0FJPrsLtZWvvNkzdxrxZqrtVmaZah499g4P\u002F7L6p2+W\u002FwsL\u002F9d74lU38s0nuWWBfg4nyXVM\u002FSkrlmpbrd3rgvrb\u002F9+XhsamMkZH7pkLzaEnjqaPojo2Ok5P1+\u002FzZ7\u002FNglJ7\u002F4pj1dk9ocXH9\u002Ff3\u002F\u002F\u002F7ohoXX2dmZnZ36\u002Ff2Bh4fioKD5nkutsbH8u27\u002F+fnvECfY0sT\u002Fxyw+schPWFj\u002F\u002F\u002F+Eg0rNAAAAgHRSTlP\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FADgFS2cAAAxCSURBVHja7Nz9n9O2GQDwi0xbCJBgaD+9wlEk2STkfB07yCUc3THKvRQYtAUKLRulg7Wl3QYHXfd6R\u002FKv17EdWZL1ZjuJ9wnSb+Qcx9ZXlh49klkY2lJ5WZjSeQ8ev3n5N7Z6K0TYfXDzwoWzZy1CZQi7Dw6EAvv7+xahIgQiYBEqQth98C4RsAhVIHACFmHmCKHAWVbAIswWYffPAgGLMEMEmYBFmBVCKLAvEbAIM0FQC1iE6SPs3tAIWIQpI5gIWISpItyQjsQWYTYIN\u002F52wUzAIkwJIY+ARZgGQk4BizBxhBsn8wpYhMki3Dh5Nr+ARZggwvWT+4UELMKkEK7fKyxgESaCcP3ezRICFqE8QmkBi1ASIRIoS2ARSiBMSMAiFEa4fu\u002FAhAQsQjGE68cnKGARCiAcPH7z7CQFLEJehCkIWIR8CP\u002F9z9raxAUsQj6E367cPvz12ppFqBbhzKVLU3CwCPkQwnLp0rHD703UwSLkRogc\u002FnT4vf01i1AlQuzw1aQcLEJBhEk6WITiCCOH299NwMEilEKIHf5Vcpy+ahFKIpR1uHr56ru2dssjjByOffeXAg6hgH0KJoYQOfw9dLAClSIkDpfXDAXsK+TTQUgctOFSKHDS1uj0EOK0hiq9ZAVmgKB0sAIzQ5A4WIEZI2TSrVagEoTY4auRgxWoECFJ8x04buuvSoRe78ytf16x1VcdQiSwffHieVt9FSGEAg+\u002F+ezixb29vfNhsTU4cwRKIC7WYcYIvV7v4Q\u002FfUgITc6gHyIXAQe2mRdAQPHz57R4vkDoUhmgBB3uDuGAHwsYbUvPdoI1a7aBpjNDrHXv5RCJQ6oEIgIsGbMEQz78AAm7c8jxohtBbOfb4ybZSoKBDAPFAUBBszTkBTFueY4DQW7n9+PvPLhoQ5HdwhAQxwzwbME1Pi9BbOXMrh0A+hzpEg4FcYX7HaIe5bzVCb6V365u8AuYODThQFieYU4OOOzBE6PVWigokDjunWl2VgTPQFGdOw6T0xlEYC0JfhtBbefhDGYG9LXTE6\u002Fc7qr6I7ntc6PsY+4Adp+d0XCB3jpF8nhAGQ4IpWR6Bn4\u002F0+6PoS4EAKAIHUBErxYDAXPZG4ztEbdlkLQyGXpYS2N6srffjX1EggHRsQj7zlzY1XON5HJzbggedRuj1zjx+Uk7g+aMNUodyhEba3jFWAPlvGkJpgb3NpfUNj+rT5Qh+atBWPSa4O38ILXJzPMJoheD7vRIE25t3+4yACqGJ0yFZOXQNwBwjIBbhf6HAdpmHYPPuYN3LxJhSBF9TyW2kDZCaYTzFjRhd1wcA+KClDG3rjQCFByHNQW3s+7Bl8iCOrqSV\u002FSy8Er\u002FVMUd4f+\u002F982WCoV8+FQioEEikjDuaKC55UEa1O6rg5Gab\u002Fijh4dGzufgjkop1xEMRgI7rxdeKXQiQsAX4kGR1wzP5TH\u002FZHF9J0pc0QfSzmM8Kk1ykT2G3\u002FFFBaVIgOtXz4fiN\u002FmtXPrlSqBfaco+IBRQIafwDdAFsMmSM1eJ\u002F+fE9eumT0AZcFgo5WYbMQaPJEn+Qm0kpYkh1mQ2PuZLxGSkEyCYlBhiQywyEFQWH1P9tUcBh6ykJR\u002FMgAG0I2m21k4IzCI2xYYrgY1HyiRvyAR5oj+oK01kIdIQI6dE4TQtnT4CBOUJeh1Bgo6\u002FMO8gQnNxTYgohTXeMEQJJIhC7ksCXPYrqzwNZKgV2BQjdNH4YIyBxZt7Pg2DusIVq\u002Fb6nSf5IENLYCORFCOjIKUEIXJN8OGWAXdelKsttCFIpHncQFCBQiResMiBTf2MEE4etn597GzoBOUKasWjnR6BafYzQpVqvt7G+Tk9V0gCYLN+F\u002FU83mi0KBqa0mwRBJ4pxSP8+PhOFQD9aMUILs1fS55pDAEPZdCExSt+RBN4wj8PW5vOBiYABAq7nRmhSbS1GSNtj\u002F2jz1Yc7O6cO1dLKCbhwC6WdTyuTw3GzQxWpModH6NCtHrPNoV9rnt7ZWTx1aEnQ6SFxiDqUOHwimJItmQoYIDj5s79eknQdReFR8IjJPR29fzA5+p2nC1w3UseCDjDgPhzPThB9XQgxUgSBuRIAmPuq1cdXsriQvddWDoSsw\u002FbmQg4BOQLMn6pmBkwXiP5y9DT16aGFAVOhRCoQXQhkG4c7lB\u002FUYO7fhQKuQW2RqsLxvBQhTdpiaOIQTopzCUwLAYG66M6X6qoAzB\u002FwUW3aR7surCsQHCc+yM8gsFdCbmvpFf3pzhJ\u002Fs\u002FkRIodQ4JdBP6fAlBC4xDeUdG0nakwL9E1+EmYarWDO7cn2IpBUC5cNO+RxmcpCCMMf374jnxXPGoH9TnfcrW+c5qfHHt3hEwTUMlhnchsmCFCcD1taZD\u002FfWeeOL4Dw45fnzp17\u002FfrzO0c+9v4PELjMN7mhhRfcNz5k7r2VhiR+Xb\u002FYh4EeAQfiS6xx39gF3LOaFyERiMvndz7N6TAFBCg51VP+Gy9qTI7QobMIji7BPlpxbWkQoGSZ6hn\u002FFdIfdQsgfEQLJA5\u002FzOWgQygQojriU3nZDmSBiQR9xO4rAH5Due4dTaUArMsRuIGDTNuf\u002FZ4rz7ipqTmCQCAuX+Rw0M0T3NwIfOp7\u002FPnGq2zO1GOWSEF2x2umjjuZnVAY+kiMgBuyJZI+VwYcmyHCSOC1vHzxlqGDdsbcyIvAPzvj9re+mO1cWIShaNdr+ER0dUnU8LGBAgRH9RRJsoDmCD+pBaLyBzMHbe5I3h+1g6Sw6wlQhrCTOcN9DmHYFm4+xiwDEh2E0ukhQYD5EYAhwk+regFzBwlCutVCPjJDLymgNEIa6TQd0Q5kl+0Vwxkclu+MnTbCP8wFUgev1HqCq90glbRjR5L7VnRHDU+0baaJQaaOM3vMAggzL024gSECch1J8XUIuQUSh7++pXgctCtryNFMXsdXKkNwZHO1NDLM7l0K65h9IrBg63EbcBBQjeALqjbXlpfhB4UEtA4yhDTzqUVIwiEZwvi4\u002Fv3MKe4qayUA9FKwZG7Wpt8iipMZ+ifBL4jwUWEBtYN+t4XkXZAmP2roELzM8tDBmi4EQymDPFamHhmoRCCpI1ARwsjhjsDBYN+ReKkf8PsgZQikGu\u002Fucn9ZXNfPCElAiuRLfA12AJMikOEJVocQpTVqnIMUoYOV299xZhukDIE8MpmR+d+eQcuEkumvMEcYrQPKEaDpDHSqCK8zaT7FXlSk6JDaONNZyxCk\u002FdG1Gn3+JmpFBQWKzXBofFBd\u002FBMaBCC9oUZcmjNCiB08T4tALxBAvmZwNmyRIpBjjy6KkxZRw+xI+gqyQbpFfcVD4kvVdEfk6eb7v3oy54GzQ4jSfB\u002FHz4MCAdNbFZgVKnrK6g91COn7X0vv0DO1JXYNSJL2cOler4PFPwKNBuZ0oEPi75MAYTYIUZpv5KB6U4dJa7ow3ivYwcyr5U5Hi0BtgaEWmdNdDg7XVwBhI4fsyhpTwWQ7gA6BjE\u002FM4h\u002FZLwiHs0aIHRwFAp\u002FWxI4DubxN+kqXAoGa1x29++rEi4PXTtyvLfCzQbIUzVQQAYTcHBIKoqM4glIgpHeUvgtfJ7eUrtbNEuH16urbyihB9RYzXxUKhICS21h\u002F9OgRtRKLsvu6EMmJpm\u002FVJ3FyPZ2+kCUdiA1nzGx7QHC0JSecdAtyA7NDWF1e\u002Fp0uVlMrYGdohCDde8gwUlTRXiFAbZ4mTwc1UI2WdACduUh2bykR6tI3s+lucEYIJgKyFL+gL9IgyHeA0hWFsUEFyZsFcoZ6hGFHcgJmKJoFgqlAnEpDsvoLhsYIktO4bNIIufpGKm0W5LFUI0hOwO4bmDpCHgE+gUM3OySZVsimv9jJrl7yrzo1hes13E+1RJ5Uk9AhCJaO+BclpouQWyCbqoyrL5tESF5Sil4tkjDQN4\u002FF\u002F3NVE2Z+CutrEdMbL5r6KwkAlfXDbmbbhjs+hT9xhGICJHfvuNjzsBsOh62ib8122w4YbTUHAHXVPxVOX93RT0leHuygcESOLie6nk7+S2ni6EoggIb\u002FScokEEKB1WHJUu90O8OZlHq3262bHDScWSmNMAmBN72URLACVSMsLy9\u002FYGuwSoRVK1AxghWoGsEKVI1QZkJgyyQQrEDVCFagaoTV5VUrUCmCFagawQpUjWDHgaoRrEDVCFagagQrUDWCFagawQpUjWAXyapGsAJVI1iByhGWl5dtRVSNYJdoKi6\u002FCjAAtUPuhnb2u1cAAAAASUVORK5CYII=",dimensions:"data:image\u002Fjpeg;base64,\u002F9j\u002F4Qr6RXhpZgAATU0AKgAAAAgADAEAAAMAAAABEnUAAAEBAAMAAAABEnYAAAECAAMAAAADAAAAngEGAAMAAAABAAIAAAESAAMAAAABAAEAAAEVAAMAAAABAAMAAAEaAAUAAAABAAAApAEbAAUAAAABAAAArAEoAAMAAAABAAIAAAExAAIAAAAfAAAAtAEyAAIAAAAUAAAA04dpAAQAAAABAAAA6AAAASAACAAIAAgAD0JAAAAnEAAPQkAAACcQQWRvYmUgUGhvdG9zaG9wIDIyLjQgKFdpbmRvd3MpADIwMjQ6MDE6MjQgMTk6MjU6MjEAAAAEkAAABwAAAAQwMjMxoAEAAwAAAAH\u002F\u002FwAAoAIABAAAAAEAAABkoAMABAAAAAEAAABkAAAAAAAAAAYBAwADAAAAAQAGAAABGgAFAAAAAQAAAW4BGwAFAAAAAQAAAXYBKAADAAAAAQACAAACAQAEAAAAAQAAAX4CAgAEAAAAAQAACXQAAAAAAAAASAAAAAEAAABIAAAAAf\u002FY\u002F+0ADEFkb2JlX0NNAAL\u002F7gAOQWRvYmUAZIAAAAAB\u002F9sAhAAMCAgICQgMCQkMEQsKCxEVDwwMDxUYExMVExMYEQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMAQ0LCw0ODRAODhAUDg4OFBQODg4OFBEMDAwMDBERDAwMDAwMEQwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAz\u002FwAARCABIAEgDASIAAhEBAxEB\u002F90ABAAF\u002F8QBPwAAAQUBAQEBAQEAAAAAAAAAAwABAgQFBgcICQoLAQABBQEBAQEBAQAAAAAAAAABAAIDBAUGBwgJCgsQAAEEAQMCBAIFBwYIBQMMMwEAAhEDBCESMQVBUWETInGBMgYUkaGxQiMkFVLBYjM0coLRQwclklPw4fFjczUWorKDJkSTVGRFwqN0NhfSVeJl8rOEw9N14\u002FNGJ5SkhbSVxNTk9KW1xdXl9VZmdoaWprbG1ub2N0dXZ3eHl6e3x9fn9xEAAgIBAgQEAwQFBgcHBgU1AQACEQMhMRIEQVFhcSITBTKBkRShsUIjwVLR8DMkYuFygpJDUxVjczTxJQYWorKDByY1wtJEk1SjF2RFVTZ0ZeLys4TD03Xj80aUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9ic3R1dnd4eXp7fH\u002F9oADAMBAAIRAxEAPwD1VJJJJSLKysbDofk5VraKK9X2PIa0Sdo1P7zlmO+t\u002FwBWG\u002FS6njj4vCzOrVv+sfXGdKrcW4HT5sybW\u002F6UgsZtnczfW79G3\u002F0L\u002FwBGubxeq9FwsL7Fn9Gx+qdSqsdWy5oqFdrA7ZXY7JsDnv8Af7N\u002FpP8A0fp2K3DlQY2eIz0kYR4Rwxn8t8TWPM+uhQhZiJn9KUPm4XtD9dfqkOerYv8A24E3\u002FPj6o\u002F8Alti\u002F9uBcn\u002FzQ+svV5a\u002FpvSPq9jOJBNdDLsrafCxm9n\u002Fbd2K9aeD\u002FAIq+hUVO+2XX517mbQ57vTra7tbXRRs+j\u002Fw1t6jnDFH9InyqX4tiJJ30ewxsnHy6GZOLa26i0bq7ayHNcD3a5qKvN\u002F8AFl1qzEuf0DMJaLi6zHBn23Nn7XR\u002FJbdsfkM\u002F4T7R\u002FpF6QopR4TTJlxyxy4ZdrHkpJJJNWP8A\u002F9D1VUOtdQOBhOfXrkWeygfyiPpx\u002FwAG33q+uZy2v6p1t2NbYKGVE1MB+lAh9npB3t9e\u002FwCl\u002FwAT7\u002FegZxiY31NV3Y8xkIER+aXpDUxMLIy6HdJwH7KXu3dTzh+eT9LHqf8ASexrPZ\u002Fw\u002FwDOfo6LP1knQuhdNzKOq9OzaBdjV5ZbU130mhg9Jj2WM2vrs2N+nWupx8enGpbRQwMrZw0flP7znfvLL+r7QLupeeU\u002F8qsy5skSiTwylRjXzGYPFKVtaHK+3PF+l80Zfuxhw+mEf5et5yzo31v+qJN31etPWekM1f0q8\u002FpWNHuf9kc3\u002FoNo\u002Ff8A6FkvW99X\u002Frj0nrkUsLsTPGlmDf7bA4fzjWfm3bNrvofpP9LVUt1Y\u002FXPqr0jrY35Nfp5TY2ZdXttERtl3+E4\u002Fwv0P8HsUU8hlqQDLv8t+boYhivhyGUYnaUfVwf4P6UXieo9Hud1vrmLhyM\u002FDtZ1fp72D3a7bMqpmnv8AddV6LP8AT0f8I9d39XetVdb6TTnsgPcNt9Y\u002FMtb\u002FADjOXe38+v8A4J9a4hmR1Do31qosuyD1u2it2O443uufXD9tF7fd+tU2fpnbrLrPT\u002FnLVc+rOe7F+tluNj4t2JidUBsdh3t2Oqe1rrvWrZH8x\u002FOVs\u002Fm\u002Fps\u002F0Nare+DIA9+F1+Y5OU8F6S9vFHPDINOKMfTmhwy4Z\u002Foe9H0vfJJJKZxX\u002F0fVVg\u002FWTA+jn1jiG37dDof0Nwj8+t\u002Ft\u002F9VreUbGMsY6t43MeC1zT3B0IUXMYvdxShdEj0y\u002Fdl+iUEA7uV0vqr97cLOI9UgGi\u002FwDNtafoO7e93+v6X9GpdEbttz\u002FPJefxKqU4VfqWdHyifaTZh3fnAH3GP60e9v7\u002FAKqFj5OXRVaKSS19h9TKaC7Ucls\u002FvfT9yyfvk8UsMswMo4zkGnqyicY8MsWQf1P87\u002Fm2WOPi0G\u002Fi7mZn42Gwuudr2Y3Vx+SxsjG67132PsPTOnO5ayfWsb3l3td\u002F57\u002F4vIV7p46YCLBaLbyZL7dHT\u002FJD1pz3V7DKfMjjnkiMfTFhlx\u002F+HZo\u002F9CDKJjAfRC8n+cyD5f8AZ45f9KTxWNidN6P1\u002FJvpr24fRsUAQZc++2Gy5x\u002Fwt3qPp\u002FcWl9UenXu+0dfz9c3qR3N59lPLGs3fmP2s2f8AAV46zcTEd1jPfQ4Rj33HLyiOSwbm01T\u002FAG9jP+uWf4JdqAGgNaIA0AHACbyR928gFY4ylweMj\u002F3kPS2+ezmEfb4jLLkjCOSR1l7UPVw\u002F9VzceWa6SSSvuW\u002F\u002F0vVUklyGPbgspqvxsl37Zsz7WNrrtc99kZVrLqcjH3WN+zMxd\u002Fq76v1KpnrVej6NaSnoupYDstrH0uDL6j7HGRIP0m7m+5V8fE6vj1Cqp9DWN4GpOvj7VmN6j1N1Afg5FNNNTsOllbq3Wbvtb2VPtse+11v6JtzH41e\u002F\u002FwAMb\u002FU\u002FQhzvrB1TFJxftFQsxheTkvFTG2mlzNjMj7Tfi1VMZVaz7d9md6v6T1aPstf6NVcnI4p5TmBnjySHCZY5cFrxMgVQI8Q7F3Ts+9pFjcdznfnQQf8AOa1V29I6xU0tpvYxpBBZvdtg6H2Orc1Bf1XrDQ\u002FLORjNrGRdiCh7HCthYyw1W3ZO71f0WRV+sWbfR+yf4Cu39IqlXXss1NufdW6+ltrLL7WgsqDndO3Xu+w5DsTKxcarLflWWfo\u002F0Pp13WYf6zaov9FYOLj4snH+9x+r\u002FGZY83kAoCNdiNHoekdMb0\u002FGLCQ66w7rXDjTRrGz7tlbVeXN09R6lkZR6bj9QptaLS057aw58Cr1zQ1od9ldk0v9L1bdnpfZ766\u002FS+0fpEPB+sGdb1HDryrq2syA2s1UNa+bC2z1PVrN327G9Syr1sa3078b7L\u002FSPf8ApVcx4444RhAVGIoBhnOU5GUjcpal6hJJJPWv\u002F9P1VQbVW17ntY1r3\u002FScAAT\u002FAFivlhJJT9VKD6qrNvqMa\u002FY4PbuAMOH0Xtn85q+WEklP1Uo1111NDK2hjBMNaABqdztB\u002FKXyukkp+p66q6mCupja2N+ixoAA+DWp\u002FTr9QW7R6gG0PgbtpM7d37ui+V0klP1UkvlVJJT\u002FAP\u002FZ\u002F+0SyFBob3Rvc2hvcCAzLjAAOEJJTQQEAAAAAAAHHAIAAAIAAAA4QklNBCUAAAAAABDo8VzzL8EYoaJ7Z63FZNW6OEJJTQQ6AAAAAADXAAAAEAAAAAEAAAAAAAtwcmludE91dHB1dAAAAAUAAAAAUHN0U2Jvb2wBAAAAAEludGVlbnVtAAAAAEludGUAAAAASW1nIAAAAA9wcmludFNpeHRlZW5CaXRib29sAAAAAAtwcmludGVyTmFtZVRFWFQAAAABAAAAAAAPcHJpbnRQcm9vZlNldHVwT2JqYwAAAAVoIWg3i75\u002FbgAAAAAACnByb29mU2V0dXAAAAABAAAAAEJsdG5lbnVtAAAADGJ1aWx0aW5Qcm9vZgAAAAlwcm9vZkNNWUsAOEJJTQQ7AAAAAAItAAAAEAAAAAEAAAAAABJwcmludE91dHB1dE9wdGlvbnMAAAAXAAAAAENwdG5ib29sAAAAAABDbGJyYm9vbAAAAAAAUmdzTWJvb2wAAAAAAENybkNib29sAAAAAABDbnRDYm9vbAAAAAAATGJsc2Jvb2wAAAAAAE5ndHZib29sAAAAAABFbWxEYm9vbAAAAAAASW50cmJvb2wAAAAAAEJja2dPYmpjAAAAAQAAAAAAAFJHQkMAAAADAAAAAFJkICBkb3ViQG\u002FgAAAAAAAAAAAAR3JuIGRvdWJAb+AAAAAAAAAAAABCbCAgZG91YkBv4AAAAAAAAAAAAEJyZFRVbnRGI1JsdAAAAAAAAAAAAAAAAEJsZCBVbnRGI1JsdAAAAAAAAAAAAAAAAFJzbHRVbnRGI1B4bEBZAAAAAAAAAAAACnZlY3RvckRhdGFib29sAQAAAABQZ1BzZW51bQAAAABQZ1BzAAAAAFBnUEMAAAAATGVmdFVudEYjUmx0AAAAAAAAAAAAAAAAVG9wIFVudEYjUmx0AAAAAAAAAAAAAAAAU2NsIFVudEYjUHJjQFkAAAAAAAAAAAAQY3JvcFdoZW5QcmludGluZ2Jvb2wAAAAADmNyb3BSZWN0Qm90dG9tbG9uZwAAAAAAAAAMY3JvcFJlY3RMZWZ0bG9uZwAAAAAAAAANY3JvcFJlY3RSaWdodGxvbmcAAAAAAAAAC2Nyb3BSZWN0VG9wbG9uZwAAAAAAOEJJTQPtAAAAAAAQAGQAAAABAAIAZAAAAAEAAjhCSU0EJgAAAAAADgAAAAAAAAAAAAA\u002FgAAAOEJJTQQNAAAAAAAEAAAAHjhCSU0EGQAAAAAABAAAAB44QklNA\u002FMAAAAAAAkAAAAAAAAAAAEAOEJJTScQAAAAAAAKAAEAAAAAAAAAAjhCSU0D9QAAAAAASAAvZmYAAQBsZmYABgAAAAAAAQAvZmYAAQChmZoABgAAAAAAAQAyAAAAAQBaAAAABgAAAAAAAQA1AAAAAQAtAAAABgAAAAAAAThCSU0D+AAAAAAAcAAA\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FwPoAAAAAP\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F8D6AAAAAD\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FA+gAAAAA\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002FwPoAAA4QklNBAgAAAAAABAAAAABAAACQAAAAkAAAAAAOEJJTQQeAAAAAAAEAAAAADhCSU0EGgAAAAADlQAAAAYAAAAAAAAAAAAAAGQAAABkAAAAMABpAG0AZwBfAHYAMwBfADAAMgA3AGQAXwAxADIAMwA0ADQAZgAwAGMALQA5ADAAMgA1AC0ANAAyAGMANQAtADgAZQBmAGEALQA4ADMAOAAzAGIAYQBmADcAYwBiADYAZwAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAZAAAAGQAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAQAAAAAAAG51bGwAAAACAAAABmJvdW5kc09iamMAAAABAAAAAAAAUmN0MQAAAAQAAAAAVG9wIGxvbmcAAAAAAAAAAExlZnRsb25nAAAAAAAAAABCdG9tbG9uZwAAAGQAAAAAUmdodGxvbmcAAABkAAAABnNsaWNlc1ZsTHMAAAABT2JqYwAAAAEAAAAAAAVzbGljZQAAABIAAAAHc2xpY2VJRGxvbmcAAAAAAAAAB2dyb3VwSURsb25nAAAAAAAAAAZvcmlnaW5lbnVtAAAADEVTbGljZU9yaWdpbgAAAA1hdXRvR2VuZXJhdGVkAAAAAFR5cGVlbnVtAAAACkVTbGljZVR5cGUAAAAASW1nIAAAAAZib3VuZHNPYmpjAAAAAQAAAAAAAFJjdDEAAAAEAAAAAFRvcCBsb25nAAAAAAAAAABMZWZ0bG9uZwAAAAAAAAAAQnRvbWxvbmcAAABkAAAAAFJnaHRsb25nAAAAZAAAAAN1cmxURVhUAAAAAQAAAAAAAG51bGxURVhUAAAAAQAAAAAAAE1zZ2VURVhUAAAAAQAAAAAABmFsdFRhZ1RFWFQAAAABAAAAAAAOY2VsbFRleHRJc0hUTUxib29sAQAAAAhjZWxsVGV4dFRFWFQAAAABAAAAAAAJaG9yekFsaWduZW51bQAAAA9FU2xpY2VIb3J6QWxpZ24AAAAHZGVmYXVsdAAAAAl2ZXJ0QWxpZ25lbnVtAAAAD0VTbGljZVZlcnRBbGlnbgAAAAdkZWZhdWx0AAAAC2JnQ29sb3JUeXBlZW51bQAAABFFU2xpY2VCR0NvbG9yVHlwZQAAAABOb25lAAAACXRvcE91dHNldGxvbmcAAAAAAAAACmxlZnRPdXRzZXRsb25nAAAAAAAAAAxib3R0b21PdXRzZXRsb25nAAAAAAAAAAtyaWdodE91dHNldGxvbmcAAAAAADhCSU0EKAAAAAAADAAAAAI\u002F8AAAAAAAADhCSU0EEQAAAAAAAQEAOEJJTQQUAAAAAAAEAAAAAThCSU0EDAAAAAAJkAAAAAEAAABIAAAASAAAANgAADzAAAAJdAAYAAH\u002F2P\u002FtAAxBZG9iZV9DTQAC\u002F+4ADkFkb2JlAGSAAAAAAf\u002FbAIQADAgICAkIDAkJDBELCgsRFQ8MDA8VGBMTFRMTGBEMDAwMDAwRDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAENCwsNDg0QDg4QFA4ODhQUDg4ODhQRDAwMDAwREQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwM\u002F8AAEQgASABIAwEiAAIRAQMRAf\u002FdAAQABf\u002FEAT8AAAEFAQEBAQEBAAAAAAAAAAMAAQIEBQYHCAkKCwEAAQUBAQEBAQEAAAAAAAAAAQACAwQFBgcICQoLEAABBAEDAgQCBQcGCAUDDDMBAAIRAwQhEjEFQVFhEyJxgTIGFJGhsUIjJBVSwWIzNHKC0UMHJZJT8OHxY3M1FqKygyZEk1RkRcKjdDYX0lXiZfKzhMPTdePzRieUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9jdHV2d3h5ent8fX5\u002FcRAAICAQIEBAMEBQYHBwYFNQEAAhEDITESBEFRYXEiEwUygZEUobFCI8FS0fAzJGLhcoKSQ1MVY3M08SUGFqKygwcmNcLSRJNUoxdkRVU2dGXi8rOEw9N14\u002FNGlKSFtJXE1OT0pbXF1eX1VmZ2hpamtsbW5vYnN0dXZ3eHl6e3x\u002F\u002FaAAwDAQACEQMRAD8A9VSSSSUiysrGw6H5OVa2iivV9jyGtEnaNT+85Zjvrf8AVhv0up44+Lwszq1b\u002FrH1xnSq3FuB0+bMm1v+lILGbZ3M31u\u002FRt\u002F9C\u002F8ARrm8XqvRcLC+xZ\u002FRsfqnUqrHVsuaKhXawO2V2OybA57\u002FAH+zf6T\u002FANH6ditw5UGNniM9JGEeEcMZ\u002FLfE1jzProUIWYiZ\u002FSlD5uF7Q\u002FXX6pDnq2L\u002FANuBN\u002Fz4+qP\u002FAJbYv\u002FbgXJ\u002F80PrL1eWv6b0j6vYziQTXQy7K2nwsZvZ\u002F23divWng\u002FwCKvoVFTvtl1+de5m0Oe7062u7W10UbPo\u002F8Nbeo5wxR\u002FSJ8ql+LYiSd9HsMbJx8uhmTi2tuotG6u2shzXA92uairzf\u002FABZdasxLn9AzCWi4usxwZ9tzZ+10fyW3bH5DP+E+0f6RekKKUeE0yZccscuGXax5KSSSTVj\u002FAP\u002FQ9VVDrXUDgYTn165FnsoH8oj6cf8ABt96vrmctr+qdbdjW2ChlRNTAfpQIfZ6Qd7fXv8Apf8AE+\u002F3oGcYmN9TVd2PMZCBEfml6Q1MTCyMuh3ScB+yl7t3U84fnk\u002FSx6n\u002FAEnsaz2f8P8Azn6Oiz9ZJ0LoXTcyjqvTs2gXY1eWW1Nd9JoYPSY9ljNr67Njfp1rqcfHpxqW0UMDK2cNH5T+8537yy\u002Fq+0C7qXnlP\u002FKrMubJEok8MpUY18xmDxSlbWhyvtzxfpfNGX7sYcPphH+Xrecs6N9b\u002FqiTd9XrT1npDNX9KvP6VjR7n\u002FZHN\u002F6DaP3\u002FAOhZL1vfV\u002F649J65FLC7EzxpZg3+2wOH841n5t2za76H6T\u002FS1VLdWP1z6q9I62N+TX6eU2NmXV7bREbZd\u002FhOP8L9D\u002FB7FFPIZakAy7\u002FLfm6GIYr4chlGJ2lH1cH+D+lF4nqPR7ndb65i4cjPw7WdX6e9g92u2zKqZp7\u002FAHXVeiz\u002FAE9H\u002FCPXd\u002FV3rVXW+k057ID3DbfWPzLW\u002FwA4zl3t\u002FPr\u002FAOCfWuIZkdQ6N9aqLLsg9btordjuON7rn1w\u002FbRe33frVNn6Z26y6z0\u002F5y1XPqznuxfrZbjY+LdiYnVAbHYd7djqnta671q2R\u002FMfzlbP5v6bP9DWq3vgyAPfhdfmOTlPBekvbxRzwyDTijH05ocMuGf6HvR9L3ySSSmcV\u002F9H1VYP1kwPo59Y4ht+3Q6H9DcI\u002FPrf7f\u002FVa3lGxjLGOreNzHgtc09wdCFFzGL3cUoXRI9Mv3ZfolBAO7ldL6q\u002Fe3CziPVIBov8AzbWn6Du3vd\u002Fr+l\u002FRqXRG7bc\u002FzyXn8SqlOFX6lnR8on2k2Yd35wB9xj+tHvb+\u002FwCqhY+Tl0VWikktfYfUymgu1HJbP730\u002Fcsn75PFLDLMDKOM5Bp6sonGPDLFkH9T\u002FO\u002F5tljj4tBv4u5mZ+NhsLrna9mN1cfksbIxuu9d9j7D0zpzuWsn1rG95d7Xf+e\u002F+LyFe6eOmAiwWi28mS+3R0\u002FyQ9ac91ewynzI455IjH0xYZcf\u002Fh2aP\u002FQgyiYwH0QvJ\u002FnMg+X\u002FAGeOX\u002FSk8VjYnTej9fyb6a9uH0bFAEGXPvthsucf8Ld6j6f3FpfVHp17vtHX8\u002FXN6kdzefZTyxrN35j9rNn\u002FAAFeOs3ExHdYz30OEY99xy8ojksG5tNU\u002FwBvYz\u002Frln+CXagBoDWiANABwAm8kfdvIBWOMpcHjI\u002F95D0tvns5hH2+Iyy5IwjkkdZe1D1cP\u002FVc3Hlmukkkr7lv\u002F9L1VJJchj24LKar8bJd+2bM+1ja67XPfZGVay6nIx91jfszMXf6u+r9SqZ61Xo+jWkp6LqWA7Lax9Lgy+o+xxkSD9Ju5vuVfHxOr49QqqfQ1jeBqTr4+1Zjeo9TdQH4ORTTTU7DpZW6t1m77W9lT7bHvtdb+ibcx+NXv\u002F8ADG\u002F1P0Ic76wdUxScX7RULMYXk5LxUxtppczYzI+034tVTGVWs+3fZner+k9Wj7LX+jVXJyOKeU5gZ48khwmWOXBa8TIFUCPEOxd07PvaRY3Hc5350EH\u002FADmtVdvSOsVNLab2MaQQWb3bYOh9jq3NQX9V6w0PyzkYzaxkXYgoexwrYWMsNVt2Tu9X9FkVfrFm30fsn+Art\u002FSKpV17LNTbn3Vuvpbayy+1oLKg53Tt17vsOQ7EysXGqy35Vln6P9D6dd1mH+s2qL\u002FRWDi4+LJx\u002Fvcfq\u002FxmWPN5AKAjXYjR6HpHTG9PxiwkOusO61w400axs+7ZW1XlzdPUepZGUem4\u002FUKbWi0tOe2sOfAq9c0NaHfZXZNL\u002FS9W3Z6X2e+uv0vtH6RDwfrBnW9Rw68q6trMgNrNVDWvmwts9T1azd9uxvUsq9bGt9O\u002FG+y\u002F0j3\u002FAKVXMeOOOEYQFRiKAYZzlORlI3KWpeoSSST1r\u002F\u002FT9VUG1Vte57WNa9\u002F0nAAE\u002FwBYr5YSSU\u002FVSg+qqzb6jGv2OD27gDDh9F7Z\u002FOavlhJJT9VKNdddTQytoYwTDWgAanc7Qfyl8rpJKfqeuqupgrqY2tjfosaAAPg1qf06\u002FUFu0eoBtD4G7aTO3d+7ovldJJT9VJL5VSSU\u002FwD\u002F2ThCSU0EIQAAAAAAVwAAAAEBAAAADwBBAGQAbwBiAGUAIABQAGgAbwB0AG8AcwBoAG8AcAAAABQAQQBkAG8AYgBlACAAUABoAG8AdABvAHMAaABvAHAAIAAyADAAMgAxAAAAAQA4QklNBAYAAAAAAAcACAEBAAEBAP\u002FhDNdodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw\u002FeHBhY2tldCBiZWdpbj0i77u\u002FIiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDcuMC1jMDAwIDc5LjIxN2JjYTYsIDIwMjEvMDYvMTQtMTg6Mjg6MTEgICAgICAgICI+IDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+IDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiIHhtbG5zOnhtcE1NPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvbW0vIiB4bWxuczpzdEV2dD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL3NUeXBlL1Jlc291cmNlRXZlbnQjIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iIHhtbG5zOnBob3Rvc2hvcD0iaHR0cDovL25zLmFkb2JlLmNvbS9waG90b3Nob3AvMS4wLyIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bXBNTTpEb2N1bWVudElEPSJCRjgyRkJFNzZFMjA0MURDMUQ3NDgwNzdDRjg3Qzc4NyIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDo5ODQ2N2YwYS0yZjBhLTZiNDMtODU4YS05NGRiNTJlODIyNDYiIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0iQkY4MkZCRTc2RTIwNDFEQzFENzQ4MDc3Q0Y4N0M3ODciIGRjOmZvcm1hdD0iaW1hZ2UvanBlZyIgcGhvdG9zaG9wOkNvbG9yTW9kZT0iMyIgcGhvdG9zaG9wOklDQ1Byb2ZpbGU9IiIgeG1wOkNyZWF0ZURhdGU9IjIwMjQtMDEtMjRUMTk6MjQ6NTIrMDg6MDAiIHhtcDpNb2RpZnlEYXRlPSIyMDI0LTAxLTI0VDE5OjI1OjIxKzA4OjAwIiB4bXA6TWV0YWRhdGFEYXRlPSIyMDI0LTAxLTI0VDE5OjI1OjIxKzA4OjAwIj4gPHhtcE1NOkhpc3Rvcnk+IDxyZGY6U2VxPiA8cmRmOmxpIHN0RXZ0OmFjdGlvbj0ic2F2ZWQiIHN0RXZ0Omluc3RhbmNlSUQ9InhtcC5paWQ6OTg0NjdmMGEtMmYwYS02YjQzLTg1OGEtOTRkYjUyZTgyMjQ2IiBzdEV2dDp3aGVuPSIyMDI0LTAxLTI0VDE5OjI1OjIxKzA4OjAwIiBzdEV2dDpzb2Z0d2FyZUFnZW50PSJBZG9iZSBQaG90b3Nob3AgMjIuNCAoV2luZG93cykiIHN0RXZ0OmNoYW5nZWQ9Ii8iLz4gPC9yZGY6U2VxPiA8L3htcE1NOkhpc3Rvcnk+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw\u002FeHBhY2tldCBlbmQ9InciPz7\u002F7gAhQWRvYmUAZEAAAAABAwAQAwIDBgAAAAAAAAAAAAAAAP\u002FbAIQAAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQICAgICAgICAgICAwMDAwMDAwMDAwEBAQEBAQEBAQEBAgIBAgIDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMD\u002F8IAEQgAZABkAwERAAIRAQMRAf\u002FEAOAAAAICAgMBAQAAAAAAAAAAAAAJBwgGCgIEBQMBAQEAAgMBAQEBAAAAAAAAAAAABwgFBgkEAQMCEAABBAICAwABBAMAAAAAAAAGBAUHCAIDAQkAECBAUDETCjARFhEAAQUBAAEDAwIFAQQLAAAABAECAwUGBxEAEgghExQVCRAgMSIWFzBAMiVBUXGBkbFCIyTVJhIAAgECBAMDCAQHCg8AAAAAAQIDEQQhEgUGABMHMUEiIFFhcTIjFAgQgUJSkWKSM9PUFUBQwdFyU4MkhBbwobHh8YKiwtJDYzR0lKT\u002F2gAMAwEBAhEDEQAAAN\u002FgAAAAAAAAAAT3YWuaI7DQtYCKrLo32DY4K1vbmh6HOu+zBML\u002FAKAABiuTwmv7crn6s+QfLSD7N22LSa0ja47yujHM17Hhx7F75NZrGAACepvpPVWboDxH37azGsF+tZiR8nsiQfL9CPHZFXmFs9uuOUGW\u002FwBYoAh\u002FH4FC3ynr\u002Fc7bxZUp1VcDEFzuIhXJWyTdEfQK+n7wt6WO\u002FfZ3lTlaAUzi\u002FUqaQBhXLXPkdUVKtNa9dXcgwXCbHra1L6u0y+T36P7+XdAuTwOAAQVxCxdjZQz+UfM2zzrPrdI+bFkqZeOdfXm3F0MjOx747u83rTb5DgAEFQD6KI859lhynciTdJm0MC6\u002Fw\u002FK1j9GRnyk6OdmQ\u002FM8PoxzsAAAFn8epHjOtW6yVuefjeAJnye2+uUlh2wD\u002FAPvdy8y7Y8GABCp8iVcB+8K1e2rs\u002FjkOXzJR17vbMUs61LkwaIAAFEipxhhxItLRmBnhHrmcjiwAiA+xkR3SFyWiPDqEpEbk+AAAAAAAAAB\u002F\u002F9oACAECAAEFAPwIDjNCYOchqIjjvRxJ0ILckrSNO\u002FD6iAGbx+HWAhYfpmaHB\u002FdpbcEcWx0xDerXEpby4x2yEUqHJDllllnkAOuvFgKkunS5\u002FMPq0IurZiUWFFTwVOJ1XQFlowAsnhyjmVeH4Wc2Da0umTb4vef40+GeOzD26OSZpRjxqrW5rV6tx3NElakcW+ft4penhQmfyxWy8sTqkJBwFKv5l\u002FuXWxyWh8VWKHUO\u002FjnjngnmXY1W89EBIwijYX2M\u002FwCr0RsDoTcPhZDkTmfxa+PMolmKEbKmFcjE4W6FPZqqVJkKaZuxwJZneOK1TjMrpZjkWh+DDQvWDUXxqEpwAP8Aiy0KIZ4ietqYTs1F29Wb07tC3msv9hRdAlU4aro3edhMgI0hzUQXeZPLvq7IW\u002F14mqB5XgI+m+TxPq3NvEcrm1fMYv7VSLXwavSq5VphEVZggY+ikWHDZg5oRUnnzKgFQ8vMOv8AqFr510Rqhq8jOAogh5T+l\u002F\u002FaAAgBAwABBQD8Cf5VVBSGKM5ROlSIWPEeKfYr1cNeOW3ahANZQNft9EpE0CQ\u002FX3Fym+T3ExxWy7HbnrMdyJnb0OHlVU6Z1GrNxXkDlPzaZ4Wme1a0GslaBkdaYzte9hTA+bmBpc028kio4GGujbjozkoLF0UxAz6yOo09eyZ81DrK0Ry8HjwKibEGNBMF4qrb+oZtaSx7nD0QwBKJzY3EvgG4t\u002F4SQOjL7MmXl5ahBfy2aPNw1ioLvQoIE5y914pwPRK82HvdIENyxfCdFIhXb41sujLambduadMg\u002FwBsOGGezOOa9Ln3BdZ2P4XZKlvZlLlkojZWsrn2epcc5tk34S7uNG9Zp3NW1A3pSZgSJWWMkJdIpUac+UhZ96EXtPIDdH0ffQnnoe0DozkCRmGls0NeSEaYjjh9qgzruAjhLAEKkxG7FxB9JlKhHv1n5fq51SmfafNc1yhq8wn2XdfhbJp4dJv0v\u002F\u002FaAAgBAQABBQD8DtwuqZV3jyp4b2ZXfdznrE7vRtstPYy4db9cBzd3r26eKNdkV\u002F8Arj7COOeMuPk4NB2OQ6mUcPFxbfzzKRJj2GRiHxV2CzPW3o46xq6bEDegakP9kqv2LNfnoTvOrsnWj57FpWcJAJXRnObQs4fXQUrj2yXv6o6k39ZiLnuu\u002Fr5unX93AVB7CGj+zFEK0pqZLJKQUYs6AnQnKAR7niXWmD4vh6DpKtNIMSxECwmGnzPpUdrPmeGG3C8HQVWOxCu0NgexquVd+vAUii8HTj0L2rJgF+93niV3lGFKT25YQBJxzxzwbcc8dnfqR5Oj2IBK8naab2YFKOdN8IWnrX020+bZQvL8WpgBkiCboBn40rcSFerQq7Fd27Un1TZchsE8W7r8mK1Rd2UiEWVnoZZsrIo3pbTOsjBUmv3xYuG0M4RhXtuGZ\u002FjNtSlFa5yayCRbWkEbwuAxdo87aCNG9SB13wm9TVNv1aEMdIal2IzGLiqUigbpaQ+ORqVRZqGOwkkZ8pbwcLkWnAgccjQL+JlsZBtek41ZWvxlGj4xMRayYVVr\u002Fr8wrBBGHm6scFb+DKv1Pw3yOYHiKJV3zOvOvi89vzceB7HzqssLsELLn06UR5lgwuJBIHNYpPNeykNlOSI1WaTaT2+nBPrn4dhVnK5Mje1nuXIAhOe0otAsIg4EsjSO3BndwUJIHIDp1VeMWU0jKOJITx9VmuUVM6SpdZkEt7o3j5QMNNWK4MMsf5\u002F\u002F2gAIAQICBj8A\u002FcGobk3LbB9qaaPErnLHNOVLBHII91CgM02KjKEVjlc8aRdav0rtbixvQ3LmtrW2MYZQGyMXZcWRlkQiodDUE0NEtrHpI73LHBVs7RmPoAWQk\u002FUOEkTpL8DbsKhrmC3iY180QZpT+SPXw37R0zS4G+6Y0z\u002FUi1f\u002FAGeP2ptuyjjmCs0fLTJzMrEMjKO80OWuIIXEAmvladomlQGXUbqZYo1HezkAVpWgHax7FUEnAcaD0h2o5l1nUIcspQVd4nak0mUVYtezAxRilfh4yo7Rxo23utS24s7Tlls8jJyI1cLapPKlCksefkvkagiZY5GqGpz+mnTayubRoi7vAyqyLTwuEiBuLqMjxGRJio+0KY8Spd6u1vAxxjgHJX1Er429Od29PDO7EsTUk4kn08XVvM35iYkfyXAOP1huJbm2p8PMc1BSgY4mlOwE4js7wBQeVc7uvULXSIyQKBVwCKSMg\u002FnHB5MeGGZz2Y8at1a6hyC633dE\u002FDwIQwtlChY7a1rVRIiZUluiMkK1WIM7FpN4bp1VEW5nupFVEFEiiS6thHGveQgJGZiXY1LEnhLfTr3n6Lmq1rNVose0x4hoX\u002FHiZTXFgww4N0sB03dT9vsh3Y+kBY7oekiOegxqMeCLhRJak+GROw+sHFTiK1FK4Ani7GekciUPrBwr+E8RiQloPsmtcB3d+OX2QB3EVx4V0aqEVB9HkSXl04CCgHpY4AD1\u002FwCSp7uMliapHTMR7K+YV+8fMMaYmg4595OzyUoK9gA7AB3D0D19vGvdPXlGecllXvBM0MlR9UZ+io4MDXzMQKAuS3cRQmtaY9ta4AVoKcTWur2jRxygiOT7BaoplcDKe0DI2STzBhjxdwSXaRzpIyAlgCrDxxtj6x66EcXm19RIW\u002FjLGMVB9k+8jBqa5T41IwKN+L5F1f6QGe+sD8Ryx2yoisJEA73yFmTzsoUe1xabZ3Tdw2+mTuBb3ZIVEdzglyewK5PgnOCkhZSFo4BBw42l0oUuUvYIyQPZFbO6mNcfxF7u8en6ZtZ3Hq0FlpiYGSVgoJNaKoOLu1PCiBnY4KpOHGr6T0\u002F0R59IhhZ7i5liz5YVqGk5ZBWGMHLSSarHMAI0kpxe7w17W5rOyhllUFRGVEMCgySMzqx9rN2GgCYivE2r6c0v92dMdmR3pnkLqyQo2UBMxQtJJkwUAL9oHybG31LUriy6RbjleWKeGLmtZGQ5bqIJ\u002FwA1baV1uBAAC9tKIlbOKrb9CfmIuebtBljOmaurmeFLaX\u002Ft5o7mn9a0mdaGCX87Z+KGRVWNobXpdLbXCS2stlC6MhDI6NpV0VZWFQysKEEEgg1B4uL29uI4bOGNnkkdgiIiAszuzEKqqoLMxIAAJJAHDbB6BaUd376klEKzxrI+npMxKqsIipNqMhYAKtuY4HDq0VzKQYjadQfmY3bdWkLUaLT0ZPiVQ0bJkQfDWEbUjYxojSsc4mggm95x\u002FdfaelxWNtqd\u002Fb2qpEDncLWZ3kkYtJIxWBY2kldmowBbs46VdENto0m6tVs7ee6jjxcfFvzo7aoK+8lmkq4JVuUi1BSXjStuxsr3arzLiQCgluHoZX89K0RK4iNUBxHk6\u002Fsx1RdeRfidPlYD3V5ErGMFj7McwLW8pxyxys4BZFpuH5V+skLW3UTaZmOl3TBfjLWEPypIlBIZ\u002Fg5wIri2cqktu8KKEe2SaLad91Wgm1ibbULm1WO5zLcWT211BZ8iSQSGCDNJ+baPNCodeVUBTc7c3l1r2\u002Fs7p5FcoBpZukheatWT4awaZJ9UnQopeS4mWKF2LQmEsICIth7dEm4nQrNqd1lmv5q+0BLlVYI2oM0NskMT5VaRXkBc8dLdnXNZbC0t3vbmJWoXW4mEagkYqwW1ky94Ele8cbo+Y3esVWnuZI7BCAVDU5bOnhB5drBltYO1RV1weAeXsX5uem9ows5L+NdTiQlUa4ClXEmUEJDqVoJIZmphKsjlubOnHVHrv82Gv2We9Z0sdMubO61GGk4ZBmWK0nj5VhaJHbW3MozFhLQSQK5ubza3VrUNr6s9SDaWesT2xY97211YStlH83bz26juAGHHO6BfOXFuPa1uQFspbbUrchagELpmrWk1kgIwZre4aQAYEEKeLe06q9PLW8jwDXOnSNbyADvNvMZ45XPflmtkr2ADABNjxXC6LqMltDAZowGtrSC3jE8syo8igIUnmYLJRiciEsy10PaO37flaNp9skMSk1bKgoWdvtSO1Xkc4u7MxxJ8vU9q7t0a31Db17Hkmt5lDxutQwqD2Mjqro6kPG6q6MrqrDHo\u002FCf7fqv69x4ujkH\u002FAL+q\u002Fr3FU6OQA\u002F8An6r+vce76Rwj+3ap+vcX97052Pb6bfXKZJJeZPPKUqGMay3Ms0kcbMqs0cbKjsiMykqpH71\u002F\u002F9oACAEDAgY\u002FAP3Bpe19uXJXdupmqlRmeG3DBC6Ch95NIRBDgTmLsozICNUitOo9ylzZMolgubi45lCSCygK2CuDG4NCjihAqOA+obuVox2kzTU\u002FCyjgK+t85x9xmYflEAf4+I+fM3JqKkk0p\u002FD9WPEt\u002FoUGa+SNnjCrQyGMnOhH3iActftBQCATXytZ3Pr12sGjWFtJPM5I8KRqWNKkVY0yovazEKMSON1dd95R8jbumzloVkOWOOWNKwQ5jRcmnW5EszVp8TKHbsPGtbh6ISXDzXZkC5I1b4iQqWungieueKTJzUzjM0gaSNaFKht17quFv1fJklBNWB8SlmPLhYHDIYwT3GuHCiGLMR3scx\u002Fi\u002FAB9GuNcMA2m3eYk9ixSR58zeZQY5CTSlAa9nFvujSrem1tbrKhUeCK5oGmiFMAsgYTw9mZHYKMsflaZ0i0O6jSB5Y5b13bLEGHjgjmY0AhhAN3PUmoSJaVNDt\u002F5feiKNZ9KtNC\u002FGXsqshvXLl5b6\u002FpR+TLLnktrAHm3LZXnKRIFh2X050eeSS2gsIXaSU1knmksrtpZm+ypdkrkQKiCgVQBib2a0EWqU\u002FPRgK5p2B8KSL6HBoPZI4tNKEvNjd1RTjTE0GBqV9VSvEWvX+hzPoDgf1hFLRqTTCWmMRxAq4CsTRGY8a\u002FtS7ANrqWlu2U4hmtXEhUjvrA09R5q93HVj5a90XAXem17yaC1mlOZ+RHM40y8JoCRDmWzuAuJtpImZqtXjVdva5ZPbazZXEkE8TijRyxMUdT6mBFRge0Eg+Re6o65pUWkaffkbBF9RPb5lBPdxdGSQ+9lMl1cuKqpc5moD7ch7I4+wAKWooxh0bQLMRWqnMx7XlkPtSSv2vI3eTgBRVCqFUbT3eqESQwRqSOwgWl2lD+X9Nvou8tP\u002Fb2zivLZHYLdxRnwkRTMGWVApNYLlZEagRXiXHja3Wz5eNzRWOvaXcc7UNJCFM1vLG8NxHJp+fmWTMkhVLi2a409mUjKpOYaJ1V2roF5eaZqWn2l1cxxRSOlzCyfBahbNlBXM4gzjNgkrRS0JA42v80WwImm21qsFtHqDZCje+QfAX0qModHmjHwl1nAKXEUQIzS1P0qI6maB+YB96gIYevKSR6RTv4TSrmNRaVqjgUoWNSH89T2Oce5sKH6Itw8sZokUV\u002F1HX\u002Fe+m025s\u002FQrnUdcnPghgQuxGFWamCItavI5VEGLMBjxtHqF1p39yN4y3kcem6dYXht1a8auSB7yNhNeSnHmWtgOSFDCa6aIsp0TpX072Rpus6ne2lrK4la5SQ3l9Iwgt4UtpoUFY+SaMpYtKAGCinC9Pt0x2MnVDeEEMNxFb52trdLeWK4v54eYzSmNbiNLS2aRyzsJpakR+TIUi8Jxp5vP\u002FHTzV83HuPFl7u8U7vq\u002FwAO4m+uyniXD8DAfw8JHGpaRiAABUknAAAYkk9g4XW9+aj+yNtxrndSUW4ZBiSzSe6tUp2vNVhT80ag8XOzuge1rWS8YUlvHVuQzDDO5alxqDjHKZmjtlNDHHImHFlvrfOvXWpalpNhcXCyTPVY5HUWsEcSCkcKB7jMkUSJGpUkKDx1o+abecwg2not\u002FdW+nSS1yL8DFyJr1a4FLS1jAhoSDdTLTxx8a\u002Fvm+Vo7ByILKAmvw9lDVbeHv8WUmSU1OaaSRvteTHIR4a4+r\u002FNxb6rbLmtZKZh3HCox9I7D569oNDcRacwi55AJK+ywZS1QO00GFDQ+fhNStNu3V\u002FqpUkShCQtO3PKAVgU17EUuwwJ+1wsWragV0xDVLaOqQL5jkr42\u002FHkLviaEDD6Ooe57aTlajezR2cEhFQhiiaUuB3hZZoWI7ylK8bW+XXZMvLgFrFJflT4uUG5scchHbLeXBe+uamprEDUMfLvNvXjDPkJQ99PR5yjUIHmoOwHjTdF2pbOXWheVXWNvDjQEspq7Es1MMKdhpwiy7fivoB3SPCj08wkSVfwsH4WHe\u002FSd7O9cYzI8Df8A0W0iSj0cyNh68eHm2lumW2lxIiu0EqegCaIK6j1xOfTwJNyvE82mrPNMI2qs9zPK3JhjYhc3MHKjrQEKGY4KeNX3NrlyZdWvp2llbuzMa0UEmiKKKi1oqAKMB5cVzazNHcIaqwNCD\u002FowI7CMDhxWPWWB\u002FkRf8HA5W4nFP+nD+j493uyQf0Vv+i48G85R\u002FQ236Hi0st17lnu7KBy6RkIkYcimfJEqKz0JAZgWAJAIBIP71\u002F\u002FaAAgBAQEGPwD\u002FAHDMcb4Zdk1HfO0SowG5pworPQYnCxnxVRdzQBTwFxP12vvZ4qWk8wEPSeWeaKNZR2KnSs1yj9xjofP9jywgFulxHU+yddrdPNXWExgH67WC0FZbq+vrb6uJrTmSpFMGfD9uRrffH7rDSXX7uVPjM5WxKRYXmj+R\u002FfqKmrYEXws51kdkoQQ4W+f+J70T1cCs\u002FflP7nrKkuUSfKfG3t\u002FyD31WLJC5qELedRLoaTl9DHEir4\u002F5kS9VaqexPKL6gr\u002Fi53b9y7swchUME+pyPS9+BzkFs7vEUx\u002FUtTYZ\u002Fm4w6u\u002FqqWjnoiKvtXwvqP40fuS9z6H0TA2t5lOedzp+rdB\u002F1YH4\u002FadBpKW75x1nEdAJGbZJnq+W+DhvIYJJK6UAsmZIVnCjl9I5qo5rkRWuRUVFRU8oqKn0VFT+bT7zW2EdXmsjSWOgujpXMT7IFYNITMkTXvYkxUyR\u002FbhiRfdLK5rG+XORPXQ\u002Fnj3OCOrwnKbv9Wy4l3M0Wlq9LR1z5cXTKbP+OHFV8ZyczbM2VXpH+vGpO\u002F6seqdG7P8AtfybVmx3w+rErv0HO1F27oOgsaaey65bYTHX7SBL\u002FMXyUDLoJpo7p5bUeYwOBzZIGvmyH7lP7jXXuVW8GjDz+cpOpVZ1vS6W1dO9LDNz6neFR8d4PpwTfA8FZZZqAghX+YZEkRInU+jz3x2z3XtfXtQis6H346TtV0xs8bHRE1NfqUJwtGiN\u002FvidV1ISN8+WeERvgSsqwhK2tAHiEBrwBoQwQhYGJHAMIIOyOAYeGNqNYxjWta1PCJ49cx6ZWVEp0XyQ4VTZwwdkKvddbbmepKyTKsVjERSTbPO7WoHbH5971a1qf1T1P8e+m3ExXefirFX4qyfbSql1sOUxPnqsDqymkPaWXdZ79Mmz14qsV0VhXMkmd9wtE\u002FmzXxZw9oAK2Q6rvekWh5zAaQMt3g\u002FO1OgNlWKAekz4cTr608vVPtQjM8K93tXO\u002FCj4qflZ\u002FwCPOMhHdvN5ZgEVzOhEEGSGW3Q+kxxuiNSivrf7xdRmVkQu4k9kpn2h4mxjfHbm2HWwJpM1l6wwy3tpnT2Wh0lxyzo6aHRGu9344pFoTCxyDDNjFHY1rImNan1siOl5OXFdYlrJK+p7jzmOupt8O1o6wiBamMkIyg6Pm4XIxH1t6KbEkSK2B473fcRDs7Az5t\u002Ft7VZzVIDWLTXmHxWaQp6OcQPEum6n8Tj2QkefvDf5NztCJEV8cL3Ijaetwmmn5l2kuuYbY8F6cTWVW2crIEkLMw9mIYTmeo51j2SOjMpCiJWwMSQocRXfbTi\u002FaqJZodFxHuIQgxw3ujJra\u002FqNGZnobCIlio8ZQtrVUUrXJ9UkY1U+qJ6+G37tvEKZ0nD\u002FAJh4HLbXqeQzQ6AVDtnpsrTkfJTlMYySkwjTbR4U2wo1ma2OPSgGMijRsaNXI9Hwd2HpMVus5T6zK39fJ9wO3oL4CCyqz4HKiOa0gQhrla5EexfLXIjkVE\u002FjpOg2bGFEV47Qs9UuejH3uosVUeip4\u002F743e0oxUdM5F8xjskk\u002FoxfVy2IuTwfcT3nUulWAyk1tQXbz\u002FnGRjwSKxltojGu8A13u9kMLY3TqyBie8HD4GpSuqhXOJNMnehNxf20zWIZe6CyVjJbO3OcxFfI5EaxqNjjbHExkbedWb40dKLn6NY5FRFVq\u002F4BvYvov9U+kq\u002FwfHIxskcjXMkje1Hsex6K1zHtcitc1zV8Ki\u002FRU9WHTPjr+H8V+5od\u002FkMBGPrHxck0ukgf+VBZ3WEp56ubGaKUyJipe5ietMher55YTJF8L1b4HfuMYG06nzvpuaZmuPfIY60jtT6rYUF1V6XHXdT2JtV+ldWrRLGmjlJqtBBT7SKGZFdPK1qRv1fxB65ssrlL7m+63+NwNxd29ZXWOF0IdsvTuO7MBD3tIUapK1CDvSJPcSBCUL5Rr3eujftpd1ngrNzzC72dzyEZ1hDYCRMpbSVercvpbAYkgE2sztvK6\u002Fo\u002FsK9s1RYEKjvYMjW\u002FxIMzrZyr7nVom6DqIv7v1sECuPCvK+KJXNR9glOdNML\u002FAFV08SMRP7\u002FVXx\u002Fof6VUY08t0mM3kA8IA1cfbzJN+l7eVjWMeLYkzJ+NbyeFic5sJSoz2TNRUVFRURUVF8oqL9UVFT6Kip6wrkYrkXP0f9\u002Fj6N\u002F\u002FAA+2RfK\u002F9f1T\u002FwAf42e76fsaDDZCoZ7jr3R2ENeEx6se+IQf7rvvHWBP21SAaBshE7\u002F7Y2Od4T103iHxR4r+tcsAy1nada6r03BwaucXCB\u002FbSw0IHP7ceegwNKxytUW40r1OdI+NRa1hCRyJrfkd3Lq245bQZLTbSor56UHIG1sOE53WDT6HUXhusz14c90Vo05vuikZG2ENVVjnuV3p\u002Fbedv1tf8efi9b3l7lLrUsAF1ulsNPSXmZ5pmrtKgcSpgtScrakXlxCLBHEPC4UZUas6\u002FwArIj3l0\u002FHemGGXNbaVoLTf8SebKrdRWsBlRkB42dsDG2DQ\u002FPmasndExyPjTwLwX5AkfmY2KER2F6BEXJa1tfQGKjaU8C6cjn6DmljErfxCneSadf8A45DUjZ4HwtkPJFPDNnqSYciF7ZYZoZMZrFjlilYrmSRyMeitc1VRUXynqUgiWOCCCN8000z2xRQxRtV8kssj1ayOONjVVzlVERE8r6XOchpv9RtodJ+DXExRGFZyI57vtxxhwVjXWusK96eGwheyF3lF\u002FITwqegem\u002FMjoehDqoHumqMJXGDN0I4cyI\u002F8IGEVs2Z5mBO1WpI0CIi2lRFSaeGX+71bce5BjM9g6HpGxyuXLqqINsRdtWgFv2GlsrexlWa0vbGeuzH2ZyzJpyZGzI1z1T18U\u002F26eVCyW3Ser4\u002FG6XrFTTIxDzXdFun6Sh54T9hXTwn7rZ2znno9rXx01e9XeYp09Yvj9Q8c65Ejm0PQNFBGrF1fRL9IidTeu8tY78dxLGChtVGrEALBH4\u002Fs\u002Fluck9sEV8L4u8fYytav6fpq+OVwKPe5W+0OyY94hP18fYncvhVanq1+N\u002FVoJ67b8wcfNgbyVkaaTPV8ZDq86tGdM1HE\u002FwCMWbfwzBJHLEUA+BqoixtkZnpegDzaEvACvZUxA2bXj2udnq7iszrKkkx0z6qqbKW5UHlb7xUa9jWO8N90tbp+h5TBZOAqFEyLLNgriWv\u002FAL4kr6GaeA7YGR+xPfOXKwaF6+WRNVfYrXZ2oZNcvgSAvS2iRm3xbPH90SGLGxoIrvp5gGbDCvhFVqqnn+HE8AfAtjSZWtsd3oKyCVI5Dm3duHTxAPk+v2JC6ijOia5f+BpCu8L66H84etDocbJoLms5tCTE1REu5Bm01pb1sL\u002FCRVODzMMGcqEaz7bEYS5vh8aL\u002FPkfkfhx1igOug4dSLC50Izr1IvxpWFLCxPs1+1o2PEId9V\u002FKja\u002F6vkT1u+r99uKv7tvKTFRZW1qbPQAvbYsWBrnxQVp47gM3TQxBCJKiOV7nSoiORF9Pmz21Pxha\u002F3Rtp63TmVLZfPn3uqbelLjY3z\u002FAOmCSBE\u002F6PHpCOW\u002FIAbSVYvj7dMWDejucxfDU8Z3VVp9W5Hp\u002FX8YqN6In08eE9MG6JgAb0dv22Ptsca6qsEai+HzS0tw8kEiRU+vhhkDfP8ARE9fYwsZ4g25IoKGknshVhKz2Tz9SP8Art9ZCMlIaOlYrjCPYj3NfLJGxFVZE9ZnA5EFtdm8nTh0tSKntV7RhI0Ys5D2MYk5hcvumnlVEWWaRz1+rl\u002FlzRPaunZbnTdlYH1WTg0Br4ztGfVV8ltaj0taNETYWK1dZGpBLoonMHi8OkVqKnnSdjyvZec33LsWNYlbPb1mrqCaHGsqAY7O2i1xTCfOYMq66Vk5A5zYJ4Intc9jUciqbRX9aDeUNwMkJtedCwkIwd6slZ7mORUVWva18b08OY9rXNVHIions5yA3x\u002FT\u002Fm+kX\u002Fzul9f2c+Cb\u002FwBlto\u002F\u002FALj17ZefhPTx48LbaJPp\u002FwB1wnrNruc1k83\u002FAJlrKbB5VbzV6Cu\u002FyHZ6JSEosvUff0Ef513bqJL+OPH5kl+272ovhfVjac8wtRm7S2HYGfZwvOOspg2SNmQJp9qWeXAE6ZjXuhjeyN72tc5qq1FT+X4EI5WJIue+XX20VU96omC557\u002FYi\u002FVfCKnnx67Xv\u002FjxfhUHYcL8A\u002FkbZ\u002FKfUY6KMyGgIranLmfEJ\u002FS0eKfkH9SrtTBfz5mG0iltWZtLXyz8B8SO\u002FcZ+SuS+V\u002FWMhrvjR2K2zvx4wg8mfn4piKzNcb4vdWtX0HnsNZX\u002FAOr1brNPrrAwht0a8quimjjrJwnMWV\u002FWaPCfITpnTw9N8Z6fo8l33S5zO00HNdxD8juPcZ3PXMlbX0GdyOXzwmF68XdEU9ixmOqD6QadIBa\u002F86KXv+ezPUO2cfbb\u002FHoPSZm3+R3cfj38h+yYjqqdu4\u002FzrM9V5pn83c6W5IwumqdnaxaMC1D\u002FAMUBtAwIwGwflkwSWnL+N\u002FLfdyn\u002FACC+OfabbO6P5P66k1lZzvtfMdZxwSm1+f19lSjMwwnQ6XqB1RKDEEXQ1VslYYFUqkRARmN5URr\u002FAJU4nqAXyj+KMfUeZfIroXK+3TB8664D1CuBE5r2CiFs36fEdWvebHDyMPOHtayxrZfYLWQStSbnv7iAXy56Vqu3bG253oX8mhua5fjxp9RuejUeFuvhxRcOjqjZQbqisTS8aGaKSm1g1YP5hhsytLBk+SfzMf8AKvsl1vOOfKHv\u002FwDprzb9QpK7honIeVfJTU8\u002FD4pq+cVtPCNtYLzJVZAhF+aQ7RCkSwShlj\u002FisY8Tad76r3EnAdO+Tug5ZxToPKesc12fxUvhdMRqKTm3x56Nwv8ARU3HN9tm5aNaiwvgRLCQnVgvnLuwoCvwIf40AfZuXYvpUGUPMtMwutow7UjPWFiDJWWJVKZNH+XWS2NdK6Aj7L2JPEvtf7moierjlmP5JzjM830TD2aPD0mNoK7MaRbUSIC1m0VOMBGFfE2gUDIipi2TSkMaiSOciJ62OfPwuRNouh2JNvvaYrPVU9ZtLU0GtrC7LUgyiuGvjiq6nEgklJbK98I0TFVWxtRCLi+yOZu7UvK3OFLsLejrLIwrE6KcMq\u002FyJM5g00hGZuya+B5YL1UYl0LFkY5Wp41Ocwnx+5RnaPbPq11daLjacge9GobCO1z1SchwxTn5\u002FNWUTZ6utRUArZU9wsMSqvmQToWBxu6Emob7LTCbDNU2kFmzWqfUyabPzDXAZkEtLoJKEFxgzmrCS4OFZGuWJntjoufcU5zmKyLYVnQWxBZitmIXc0kLxqLWSWBsJVhNf0AkiwV5L5XSgj+IoFjjRGpP3cPhfMxuukWRN7Ju4srWNuU0hsUg52th\u002FwDZ\u002FDH2Z4sr4SLmOJtpPA90chDo1Vq3OJnxGTmx2iOubS\u002Fy0lBVvz11ZaO5J0V+faU7hVAPMur8yU0qSWNzyCpHSvVz3K5Su60vEeaVXXjSrWxJ3wGTqRb99zetnjvtE2eEdkUOpvoCpYjrVjG2JkMj45pnxuc1f9v\u002FAP\u002FZ",nav:[{name:"Home",url:ci,thispartype:t,sort:d},{name:cf,url:a,thispartype:d,sort:g,children:[{name:"About the Journal",url:"\u002Fir\u002Fabout_the_journal",sort:d},{name:"Aims and Scope",url:"\u002Fir\u002Faims_and_scope",sort:g},{name:cg,url:"\u002Fir\u002Feditorial_policies",sort:i},{name:"Editorial Board",url:"\u002Fir\u002Feditor",sort:k},{name:"Journal Awards",url:"\u002Fir\u002Fawards",sort:l},{name:"News",url:"\u002Fir\u002Fnews",sort:n},{name:"Journal History",url:"\u002Fir\u002Fhistory",sort:G},{name:"Partners",url:"\u002Fir\u002Fpartners",sort:H},{name:"Advertise",url:"\u002Fir\u002Fadvertise",sort:I},{name:O,url:ck,sort:J}]},{name:"Publish with us",url:a,thispartype:d,sort:i,children:[{name:"For Authors",url:a,sort:d,children:[{name:"Author Instructions",url:"\u002Fir\u002Fauthor_instructions",sort:d},{name:"Article Processing Charges",url:"\u002Fir\u002Farticle_processing_charges",sort:g},{name:"Editorial Process",url:"\u002Fir\u002Feditorial_process",sort:i},{name:"Manuscript Templates",url:"\u002Fir\u002Fmanuscript_templates",sort:k},{name:"Submit a Manuscript",url:"https:\u002F\u002Fwww.oaecenter.com\u002Flogin?JournalId=ir",sort:l},{name:cl,url:cm,sort:n}]},{name:"For Reviewers",url:a,sort:g,children:[{name:"Peer Review Guidelines",url:"\u002Fir\u002Fpeer_review_guidelines",sort:d}]}]},{name:"Articles",url:a,thispartype:d,sort:l,children:[{name:"All Articles",url:"\u002Fir\u002Farticles",sort:d},{name:"Articles With Video Abstracts",url:"\u002Fir\u002Farticles_videos",sort:g},{name:cl,url:cm,sort:a}]},{name:"Special Collections",url:"\u002Fir\u002Fspecial_collections",thispartype:d,sort:n},{name:"Special Topics",url:a,thispartype:d,sort:G,children:[{name:"All Special Topics",url:"\u002Fir\u002Fspecial_issues",sort:d},{name:"Ongoing Special Topics",url:"\u002Fir\u002Fongoing_special_issues",sort:g},{name:"Completed Special Topics",url:"\u002Fir\u002Fcompleted_special_issues",sort:i},{name:"Closed Special Topics",url:"\u002Fir\u002Fclosed_special_issues",sort:k},{name:"Special Topic Ebooks",url:"\u002Fir\u002Fspecial_issues_ebooks",sort:l},{name:"Special Topic Guidelines",url:"\u002Fir\u002Fspecial_issue_guidelines",sort:n}]},{name:"Volumes",url:"\u002Fir\u002Fvolumes",thispartype:t,sort:H},{name:"Pre-onlines",url:"\u002Fir\u002Fpre_onlines",thispartype:t,sort:I},{name:"Features",url:a,thispartype:d,sort:J,children:[{name:"Webinars",url:"\u002Fir\u002Fwebinars",sort:d},{name:"Interviews",url:"\u002Fir\u002Finterviews",sort:i}]}],qksearch:{},footer:{journal_name:f,issn:cn,email:"editorial@intellrobot.com",navigation:[{title:O,url:ck},{title:"Sitemap",url:"\u002Fir\u002Fsitemap"}],cope:{title:"Committee on Publication Ethics",url:"https:\u002F\u002Fmembers.publicationethics.org\u002Fmembers\u002Fintelligence-robotics",img:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20230811\u002F49f92f416c9845b58a01de02ecea785f.jpg"},open:{title:"Portico",desc:"All published articles are preserved here permanently:",url:"https:\u002F\u002Fwww.portico.org\u002Fpublishers\u002Foae\u002F",img:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20230911\u002F67d78ebf8c55485db6ae5b5b4bcda421.jpg"},follow:[{title:"LinkedIn",url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Firene-liu-799041249\u002F",icon:"icon-linkedin"},{title:"Twitter",url:co,icon:"icon-tuite1"}],wechat_img:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20250411\u002Fd65c4b141d53429ab98ad2649306cc9c.jpg",twitter:{url:co,img:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20230824\u002F5249ddabb6d642558c9843fba9283219.png"},rss:"https:\u002F\u002Ff.oaes.cc\u002Frss\u002Fir.xml"},top:{path:e,pid:h,journal_img:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20260213\u002F7306c5708f5c4541a0c978663fd0b5a5.jpg",journal_name:f,mpt:"8 days",issn:cn,indexing:{ESCI:"https:\u002F\u002Fwww.oaepublish.com\u002Fnews\u002Fir.852",Scopus:cp,"Google Scholar":"https:\u002F\u002Fscholar.google.com.hk\u002Fcitations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=-Hx5OVYAAAAJ",Dimensions:"https:\u002F\u002Fapp.dimensions.ai\u002Fdiscover\u002Fpublication?and_facet_source_title=jour.1423782",Lens:"https:\u002F\u002Fwww.lens.org\u002Flens\u002Fsearch\u002Fscholar\u002Flist?p=0&n=10&s=date_published&d=%2B&f=false&e=false&l=en&authorField=author&dateFilterField=publishedYear&orderBy=%2Bdate_published&presentation=false&preview=true&stemmed=true&useAuthorId=false&publicationType.must=journal%20article&sourceTitle.must=Intelligence%20%26%20Robotics&publisher.must=OAE%20Publishing%20Inc.",ASCI:a},editor:"Simon X. Yang",journal_rank:"Impact factor 2.3 -Q2; CiteScore 3.7-Q2",journal_flyer:"https:\u002F\u002Ff.oaes.cc\u002Findex_ad\u002Fflyer\u002FIR-flyer.pdf",qksearch:["Intelligence","Robotics","Reinforcement Learning","Machine Learning","Unmanned Vehicles","UAV"],sitetag:"Intell Robot",ad:[],colour_tag:"#0047bb",score:a,mobile_top_img:a,impact_factor:[{factor:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20250623\u002Fff9dcb103f1746dd9c8d2f3ec900479f.png",url:a},{factor:"https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20250606\u002F5ac28259735646cc836f1a10e3c11d27.png",url:cp}],rgba:"rgb(0,71,187)",log_image:"https:\u002F\u002Fi.oaes.cc\u002Fupload\u002Fjournal_logo\u002Fir.png",heic:a},webinfo:{},searchKey:a,loading:z,appid:a,videoPlay:{show:P,href:a}},editer:{editList:{list:{}}},userdata:{showLogin:P,logined:P}},serverRendered:z,routePath:"\u002Farticles\u002Fir.2023.22",config:{_app:{basePath:cq,assetsPath:cq,cdnURL:"https:\u002F\u002Fg.oaes.cc\u002Foae\u002Fnuxt\u002F"}}}}("",6014,"[\"1\"]","1","ir","Intelligence & Robotics","2",40,"3","[\"2\"]","4","5",927,"6","Yang",0,935,"[]",null,"0","Yue","Simon X.","Chen","Wang","Deep learning-based scene understanding for autonomous robots: a survey",true,"Zhuang","Liu","Tang","[\"1\",\"2\"]","Zhang","syang@uoguelph.ca","7","8","9","10","\u003Cp\u003EAutonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed.\u003C\u002Fp\u003E","Leveraging active queries in collaborative robotic mission planning","ir.2024.06","en","Contact Us",false,"https:\u002F\u002Fv.oaes.cc\u002Fuploads\u002F20230816\u002Fa802fc5ba47f4c08ba0a95089779d731.mp4",3,2023,37,"Autonomous robots are a hot research subject within the fields of science and technology, which has a big impact on social-economic development. The ability of the autonomous robot to perceive and understand its working environment is the basis for solving more complicated issues. In recent years, an increasing number of artificial intelligence-based methods have been proposed in the field of scene understanding for autonomous robots, and deep learning is one of the current key areas in this field. Outstanding gains have been attained in the field of scene understanding for autonomous robots based on deep learning. Thus, this paper presents a review of recent research on the deep learning-based scene understanding for autonomous robots. This survey provides a detailed overview of the evolution of robotic scene understanding and summarizes the applications of deep learning methods in scene understanding for autonomous robots. In addition, the key issues in autonomous robot scene understanding are analyzed, such as pose estimation, saliency prediction, semantic segmentation, and object detection. Then, some representative deep learning-based solutions for these issues are summarized. Finally, future challenges in the field of the scene understanding for autonomous robots are discussed.","https:\u002F\u002Fi.oaes.cc\u002Fuploads\u002F20240205\u002F5aa534648ac046f0ba42673856c3bb89.jpg","Ni J, Chen Y, Tang G, Shi J, Cao W, Shi P. Deep learning-based scene understanding for autonomous robots: a survey. \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E 2023;3(3):374-401. http:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2023.22","Ni J, Chen Y, Tang G, Shi J, Cao W, Shi P. Deep learning-based scene understanding for autonomous robots: a survey. \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E. 2023;3:374-401. http:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2023.22","Ni, Jianjun, Weidong Cao, and Pengfei Shi. 2023. \"Deep learning-based scene understanding for autonomous robots: a survey\" \u003Ci\u003EIntell Robot\u003C\u002Fi\u003E. 3, no.3:  374-401. http:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2023.22","Ni, J.; Chen, Y.; Tang, G.; Shi, J.; Cao, W.; Shi, P. Deep learning-based scene understanding for autonomous robots: a survey. \u003Ci\u003EIntell. Robot.\u003C\u002Fi\u003E \u003Cb\u003E2023\u003C\u002Fb\u003E, \u003Ci\u003E3\u003C\u002Fi\u003E,  374-401. http:\u002F\u002Fdx.doi.org\u002F10.20517\u002Fir.2023.22",7558,"SANet: scale-adaptive network for lightweight salient object detection","2024-12-31 00:00:00","ir.2024.29","Weidong","Zhao","zhaowd494@163.com","Ning","Jia","Xianhui","Jiaxiong",7199,"An in-vehicle real-time infrared object detection system based on deep learning with resource-constrained hardware","2024-09-24 00:00:00","ir.2024.18","Tingting","Xunru","Liang","Bohuan","Xue","[\"3\"]","Xiaoyu","tangxy@scnu.edu.cn",6955,"Structural damage identification method based on Swin Transformer and continuous wavelet transform","2024-06-21 00:00:00","ir.2024.13","Jingzhou","Xin","Guangjiong","Tao","Qizhi","Fei","Zou","Chenglong","Xiang",6671,"2024-03-18 00:00:00","Cyrille","Berger","cyrille.berger@liu.se","Patrick","Doherty","Piotr","Rudol","Mariusz","Wzorek",6192,"Deep learning approaches for object recognition in plant diseases: a review","2023-10-28 00:00:00","ir.2023.29","Zimo","Zhou","Zhaohui","Gu",6099,"A wearable assistive system for the visually impaired using object detection, distance measurement and tactile presentation","2023-09-19 00:00:00","ir.2023.24","Yiwen","Junjie","Shen","Hideyuki","Sawada",6065,"Reinforcement learning methods for network-based transfer parameter selection","2023-08-31 00:00:00","ir.2023.23","Guo","yueguo@cs.cmu.edu","Yu","I-Hsuan","Katia","Sycara",5786,"Intelligent flood forecasting and warning: a survey","2023-06-28 00:00:00","ir.2023.12","Daiwei","Pan","Jesse","Van Griensven","Bahram","Gharabaghi","[\"*\",\"1\"]","bgharaba@uoguelph.ca",5424,"An overview of intelligent image segmentation using active contour models","2023-02-22 00:00:00","ir.2023.02","Yiyang","Pengqiang","Ge","Guina","wangguina@suda.edu.cn","Guirong","Weng","Hongtian","chtbaylor@163.com",5324,"Intelligent feature extraction, data fusion and detection of concrete bridge cracks: current development and challenges","2022-12-23 00:00:00","ir.2022.25","Di",2,6,"40",Array(10),"Autonomous robots, scene understanding, deep learning, object detection, pose estimation","11","12","13","Sarhan&nbsp;S, Nasr&nbsp;AA, Shams&nbsp;MY. Multipose face recognition-based combined adaptive deep learning vector quantization. \u003Ci\u003EComput Intell Neurosci\u003C\u002Fi\u003E 2020;2020:8821868.","https:\u002F\u002Fdx.doi.org\u002F10.1155\u002F2020\u002F8821868","http:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F33029115","About","Editorial Policies","Journals","\u002Fir","5555","\u002Fir\u002Fcontact_us","Video Abstract Guidelines","\u002Fir\u002Fvideo_abstract_guidelines","2770-3541 (Online)","https:\u002F\u002Ftwitter.com\u002FOAE_IR","https:\u002F\u002Fwww.scopus.com\u002Fsourceid\u002F21101199351","\u002F"));</script><script src="https://g.oaes.cc/oae/nuxt/ed95672.js" defer></script><script src="https://g.oaes.cc/oae/nuxt/d45592b.js" defer></script><script src="https://g.oaes.cc/oae/nuxt/535cccf.js" defer></script><script src="https://g.oaes.cc/oae/nuxt/27419e7.js" defer></script><script src="https://g.oaes.cc/oae/nuxt/08525cf.js" defer></script>
</body>

</html>
<div id="noIe" style="display: none;">
  <style>
    #noIe {
      background: rgba(99, 125, 255, 1);
      width: 100%;
      height: 100vh;
      position: fixed;
      top: 0;
      left: 0;
      z-index: 999999;
    }

    #noIe .container {
      width: 802px;
      height: 594px;
      background: #ffffff;
      border-radius: 10px;
      position: absolute;
      left: 50%;
      margin-left: -401px;
      margin-top: -297px;
      top: 50%;
      text-align: center;
    }

    #noIe .container ul {
      display: inline-block;
      height: 164px;
      margin-left: -30px;
      margin-top: 100px;
    }

    #noIe .container li {
      float: left;
      list-style: none;
      text-align: center;
      margin-left: 30px;
    }

    #noIe li img {
      width: 115px;
      height: 115px;
    }

    #noIe li p {
      margin-top: 12px;
      font-size: 14px;
      line-height: 150%;
      font-weight: 500;
      color: #526efa;
    }

    #noIe a {
      text-decoration: none;
    }

    #noIe li p a:visited {
      color: #526efa;
    }

    #noIe li p.tip {
      font-size: 10px;
      line-height: 160%;
      font-weight: normal;
      text-align: center;
      margin-top: 0;
      color: rgba(37, 38, 43, 0.36);
    }

    #noIe .title {
      margin-top: 72px;
      font-size: 36px;
      line-height: 140%;
      font-weight: bold;
      color: #25262b;
    }

    #noIe .title2 {
      margin-top: 7px;
      font-size: 14px;
      line-height: 170%;
      color: #25262b;
    }

    #noIe .logo-container {
      width: 100%;
      text-align: center;
      padding-top: 100px;
    }

    #noIe .logo {
      height: 24px;
    }

    @media screen and (max-width: 820px) {
      #noIe .container {
        width: 432px;
        left: 50%;
        margin-left: -216px;
        margin-top: 48px;
        position: relative;
        top: 0;
      }

      #noIe .container ul {
        width: 290px;
        height: 352px;
        margin-left: -30px;
        margin-top: 40px;
      }

      #noIe .container li {
        margin-left: 30px;
        margin-top: 24px;
      }

      #noIe .logo-container {
        padding-top: 121px;
        padding-bottom: 20px;
      }
    }

  </style>
  <div class="container">
    <p class="title">The current browser is not compatible</p>
    <p class="title2">The following browsers are recommended for the best use experience</p>
    <ul>
      <li>
        <a href="https://www.google.cn/chrome/" target="_blank">
          <img src="https://gw.alicdn.com/imgextra/i2/O1CN01Nn0IoE1cmXZ6gFiM3_!!6000000003643-2-tps-230-230.png" />
          <p>Chrome</p>
        </a>
      </li>
      <li>
        <a href="http://www.firefox.com.cn/" target="_blank">
          <img src="https://gw.alicdn.com/imgextra/i3/O1CN01P8aqdX1HdHczGialK_!!6000000000780-2-tps-230-230.png" />
          <p>Firefox</p>
        </a>
      </li>
      <li>
        <a href="https://www.apple.com/safari/" target="_blank">
          <img src="https://gw.alicdn.com/imgextra/i4/O1CN01vVxDF11chVD0nsbiZ_!!6000000003632-2-tps-230-230.png" />
          <p>Safari</p>
          <p class="tip">Only supports Mac</p>
        </a>
      </li>
      <li>
        <a href="https://www.microsoft.com/zh-cn/edge" target="_blank">
          <img src="https://gw.alicdn.com/imgextra/i4/O1CN01UW7hs31Xa6jfm2a2O_!!6000000002939-2-tps-230-230.png" />
          <p>Edge</p>
        </a>
      </li>
    </ul>
    <div class="logo-container">
      <img
        src="data:image/png;base64,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"
        class="logo" />
    </div>
  </div>

</div>

</html>
<script>
  window.onload = function () {
    if (!!window.ActiveXObject || 'ActiveXObject' in window) {
      document.getElementById('noIe').style.display = '';
    }
  }
</script>
